commit 3b77140807157af88b18e100c41fe7b40be660f2
Author: 22t0007 <22t0007@master.gpu01.cis.k.hosei.ac.jp>
Date:   Mon Mar 4 14:25:42 2024 +0900

    first commit

diff --git a/__pycache__/distributed_util.cpython-38.pyc b/__pycache__/distributed_util.cpython-38.pyc
new file mode 100644
index 0000000..a7b16b9
Binary files /dev/null and b/__pycache__/distributed_util.cpython-38.pyc differ
diff --git a/__pycache__/generate.cpython-38.pyc b/__pycache__/generate.cpython-38.pyc
new file mode 100644
index 0000000..f61b285
Binary files /dev/null and b/__pycache__/generate.cpython-38.pyc differ
diff --git a/__pycache__/losses.cpython-38.pyc b/__pycache__/losses.cpython-38.pyc
new file mode 100644
index 0000000..1a23414
Binary files /dev/null and b/__pycache__/losses.cpython-38.pyc differ
diff --git a/__pycache__/utils.cpython-38.pyc b/__pycache__/utils.cpython-38.pyc
new file mode 100644
index 0000000..aab42a5
Binary files /dev/null and b/__pycache__/utils.cpython-38.pyc differ
diff --git a/configs/config.yaml b/configs/config.yaml
new file mode 100644
index 0000000..f1c898f
--- /dev/null
+++ b/configs/config.yaml
@@ -0,0 +1,40 @@
+defaults:
+  - _self_
+  - experiment: binaural
+
+train: # Not used in generate.py
+  name: null # Name of experiment (prefix of experiment name)
+  ckpt_iter: max
+  iters_per_ckpt: 10000
+  iters_per_logging: 100
+  n_iters: 1000001
+  learning_rate: 2e-4
+  batch_size_per_gpu: 4
+  clip_length: 32000
+
+generate:
+  ckpt_iter: max # Which checkpoint to use; assign a number or "max". Is ignored when sampling during training
+  ckpt_smooth: null # Which checkpoint to start averaging from (experimental feature, can ignore)
+  n_samples: 16 # Number of utterances to be generated (per GPU)
+  batch_size: null # Number of samples to generate at once per GPU. null means max (equal to samples_per_gpu)
+  mel_path: null # Folder of preprocessed spectrograms (optional)
+  mel_name: null # Name of specific spectrogram to condition on (set to null for unconditional, set to an audio file name for vocoding)
+
+distributed:
+  dist_backend: nccl
+  dist_url: tcp://localhost:54321
+
+wandb:
+  mode: online # Pass in 'wandb.mode=online' to turn on wandb logging
+  project: test-b
+  entity: null
+  id: null # Set to string to resume logging from run
+  job_type: training
+
+validate:
+  iters_per_ckpt: 10000
+  max_ckpt: 247000
+  min_ckpt: 107000
+  data_path: "dataset"
+  exp_path: exp/snet_d64_n8_pool_2-2_expand2_ff2_L32000half_mean_condition_batch4_mse
+  use_pesq: false
\ No newline at end of file
diff --git a/configs/dataset/binaural.yaml b/configs/dataset/binaural.yaml
new file mode 100644
index 0000000..c03fb64
--- /dev/null
+++ b/configs/dataset/binaural.yaml
@@ -0,0 +1,12 @@
+_name_: binaural
+data_path: dataset
+clip_length: 32000
+segment_length: 16000
+sampling_rate: 48000
+valid: false
+filter_length: 1024
+hop_length: 256
+win_length: 1024
+mel_fmin: 0.0
+mel_fmax: 8000.0
+batch_size: 4
\ No newline at end of file
diff --git a/configs/experiment/binaural.yaml b/configs/experiment/binaural.yaml
new file mode 100644
index 0000000..1dc5e5d
--- /dev/null
+++ b/configs/experiment/binaural.yaml
@@ -0,0 +1,17 @@
+# @package _global_
+defaults:
+  - /model: binaurals4
+  - /dataset: binaural
+
+train:
+  iters_per_ckpt: 1000
+  batch_size_per_gpu: 4
+  loss_function: mse # mse or stft
+
+generate:
+  mel_name: BI001-0001
+  n_samples: 2
+
+model:
+  unconditional: false
+  mel_upsample: [16, 16] # Product should equal hop size in the dataset config!
\ No newline at end of file
diff --git a/configs/model/binaurals4.yaml b/configs/model/binaurals4.yaml
new file mode 100644
index 0000000..93588c6
--- /dev/null
+++ b/configs/model/binaurals4.yaml
@@ -0,0 +1,15 @@
+_name_: binaurals4
+__name__: binaurals4
+name: binaurals4
+unconditional: false
+use_mean_condition: true
+in_channels: 2
+out_channels: 2
+unet: false
+d_model: 64
+n_layers: 8
+pool: [2, 2]
+expand: 2
+ff: 2
+loss: mse
+L: ${dataset.segment_length} # Truncates infinite kernel to length of training inputs
\ No newline at end of file
diff --git a/dataset/__init__.py b/dataset/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/dataset/__pycache__/__init__.cpython-38.pyc b/dataset/__pycache__/__init__.cpython-38.pyc
new file mode 100644
index 0000000..563599f
Binary files /dev/null and b/dataset/__pycache__/__init__.cpython-38.pyc differ
diff --git a/dataset/dataset.py b/dataset/dataset.py
new file mode 100644
index 0000000..a0cfe87
--- /dev/null
+++ b/dataset/dataset.py
@@ -0,0 +1,228 @@
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import numpy as np
+import os
+import random
+import torch
+import torchaudio
+
+from glob import glob
+from torch.utils.data.distributed import DistributedSampler
+import torch.nn.functional as F
+
+class BinauralConditionalDataset(torch.utils.data.Dataset):
+  def __init__(self, paths, binaural_type="", predict_mean_condition=False):
+    super().__init__()
+    self.mono, self.binaural, self.binaural_geowarp, self.view = [], [], [], []
+    self.binaural_type = binaural_type
+    self.predict_mean_condition = predict_mean_condition
+    for subject_id in range(8):
+      mono, _ = torchaudio.load(f"{paths}/subject{subject_id + 1}/mono.wav")
+      binaural, _ = torchaudio.load(f"{paths}/subject{subject_id + 1}/binaural.wav")
+      binaural_geowarp, _ = torchaudio.load(f"{paths}/subject{subject_id + 1}/binaural_geowarp.wav")
+      # receiver is fixed at origin in this dataset, so we only need transmitter view
+      tx_view = np.loadtxt(f"{paths}/subject{subject_id + 1}/tx_positions.txt").transpose()
+      self.mono.append(mono)
+      self.binaural.append(binaural)
+      self.binaural_geowarp.append(binaural_geowarp)
+      self.view.append(tx_view.astype(np.float32))
+    # ensure that chunk_size is a multiple of 400 to match audio (48kHz) and receiver/transmitter positions (120Hz)
+    self.chunk_size = 2000 * 48
+    if self.chunk_size % 400 > 0:
+      self.chunk_size = self.chunk_size + 400 - self.chunk_size % 400
+    # compute chunks
+    self.chunks = []
+    for subject_id in range(8):
+      last_chunk_start_frame = self.mono[subject_id].shape[-1] - self.chunk_size + 1
+      hop_length = int((1 - 0.5) * self.chunk_size)
+      for offset in range(0, last_chunk_start_frame, hop_length):
+        self.chunks.append({'subject': subject_id, 'offset': offset})
+
+  def __len__(self):
+    return len(self.chunks)
+
+  def __getitem__(self, idx):
+    subject = self.chunks[idx]['subject']
+    offset = self.chunks[idx]['offset']
+    mono = self.mono[subject][:, offset:offset+self.chunk_size]
+    view = self.view[subject][:, offset//400:(offset+self.chunk_size)//400]
+
+    binaural = self.binaural[subject][0:2, offset:offset+self.chunk_size]
+    binaural_geowarp = self.binaural_geowarp[subject][0:2, offset:offset+self.chunk_size]
+
+    mean_condition = self.binaural[subject][0:2, offset:offset+self.chunk_size].mean(0, keepdim=True)
+
+    return {
+        'mono': mono,
+        'binaural': binaural,
+        'binaural_geowarp': binaural_geowarp,
+        'view': view,
+        'mean_condition': mean_condition,
+    }
+
+
+
+
+class Collator:
+  def __init__(self, params):
+    self.params = params
+
+  def collate_binaural(self, minibatch):
+
+    clip_length = self.params.clip_length
+    for record in minibatch:
+
+      start_view = random.randint(0, record['mono'].shape[1] // 400 - clip_length // 400)
+      start = start_view * 400
+      end_view = start_view + clip_length // 400
+      end = end_view * 400
+      record['mono'] = record['mono'][:, start:end]
+      record['mean_condition'] = record['mean_condition'][:, start:end]
+      record['binaural'] = record['binaural'][:, start:end]
+      record['binaural_geowarp'] = record['binaural_geowarp'][:, start:end]
+      record['view'] = record['view'][:, start_view:end_view].T
+      record['view'] = np.repeat(record['view'], 400, axis=0).T
+
+    mono = np.stack([record['mono'] for record in minibatch if 'mono' in record])
+    mean_condition = np.stack([record['mean_condition'] for record in minibatch if 'mean_condition' in record])
+    binaural = np.stack([record['binaural'] for record in minibatch if 'binaural' in record])
+    binaural_geowarp = np.stack([record['binaural_geowarp'] for record in minibatch if 'binaural_geowarp' in record])
+    view = np.stack([record['view'] for record in minibatch if 'view' in record])
+
+    assert binaural_geowarp.shape[0] == view.shape[0]
+
+    return {
+        'mono': torch.from_numpy(mono),
+        'mean_condition': torch.from_numpy(mean_condition),
+        'audio': torch.from_numpy(binaural),
+        'binaural_geowarp': torch.from_numpy(binaural_geowarp),
+        'view': torch.from_numpy(view),
+    }
+
+class AttrDict(dict):
+  def __init__(self, *args, **kwargs):
+      super(AttrDict, self).__init__(*args, **kwargs)
+      self.__dict__ = self
+
+  def override(self, attrs):
+    if isinstance(attrs, dict):
+      self.__dict__.update(**attrs)
+    elif isinstance(attrs, (list, tuple, set)):
+      for attr in attrs:
+        self.override(attr)
+    elif attrs is not None:
+      raise NotImplementedError
+    return self
+
+def from_path(data_dirs, params, binaural_type="", is_distributed=False):
+  if binaural_type:
+    dataset = BinauralConditionalDataset(data_dirs[0], binaural_type,
+      predict_mean_condition=getattr(params, "predict_mean_condition", False))
+  else:
+    raise ValueError("Unsupported binaural_type")
+  return torch.utils.data.DataLoader(
+      dataset,
+      batch_size=params.batch_size,
+      collate_fn=Collator(params).collate_binaural,
+      shuffle=not is_distributed,
+      num_workers=os.cpu_count(),
+      sampler=DistributedSampler(dataset) if is_distributed else None,
+      pin_memory=True,
+      drop_last=True)
+
+
+
+params = AttrDict(
+    # Training params
+    batch_size=8,
+    learning_rate=2e-4,
+    max_grad_norm=None,
+    use_mono=False,
+    clip_length= 16000,
+    lambda_phase=0.0,
+
+    # Data params
+    sample_rate=48000,
+    n_mels=80,
+    n_fft=1024,
+    hop_samples=256,
+    crop_mel_frames=62,  # Probably an error in paper.
+
+    # Model params
+    residual_layers=30,
+    residual_channels=64,
+    dilation_cycle_length=10,
+    unconditional = False,
+    noise_schedule=np.linspace(1e-4, 0.05, 50).tolist(),
+    inference_noise_schedule=[0.0001, 0.001, 0.01, 0.05, 0.2, 0.5],
+
+    # unconditional sample len
+    audio_len = 48000*5, # unconditional_synthesis_samples
+)
+
+
+
+
+# params_stage_two = AttrDict(
+#     # Training params
+#     batch_size=4,
+#     learning_rate=2e-4,
+#     max_grad_norm=None,
+#     use_mono=True,
+#     clip_length= 6400,
+#     lambda_phase=0.01,
+#     loss_per_layer=3,
+#     use_l2_loss=True,
+#     use_mean_condition=True,
+
+
+#     # Data params
+#     sample_rate=48000,
+#     n_mels=80,
+#     n_fft=1024,
+#     hop_samples=256,
+#     crop_mel_frames=62,  # Probably an error in paper.
+
+#     # Model params
+#     residual_layers=30,
+#     residual_channels=128,
+#     dilation_cycle_length=10,
+#     unconditional = False,
+#     noise_schedule=np.linspace(1e-4, 0.02, 200).tolist(),
+#     inference_noise_schedule=[0.0001, 0.001, 0.01, 0.05, 0.2, 0.5],
+
+#     # unconditional sample len
+#     audio_len = 48000*5, # unconditional_synthesis_samples
+# )
+
+# params_stage_one = AttrDict(
+#     # Training params
+#     batch_size=4,
+#     learning_rate=2e-4,
+#     max_grad_norm=None,
+#     use_mono=True,
+#     clip_length= 6400,
+#     lambda_phase=0.01,
+#     loss_per_layer=3,
+#     use_l2_loss=True,
+#     predict_mean_condition=True,
+
+
+#     # Data params
+#     sample_rate=48000,
+#     n_mels=80,
+#     n_fft=1024,
+#     hop_samples=256,
+#     crop_mel_frames=62,  # Probably an error in paper.
+
+#     # Model params
+#     residual_layers=30,
+#     residual_channels=128,
+#     dilation_cycle_length=10,
+#     unconditional = False,
+#     noise_schedule=np.linspace(1e-4, 0.02, 200).tolist(),
+#     inference_noise_schedule=[0.0001, 0.001, 0.01, 0.05, 0.2, 0.5],
+
+#     # unconditional sample len
+#     audio_len = 48000*5, # unconditional_synthesis_samples
+# )
\ No newline at end of file
diff --git a/distributed_util.py b/distributed_util.py
new file mode 100644
index 0000000..1e90248
--- /dev/null
+++ b/distributed_util.py
@@ -0,0 +1,149 @@
+# *****************************************************************************
+# Adapted from https://github.com/NVIDIA/waveglow/blob/master/distributed.py
+# *****************************************************************************
+
+# *****************************************************************************
+#  Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#      * Redistributions of source code must retain the above copyright
+#        notice, this list of conditions and the following disclaimer.
+#      * Redistributions in binary form must reproduce the above copyright
+#        notice, this list of conditions and the following disclaimer in the
+#        documentation and/or other materials provided with the distribution.
+#      * Neither the name of the NVIDIA CORPORATION nor the
+#        names of its contributors may be used to endorse or promote products
+#        derived from this software without specific prior written permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+#  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+#  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+#  DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
+#  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+#  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+#  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+# *****************************************************************************\
+
+import os
+import sys
+import time
+import subprocess
+import argparse
+import warnings
+warnings.filterwarnings("ignore")
+
+import torch
+import torch.distributed as dist
+from torch.autograd import Variable
+
+def reduce_tensor(tensor, num_gpus):
+    rt = tensor.clone()
+    dist.all_reduce(rt, op=dist.ReduceOp.SUM)
+    rt /= num_gpus
+    return rt
+
+def init_distributed(rank, num_gpus, group_name, dist_backend, dist_url):
+    assert torch.cuda.is_available(), "Distributed mode requires CUDA."
+    print("Initializing Distributed")
+
+    # Set cuda device so everything is done on the right GPU.
+    torch.cuda.set_device(rank % torch.cuda.device_count())
+
+    # Initialize distributed communication
+    dist.init_process_group(dist_backend, init_method=dist_url,
+                            world_size=num_gpus, rank=rank,
+                            group_name=group_name)
+
+def _flatten_dense_tensors(tensors):
+    """Flatten dense tensors into a contiguous 1D buffer. Assume tensors are of
+    same dense type.
+    Since inputs are dense, the resulting tensor will be a concatenated 1D
+    buffer. Element-wise operation on this buffer will be equivalent to
+    operating individually.
+    Arguments:
+        tensors (Iterable[Tensor]): dense tensors to flatten.
+    Returns:
+        A contiguous 1D buffer containing input tensors.
+    """
+    if len(tensors) == 1:
+        return tensors[0].contiguous().view(-1)
+    flat = torch.cat([t.contiguous().view(-1) for t in tensors], dim=0)
+    return flat
+
+def _unflatten_dense_tensors(flat, tensors):
+    """View a flat buffer using the sizes of tensors. Assume that tensors are of
+    same dense type, and that flat is given by _flatten_dense_tensors.
+    Arguments:
+        flat (Tensor): flattened dense tensors to unflatten.
+        tensors (Iterable[Tensor]): dense tensors whose sizes will be used to
+          unflatten flat.
+    Returns:
+        Unflattened dense tensors with sizes same as tensors and values from
+        flat.
+    """
+    outputs = []
+    offset = 0
+    for tensor in tensors:
+        numel = tensor.numel()
+        outputs.append(flat.narrow(0, offset, numel).view_as(tensor))
+        offset += numel
+    return tuple(outputs)
+
+def apply_gradient_allreduce(module):
+    """
+    Modifies existing model to do gradient allreduce, but doesn't change class
+    so you don't need "module"
+    """
+    if not hasattr(dist, '_backend'):
+        module.warn_on_half = True
+    else:
+        module.warn_on_half = True if dist._backend == dist.dist_backend.GLOO else False
+
+    for p in module.state_dict().values():
+        if not torch.is_tensor(p):
+            continue
+        dist.broadcast(p, 0)
+
+    def allreduce_params():
+        if(module.needs_reduction):
+            module.needs_reduction = False
+            buckets = {}
+            for param in module.parameters():
+                if param.requires_grad and param.grad is not None:
+                    tp = type(param.data)
+                    if tp not in buckets:
+                        buckets[tp] = []
+                    buckets[tp].append(param)
+            if module.warn_on_half:
+                if torch.cuda.HalfTensor in buckets:
+                    print("WARNING: gloo dist backend for half parameters may be extremely slow." +
+                          " It is recommended to use the NCCL backend in this case. This currently requires" +
+                          "PyTorch built from top of tree master.")
+                    module.warn_on_half = False
+
+            for tp in buckets:
+                bucket = buckets[tp]
+                grads = [param.grad.data for param in bucket]
+                coalesced = _flatten_dense_tensors(grads)
+                dist.all_reduce(coalesced)
+                coalesced /= dist.get_world_size()
+                for buf, synced in zip(grads, _unflatten_dense_tensors(coalesced, grads)):
+                    buf.copy_(synced)
+
+    for param in list(module.parameters()):
+        def allreduce_hook(*unused):
+            Variable._execution_engine.queue_callback(allreduce_params)
+        if param.requires_grad:
+            param.register_hook(allreduce_hook)
+            dir(param)
+
+    def set_needs_reduction(self, input, output):
+        self.needs_reduction = True
+
+    module.register_forward_hook(set_needs_reduction)
+    return module
diff --git a/extensions/kernels/README.md b/extensions/kernels/README.md
new file mode 100644
index 0000000..43dc2ca
--- /dev/null
+++ b/extensions/kernels/README.md
@@ -0,0 +1,10 @@
+Install the Cauchy and Vandermonde CUDA kernels:
+```
+python setup.py install
+```
+
+(Optional) Test the extensions
+```
+pytest -q -s test_cauchy.py
+pytest -q -s test_vandermonde.py
+```
diff --git a/extensions/kernels/__pycache__/cauchy.cpython-38.pyc b/extensions/kernels/__pycache__/cauchy.cpython-38.pyc
new file mode 100644
index 0000000..e04d591
Binary files /dev/null and b/extensions/kernels/__pycache__/cauchy.cpython-38.pyc differ
diff --git a/extensions/kernels/__pycache__/vandermonde.cpython-38.pyc b/extensions/kernels/__pycache__/vandermonde.cpython-38.pyc
new file mode 100644
index 0000000..fab792f
Binary files /dev/null and b/extensions/kernels/__pycache__/vandermonde.cpython-38.pyc differ
diff --git a/extensions/kernels/benchmark_cauchy.py b/extensions/kernels/benchmark_cauchy.py
new file mode 100644
index 0000000..23b2513
--- /dev/null
+++ b/extensions/kernels/benchmark_cauchy.py
@@ -0,0 +1,42 @@
+import math
+from functools import partial
+
+import torch
+
+from einops import rearrange
+
+from .cauchy import cauchy_mult_torch, cauchy_mult_keops, cauchy_mult
+from benchmark.utils import benchmark_all, benchmark_combined, benchmark_forward, benchmark_backward
+
+
+def generate_data(batch_size, N, L, symmetric=True, device='cuda'):
+    if not symmetric:
+        v = torch.randn(batch_size, N, dtype=torch.complex64, device=device, requires_grad=True)
+        w = torch.randn(batch_size, N, dtype=torch.complex64, device=device, requires_grad=True)
+        z = torch.randn(L, dtype=torch.complex64, device=device)
+    else:
+        assert N % 2 == 0
+        v_half = torch.randn(batch_size, N // 2, dtype=torch.complex64, device=device)
+        v = torch.cat([v_half, v_half.conj()], dim=-1).requires_grad_(True)
+        w_half = torch.randn(batch_size, N // 2, dtype=torch.complex64, device=device)
+        w = torch.cat([w_half, w_half.conj()], dim=-1).requires_grad_(True)
+        z = torch.exp(1j * torch.randn(L, dtype=torch.float32, device=device))
+    return v, z, w
+
+
+if __name__ == '__main__':
+    device = 'cuda'
+    bs = 1024
+    N = 64
+    L = 16384
+
+    v, z, w = generate_data(bs, N, L, symmetric=True)
+    v_half = v[:, :N // 2].clone().detach().requires_grad_(True)
+    w_half = w[:, :N // 2].clone().detach().requires_grad_(True)
+
+    repeat = 30
+    benchmark_all(repeat, cauchy_mult_keops, v, z, w, desc='Cauchy mult keops')
+    fn = partial(cauchy_mult, symmetric=False)
+    benchmark_all(repeat, fn, v, z, w, desc='Cauchy mult')
+    fn = partial(cauchy_mult, symmetric=True)
+    benchmark_all(repeat, fn, v_half, z, w_half, desc='Cauchy mult symmetric')
diff --git a/extensions/kernels/benchmark_cauchy_tune.py b/extensions/kernels/benchmark_cauchy_tune.py
new file mode 100644
index 0000000..4934a6e
--- /dev/null
+++ b/extensions/kernels/benchmark_cauchy_tune.py
@@ -0,0 +1,56 @@
+import importlib
+import json
+import argparse
+
+import torch
+
+from benchmark.utils import benchmark_forward
+
+
+def generate_data(batch_size, N, L, symmetric=True, device='cuda'):
+    if not symmetric:
+        v = torch.randn(batch_size, N, dtype=torch.complex64, device=device, requires_grad=True)
+        w = torch.randn(batch_size, N, dtype=torch.complex64, device=device, requires_grad=True)
+        z = torch.randn(L, dtype=torch.complex64, device=device)
+    else:
+        assert N % 2 == 0
+        v_half = torch.randn(batch_size, N // 2, dtype=torch.complex64, device=device)
+        v = torch.cat([v_half, v_half.conj()], dim=-1).requires_grad_(True)
+        w_half = torch.randn(batch_size, N // 2, dtype=torch.complex64, device=device)
+        w = torch.cat([w_half, w_half.conj()], dim=-1).requires_grad_(True)
+        z = torch.exp(1j * torch.randn(L, dtype=torch.float32, device=device))
+    return v, z, w
+
+
+parser = argparse.ArgumentParser(description='Tuning Cauchy multiply')
+parser.add_argument('--name', default='cauchy_mult')
+parser.add_argument('--mode', default='forward', choices=['forward', 'backward'])
+parser.add_argument('-bs', '--batch-size', default=1024, type=int)
+parser.add_argument('-N', default=64, type=int)
+parser.add_argument('-L', default=2 ** 14, type=int)
+
+
+if __name__ == '__main__':
+    args = parser.parse_args()
+    device = 'cuda'
+    bs = args.batch_size
+    N = args.N
+    L = args.L
+    repeat = 30
+    v, z, w = generate_data(bs, N, L, symmetric=True)
+    v_half = v[:, :N // 2].clone().detach().requires_grad_(True)
+    w_half = w[:, :N // 2].clone().detach().requires_grad_(True)
+
+    tuning_extension_name = args.name
+    # print('Extension name:', tuning_extension_name)
+    module = importlib.import_module(tuning_extension_name)
+    if args.mode == 'forward':
+        _, m = benchmark_forward(repeat, module.cauchy_mult_sym_fwd, v_half, z, w_half,
+                                 verbose=False, desc='Cauchy mult symmetric fwd')
+    else:
+        out = module.cauchy_mult_sym_fwd(v_half, z, w_half)
+        dout = torch.randn_like(out)
+        _, m = benchmark_forward(repeat, module.cauchy_mult_sym_bwd, v_half, z, w_half, dout,
+                                 verbose=False, desc='Cauchy mult symmetric bwd')
+    result_dict = dict(time_mean = m.mean, time_iqr = m.iqr)
+    print(json.dumps(result_dict))
diff --git a/extensions/kernels/cauchy.cpp b/extensions/kernels/cauchy.cpp
new file mode 100644
index 0000000..7951441
--- /dev/null
+++ b/extensions/kernels/cauchy.cpp
@@ -0,0 +1,102 @@
+#include <vector>
+#include <utility>
+#include <cmath>
+#include <torch/extension.h>
+#include <c10/cuda/CUDAGuard.h>
+
+#define CHECK_DEVICE(x) TORCH_CHECK(x.device().type() == torch::kCUDA, #x " must be on CUDA")
+#define CHECK_SHAPE(x, ...) TORCH_CHECK(x.sizes() == torch::IntArrayRef({__VA_ARGS__}), #x " must have shape (" #__VA_ARGS__ ")")
+
+torch::Tensor cauchy_mult_sym_fwd_cuda(torch::Tensor v,
+                                       torch::Tensor z,
+                                       torch::Tensor w);
+std::tuple<torch::Tensor, torch::Tensor> cauchy_mult_sym_bwd_cuda(torch::Tensor v,
+                                                                  torch::Tensor z,
+                                                                  torch::Tensor w,
+                                                                  torch::Tensor dout);
+
+namespace cauchy {
+
+torch::Tensor cauchy_mult_sym_fwd(torch::Tensor v,
+                                  torch::Tensor z,
+                                  torch::Tensor w) {
+  CHECK_DEVICE(v); CHECK_DEVICE(z); CHECK_DEVICE(w);
+  const auto batch_size = v.size(0);
+  const auto N = v.size(1);
+  const auto L = z.size(0);
+  CHECK_SHAPE(v, batch_size, N);
+  CHECK_SHAPE(z, L);
+  CHECK_SHAPE(w, batch_size, N);
+  // Otherwise the kernel will be launched from cuda:0 device
+  // Cast to char to avoid compiler warning about narrowing
+  at::cuda::CUDAGuard device_guard{(char)v.get_device()};
+  return cauchy_mult_sym_fwd_cuda(v, z, w);
+}
+
+std::tuple<torch::Tensor, torch::Tensor>
+cauchy_mult_sym_bwd(torch::Tensor v,
+                    torch::Tensor z,
+                    torch::Tensor w,
+                    torch::Tensor dout) {
+  CHECK_DEVICE(v); CHECK_DEVICE(z); CHECK_DEVICE(w); CHECK_DEVICE(dout);
+  const auto batch_size = v.size(0);
+  const auto N = v.size(1);
+  const auto L = z.size(0);
+  CHECK_SHAPE(v, batch_size, N);
+  CHECK_SHAPE(z, L);
+  CHECK_SHAPE(w, batch_size, N);
+  CHECK_SHAPE(dout, batch_size, L);
+  // Otherwise the kernel will be launched from cuda:0 device
+  // Cast to char to avoid compiler warning about narrowing
+  at::cuda::CUDAGuard device_guard{(char)v.get_device()};
+  return cauchy_mult_sym_bwd_cuda(v, z, w, dout);
+}
+
+}  // cauchy
+
+torch::Tensor vand_log_mult_sym_fwd_cuda(torch::Tensor v, torch::Tensor x, int L);
+
+std::tuple<torch::Tensor, torch::Tensor>
+vand_log_mult_sym_bwd_cuda(torch::Tensor v, torch::Tensor x, torch::Tensor dout);
+
+namespace vand {
+
+torch::Tensor vand_log_mult_sym_fwd(torch::Tensor v, torch::Tensor x, int L) {
+  CHECK_DEVICE(v); CHECK_DEVICE(x);
+  const auto batch_size = v.size(0);
+  const auto N = v.size(1);
+  CHECK_SHAPE(v, batch_size, N);
+  CHECK_SHAPE(x, batch_size, N);
+  // Otherwise the kernel will be launched from cuda:0 device
+  // Cast to char to avoid compiler warning about narrowing
+  at::cuda::CUDAGuard device_guard{(char)v.get_device()};
+  return vand_log_mult_sym_fwd_cuda(v, x, L);
+}
+
+std::tuple<torch::Tensor, torch::Tensor>
+vand_log_mult_sym_bwd(torch::Tensor v, torch::Tensor x, torch::Tensor dout) {
+  CHECK_DEVICE(v); CHECK_DEVICE(x); CHECK_DEVICE(dout);
+  const auto batch_size = v.size(0);
+  const auto N = v.size(1);
+  const auto L = dout.size(1);
+  CHECK_SHAPE(v, batch_size, N);
+  CHECK_SHAPE(x, batch_size, N);
+  CHECK_SHAPE(dout, batch_size, L);
+  // Otherwise the kernel will be launched from cuda:0 device
+  // Cast to char to avoid compiler warning about narrowing
+  at::cuda::CUDAGuard device_guard{(char)v.get_device()};
+  return vand_log_mult_sym_bwd_cuda(v, x, dout);
+}
+
+}  // vand
+
+PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
+  m.def("cauchy_mult_sym_fwd", &cauchy::cauchy_mult_sym_fwd,
+        "Cauchy multiply symmetric forward");
+  m.def("cauchy_mult_sym_bwd", &cauchy::cauchy_mult_sym_bwd,
+        "Cauchy multiply symmetric backward");
+  m.def("vand_log_mult_sym_fwd", &vand::vand_log_mult_sym_fwd,
+        "Log Vandermonde multiply symmetric forward");
+  m.def("vand_log_mult_sym_bwd", &vand::vand_log_mult_sym_bwd,
+        "Log Vandermonde multiply symmetric backward");
+}
diff --git a/extensions/kernels/cauchy.py b/extensions/kernels/cauchy.py
new file mode 100644
index 0000000..c61bbbf
--- /dev/null
+++ b/extensions/kernels/cauchy.py
@@ -0,0 +1,102 @@
+from pathlib import Path
+import torch
+
+from einops import rearrange
+
+from structured_kernels import cauchy_mult_sym_fwd, cauchy_mult_sym_bwd
+# try:
+#     from cauchy_mult import cauchy_mult_sym_fwd, cauchy_mult_sym_bwd
+# except ImportError:
+#     from torch.utils.cpp_extension import load
+#     current_dir = Path(__file__).parent.absolute()
+#     cauchy_mult_extension = load(
+#         name='cauchy_mult',
+#         sources=[str(current_dir / 'cauchy.cpp'), str(current_dir / 'cauchy_cuda.cu')],
+#         extra_cflags=['-g', '-march=native', '-funroll-loops'],
+#         extra_cuda_cflags=['-O3', '-lineinfo', '--use_fast_math'],
+#         extra_include_paths=str(current_dir),
+#         build_directory=str(current_dir),
+#         verbose=True
+#     )
+#     cauchy_mult_sym_fwd = cauchy_mult_extension.cauchy_mult_sym_fwd
+#     cauchy_mult_sym_bwd = cauchy_mult_extension.cauchy_mult_sym_bwd
+
+
+def cauchy_mult_torch(v: torch.Tensor, z: torch.Tensor, w: torch.Tensor,
+                      symmetric=True) -> torch.Tensor:
+    """
+    v: (B, N)
+    z: (L)
+    w: (B, N)
+    symmetric: whether to assume that v and w contain complex conjugate pairs, of the form
+    [v_half, v_half.conj()] and [w_half, w_half.conj()]
+    """
+    if not symmetric:
+        return (rearrange(v, 'b n -> b 1 n') / (rearrange(z, 'l -> l 1') - rearrange(w, 'b n -> b 1 n'))).sum(dim=-1)
+    else:
+        N = v.shape[-1]
+        assert N % 2 == 0
+        vv = rearrange(v[:, :N // 2], 'b n -> b 1 n')
+        zz = rearrange(z, 'l -> l 1')
+        ww = rearrange(w[:, :N // 2], 'b n -> b 1 n')
+        # return 2 * ((zz * vv.real - vv.real * ww.real - vv.imag * ww.imag)
+        #             / (zz * zz - 2 * zz * ww.real + ww.abs().square())).sum(dim=-1)
+        return (vv / (zz - ww) + vv.conj() / (zz - ww.conj())).sum(dim=-1)
+
+
+def cauchy_mult_keops(v, z, w):
+    from pykeops.torch import LazyTensor
+    v_l = LazyTensor(rearrange(v, 'b N -> b 1 N 1'))
+    z_l = LazyTensor(rearrange(z, 'L -> 1 L 1 1'))
+    w_l = LazyTensor(rearrange(w, 'b N -> b 1 N 1'))
+    sub = z_l - w_l  # (b N L 1), for some reason it doesn't display the last dimension
+    div = v_l / sub
+    s = div.sum(dim=2, backend='GPU')
+    return s.squeeze(-1)
+
+
+def _cauchy_mult(v, z, w):
+    return CauchyMultiplySymmetric.apply(v, z, w)
+
+
+def cauchy_mult(v, z, w):
+    """ Wrap the cuda method to deal with shapes """
+    v, w = torch.broadcast_tensors(v, w)
+    shape = v.shape
+    # z_shape = z.shape
+    # z = z.squeeze()
+    assert len(z.shape) == 1
+
+    v = v.contiguous()
+    w = w.contiguous()
+    z = z.contiguous()
+
+    N = v.size(-1)
+    assert w.size(-1) == N
+    y = _cauchy_mult(v.view(-1, N), z, w.view(-1, N))
+    y = y.view(*shape[:-1], z.size(-1))
+    return y
+
+
+class CauchyMultiplySymmetric(torch.autograd.Function):
+
+    @staticmethod
+    def forward(ctx, v, z, w):
+        batch, N = v.shape
+        supported_N_values = [1 << log_n for log_n in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]
+        L = z.shape[-1]
+        if not N in supported_N_values:
+            raise NotImplementedError(f'Only support N values in {supported_N_values}')
+        max_L_value = 32 * 1024 * 64 * 1024
+        if L > max_L_value:
+            raise NotImplementedError(f'Only support L values <= {max_L_value}')
+        if not (v.is_cuda and z.is_cuda and w.is_cuda):
+            raise NotImplementedError(f'Only support CUDA tensors')
+        ctx.save_for_backward(v, z, w)
+        return cauchy_mult_sym_fwd(v, z, w)
+
+    @staticmethod
+    def backward(ctx, dout):
+        v, z, w = ctx.saved_tensors
+        dv, dw = cauchy_mult_sym_bwd(v, z, w, dout)
+        return dv, None, dw
diff --git a/extensions/kernels/cauchy_cuda.cu b/extensions/kernels/cauchy_cuda.cu
new file mode 100644
index 0000000..2d645d6
--- /dev/null
+++ b/extensions/kernels/cauchy_cuda.cu
@@ -0,0 +1,368 @@
+#include <stdio.h>
+// On pytorch 1.10 and CUDA 10.2, I get compilation errors on torch/csrc/api/include/torch/nn/cloneable.h
+// So we'll only include torch/python.h instead of torch/extension.h
+// Similar to https://github.com/getkeops/keops/blob/3efd428b55c724b12f23982c06de00bc4d02d903/pykeops/torch_headers.h.in#L8
+// #include <torch/extension.h>
+#include <torch/python.h>
+#include <ATen/cuda/CUDAContext.h>  // For getCurrentCUDAStream
+#include <THC/THCAtomics.cuh>  // For atomicAdd on complex
+#include <ATen/native/cuda/block_reduce.cuh>
+#include <c10/util/complex.h>  // For scalar_value_type
+#include "map.h"  // For the MAP macro, i.e. for_each over the arguments
+
+
+#ifndef ITEMS_PER_THREAD_SYM_FWD_VALUES
+  #define ITEMS_PER_THREAD_SYM_FWD_VALUES {2, 4, 8, 16, 32, 32, 32, 64, 64, 64}
+#endif
+#ifndef MAX_BLOCK_SIZE_VALUE
+  #define MAX_BLOCK_SIZE_VALUE 256
+#endif
+#ifndef ITEMS_PER_THREAD_SYM_BWD_VALUE
+  #define ITEMS_PER_THREAD_SYM_BWD_VALUE 32
+#endif
+
+static constexpr int ITEMS_PER_THREAD_SYM_FWD[] = ITEMS_PER_THREAD_SYM_FWD_VALUES;
+static constexpr int MAX_BLOCK_SIZE = MAX_BLOCK_SIZE_VALUE;
+static constexpr int ITEMS_PER_THREAD_SYM_BWD = ITEMS_PER_THREAD_SYM_BWD_VALUE;
+
+template <typename T, size_t N>
+using CudaAcsr = at::GenericPackedTensorAccessor<T, N, at::RestrictPtrTraits, int32_t>;
+constexpr __host__ __device__ int div_up_const(int a, int b) { return (a + b - 1) / b; }
+
+__host__ __device__ static inline int div_up(int a, int b) { return (a + b - 1) / b;}
+
+template <typename scalar_t, int log_N,
+            int items_per_thread=ITEMS_PER_THREAD_SYM_FWD[log_N - 1]>
+__global__ void cauchy_mult_sym_fwd_cuda_kernel(CudaAcsr<scalar_t, 2> v,
+                                                CudaAcsr<scalar_t, 1> z,
+                                                CudaAcsr<scalar_t, 2> w,
+                                                CudaAcsr<scalar_t, 2> out,
+                                                int L) {
+  // Get the float type from the complex type
+  // https://github.com/pytorch/pytorch/blob/bceb1db885cafa87fe8d037d8f22ae9649a1bba0/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp#L213
+  using float_t = typename at::scalar_value_type<scalar_t>::type;
+  constexpr int N = 1 << log_N;
+  constexpr int blockDimx = div_up_const(N, items_per_thread);
+  constexpr int blockDimy = MAX_BLOCK_SIZE / blockDimx;
+  // We just want a shared array:
+  // __shared__ scalar_t s_b[16];
+  // But it doesn't work for complex: https://github.com/pytorch/pytorch/issues/39270
+  // So we declare a char array and cast it.
+  // The casting is subtle: https://stackoverflow.com/questions/12692310/convert-array-to-two-dimensional-array-by-pointer
+  __shared__ char v_smem_char[N * sizeof(scalar_t)];
+  scalar_t *v_smem = (scalar_t *)&v_smem_char;
+  __shared__ char w_smem_char[N * sizeof(scalar_t)];
+  scalar_t *w_smem = (scalar_t *)&w_smem_char;
+  __shared__ char out_smem_char[blockDimy * sizeof(scalar_t)];
+  scalar_t *out_smem = (scalar_t *)&out_smem_char;
+  int batch_idx = blockIdx.x;
+  int tid = threadIdx.x + threadIdx.y * blockDim.x;
+  int L_idx = blockIdx.y * blockDim.y + threadIdx.y;
+  int L_block_start = blockIdx.y * blockDim.y;
+  scalar_t z_t = L_block_start + threadIdx.y < L ? z[L_block_start + threadIdx.y] : scalar_t(0.f);
+  for (int N_idx = threadIdx.x + threadIdx.y * blockDim.x; N_idx < N; N_idx += blockDim.x * blockDim.y) {
+    v_smem[N_idx] = v[batch_idx][N_idx];
+    w_smem[N_idx] = w[batch_idx][N_idx];
+  }
+  __syncthreads();
+  scalar_t result = 0;
+  if (L_idx < L) {
+    // Combining the two terms (a/b + c/d = (ad + bc)/(bd)) seems to increase numerical errors.
+    // Using nvcc --use_fast_math yields the same speed between the two versions.
+    // So we don't combine the two terms.
+    #pragma unroll
+    for (int item = 0; item < items_per_thread; ++item) {
+      int N_idx = item * blockDimx + threadIdx.x;
+      scalar_t v_t = v_smem[N_idx], w_t = w_smem[N_idx];
+      result += v_t / (z_t - w_t) + std::conj(v_t) / (z_t - std::conj(w_t));
+    }
+  }
+  // TODO: this only works for N a power of 2
+  #pragma unroll
+  for (int offset = blockDimx / 2; offset > 0; offset /= 2) {
+    result += WARP_SHFL_DOWN(result, offset);
+  }
+  if ((threadIdx.x == 0) && (L_idx < L)) {
+    out_smem[threadIdx.y] = result;
+  }
+  __syncthreads();
+  if (tid < blockDim.y && L_block_start + tid < L) {
+    out[batch_idx][L_block_start + tid] = out_smem[tid];
+  }
+}
+
+torch::Tensor cauchy_mult_sym_fwd_cuda(torch::Tensor v,
+                                       torch::Tensor z,
+                                       torch::Tensor w) {
+  const int batch_size = v.size(0);
+  const int N = v.size(1);
+  const int L = z.size(0);
+  auto out = torch::empty({batch_size, L}, torch::dtype(v.dtype()).device(v.device()));
+  auto stream = at::cuda::getCurrentCUDAStream();
+  using scalar_t = c10::complex<float>;
+  const auto v_a = v.packed_accessor32<scalar_t, 2, at::RestrictPtrTraits>();
+  const auto z_a = z.packed_accessor32<scalar_t, 1, at::RestrictPtrTraits>();
+  const auto w_a = w.packed_accessor32<scalar_t, 2, at::RestrictPtrTraits>();
+  auto out_a = out.packed_accessor32<scalar_t, 2, at::RestrictPtrTraits>();
+  int log_N = int(log2((double) N));
+  int block_x = div_up(N, ITEMS_PER_THREAD_SYM_FWD[log_N - 1]);
+  dim3 block(block_x, MAX_BLOCK_SIZE / block_x);
+  dim3 grid(batch_size, div_up(L, block.y));
+  switch (log_N) {
+    #define CASE_LOG_N(log_N_val) case log_N_val:                 \
+    cauchy_mult_sym_fwd_cuda_kernel<scalar_t, log_N_val>          \
+      <<<grid, block, 0, stream>>>(v_a, z_a, w_a, out_a, L); break;
+    MAP(CASE_LOG_N, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
+  }
+  #undef CASE_LOG_N
+  C10_CUDA_KERNEL_LAUNCH_CHECK();
+  return out;
+}
+
+template <typename scalar_t, bool check_L_boundary>
+__global__ void cauchy_mult_sym_bwd_cuda_kernel(CudaAcsr<scalar_t, 2> v,
+                                                CudaAcsr<scalar_t, 1> z,
+                                                CudaAcsr<scalar_t, 2> w,
+                                                CudaAcsr<scalar_t, 2> dout,
+                                                CudaAcsr<scalar_t, 3> dv,
+                                                CudaAcsr<scalar_t, 3> dw,
+                                                int L,
+                                                int L_chunk_size) {
+  // We just want a shared array:
+  // __shared__ scalar_t s_b[16];
+  // But it doesn't work for complex: https://github.com/pytorch/pytorch/issues/39270
+  // So we declare a char array and cast it.
+  // The casting is subtle: https://stackoverflow.com/questions/12692310/convert-array-to-two-dimensional-array-by-pointer
+  __shared__ char dv_smem_char[C10_WARP_SIZE * sizeof(scalar_t)];
+  scalar_t *dv_smem = (scalar_t *)&dv_smem_char;
+  __shared__ char dw_smem_char[C10_WARP_SIZE * sizeof(scalar_t)];
+  scalar_t *dw_smem = (scalar_t *)&dw_smem_char;
+  int batch_idx = blockIdx.x;
+  int N_idx = blockIdx.y;
+  int L_chunk_idx = blockIdx.z;
+  int tid = threadIdx.x;
+  scalar_t w_conj_t = std::conj(w[batch_idx][N_idx]);
+  scalar_t dv_t = 0;
+  scalar_t dw_t = 0;
+  #pragma unroll
+  for (int item = 0; item < ITEMS_PER_THREAD_SYM_BWD; ++item) {
+    int l = L_chunk_idx * L_chunk_size + item * blockDim.x + threadIdx.x;
+    scalar_t dout_t, z_t;
+    if (check_L_boundary) {
+      dout_t = l < L ? dout[batch_idx][l] : 0;
+      z_t = l < L ? z[l] : 1;
+    } else { // Not checking boundary can speed it up quite a bit, around 30%.
+      dout_t = dout[batch_idx][l];
+      z_t = z[l];
+    }
+    scalar_t denom_1 = std::conj(z_t) - w_conj_t;
+    scalar_t denom_2 = z_t - w_conj_t;
+    scalar_t term_1 = dout_t / denom_1;
+    scalar_t term_2 = std::conj(dout_t) / denom_2;
+    dv_t += term_1 + term_2;
+    dw_t += term_1 / denom_1 + term_2 / denom_2;
+  }
+  dv_t = at::native::cuda_utils::BlockReduceSum<scalar_t>(dv_t, dv_smem);
+  dw_t = at::native::cuda_utils::BlockReduceSum<scalar_t>(dw_t, dw_smem);
+  if (tid == 0) {
+    dw[batch_idx][N_idx][L_chunk_idx] = dw_t * std::conj(v[batch_idx][N_idx]);
+    dv[batch_idx][N_idx][L_chunk_idx] = dv_t;
+  }
+}
+
+std::tuple<torch::Tensor, torch::Tensor>
+cauchy_mult_sym_bwd_cuda(torch::Tensor v,
+                         torch::Tensor z,
+                         torch::Tensor w,
+                         torch::Tensor dout) {
+  const int batch_size = v.size(0);
+  const int N = v.size(1);
+  const int L = z.size(0);
+  constexpr int MAX_BLOCK_SIZE = 1024;
+  constexpr int MAX_L_CHUNK_SIZE = ITEMS_PER_THREAD_SYM_BWD * MAX_BLOCK_SIZE;
+  const int n_L_chunks = div_up(L, MAX_L_CHUNK_SIZE);
+  auto dv = torch::empty({batch_size, N, n_L_chunks}, torch::dtype(v.dtype()).device(v.device()));
+  auto dw = torch::empty({batch_size, N, n_L_chunks}, torch::dtype(w.dtype()).device(w.device()));
+  auto stream = at::cuda::getCurrentCUDAStream();
+  using scalar_t = c10::complex<float>;
+  const auto v_a = v.packed_accessor32<scalar_t, 2, at::RestrictPtrTraits>();
+  const auto z_a = z.packed_accessor32<scalar_t, 1, at::RestrictPtrTraits>();
+  const auto w_a = w.packed_accessor32<scalar_t, 2, at::RestrictPtrTraits>();
+  const auto dout_a = dout.packed_accessor32<scalar_t, 2, at::RestrictPtrTraits>();
+  auto dv_a = dv.packed_accessor32<scalar_t, 3, at::RestrictPtrTraits>();
+  auto dw_a = dw.packed_accessor32<scalar_t, 3, at::RestrictPtrTraits>();
+  // Each block need to have a multiple of 32 threads, otherwise
+  // at::native::cuda_utils::BlockReduceSum to produce wrong result.
+  // int block_x = max(div_up(L, ITEMS_PER_THREAD_SYM_BWD), C10_WARP_SIZE);
+  const int L_chunk_size = min(L, MAX_L_CHUNK_SIZE);
+  int block_x = div_up(L_chunk_size, ITEMS_PER_THREAD_SYM_BWD * C10_WARP_SIZE) * C10_WARP_SIZE;
+  bool check_L_boundary = L != block_x * ITEMS_PER_THREAD_SYM_BWD * n_L_chunks;
+  dim3 block(block_x);
+  dim3 grid(batch_size, N, n_L_chunks);
+  check_L_boundary
+    ? cauchy_mult_sym_bwd_cuda_kernel<scalar_t, true>
+      <<<grid, block, 0, stream>>>(v_a, z_a, w_a, dout_a, dv_a, dw_a, L, L_chunk_size)
+    : cauchy_mult_sym_bwd_cuda_kernel<scalar_t, false>
+      <<<grid, block, 0, stream>>>(v_a, z_a, w_a, dout_a, dv_a, dw_a, L, L_chunk_size);
+  C10_CUDA_KERNEL_LAUNCH_CHECK();
+  return std::make_tuple(dv.sum(-1), dw.sum(-1));
+}
+
+template <int log_N, int items_per_thread=ITEMS_PER_THREAD_SYM_FWD[log_N - 1]>
+__global__ void vand_log_mult_sym_fwd_cuda_kernel(CudaAcsr<c10::complex<float>, 2> v,
+                                                  CudaAcsr<c10::complex<float>, 2> x,
+                                                  CudaAcsr<float, 2> out,
+                                                  int L) {
+  using cfloat_t = typename c10::complex<float>;
+  constexpr int N = 1 << log_N;
+  constexpr int blockDimx = div_up_const(N, items_per_thread);
+  constexpr int blockDimy = MAX_BLOCK_SIZE / blockDimx;
+  // We just want a shared array:
+  // __shared__ cfloat_t s_b[16];
+  // But it doesn't work for complex: https://github.com/pytorch/pytorch/issues/39270
+  // So we declare a char array and cast it.
+  // The casting is subtle: https://stackoverflow.com/questions/12692310/convert-array-to-two-dimensional-array-by-pointer
+  __shared__ char v_smem_char[N * sizeof(cfloat_t)];
+  cfloat_t *v_smem = (cfloat_t *)&v_smem_char;
+  __shared__ char x_smem_char[N * sizeof(cfloat_t)];
+  cfloat_t *x_smem = (cfloat_t *)&x_smem_char;
+  __shared__ float out_smem[blockDimy];
+  int batch_idx = blockIdx.x;
+  int tid = threadIdx.x + threadIdx.y * blockDim.x;
+  int L_idx = blockIdx.y * blockDim.y + threadIdx.y;
+  int L_block_start = blockIdx.y * blockDim.y;
+  for (int N_idx = threadIdx.x + threadIdx.y * blockDim.x; N_idx < N; N_idx += blockDim.x * blockDim.y) {
+    v_smem[N_idx] = v[batch_idx][N_idx];
+    x_smem[N_idx] = x[batch_idx][N_idx];
+  }
+  __syncthreads();
+  float result = 0;
+  if (L_idx < L) {
+    #pragma unroll
+    for (int item = 0; item < items_per_thread; ++item) {
+      int N_idx = item * blockDimx + threadIdx.x;
+      cfloat_t v_t = v_smem[N_idx], x_t = x_smem[N_idx];
+      result += (std::exp(x_t * L_idx) * v_t).real_;
+    }
+  }
+  // TODO: this only works for N a power of 2
+  #pragma unroll
+  for (int offset = blockDimx / 2; offset > 0; offset /= 2) {
+    result += WARP_SHFL_DOWN(result, offset);
+  }
+  if ((threadIdx.x == 0) && (L_idx < L)) {
+    out_smem[threadIdx.y] = 2 * result;
+  }
+  __syncthreads();
+  if (tid < blockDim.y && L_block_start + tid < L) {
+    out[batch_idx][L_block_start + tid] = out_smem[tid];
+  }
+}
+
+torch::Tensor vand_log_mult_sym_fwd_cuda(torch::Tensor v, torch::Tensor x, int L) {
+  const int batch_size = v.size(0);
+  const int N = v.size(1);
+  auto opts = v.options();
+  auto out = torch::empty({batch_size, L}, opts.dtype(torch::kFloat32));
+  auto stream = at::cuda::getCurrentCUDAStream();
+  const auto v_a = v.packed_accessor32<c10::complex<float>, 2, at::RestrictPtrTraits>();
+  const auto x_a = x.packed_accessor32<c10::complex<float>, 2, at::RestrictPtrTraits>();
+  auto out_a = out.packed_accessor32<float, 2, at::RestrictPtrTraits>();
+  int log_N = int(log2((double) N));
+  int block_x = div_up(N, ITEMS_PER_THREAD_SYM_FWD[log_N - 1]);
+  dim3 block(block_x, MAX_BLOCK_SIZE / block_x);
+  dim3 grid(batch_size, div_up(L, block.y));
+  switch (log_N) {
+    #define CASE_LOG_N(log_N_val) case log_N_val:                 \
+    vand_log_mult_sym_fwd_cuda_kernel<log_N_val>          \
+      <<<grid, block, 0, stream>>>(v_a, x_a, out_a, L); break;
+    MAP(CASE_LOG_N, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
+  }
+  #undef CASE_LOG_N
+  C10_CUDA_KERNEL_LAUNCH_CHECK();
+  return out;
+}
+
+template <bool check_L_boundary>
+__global__ void vand_log_mult_sym_bwd_cuda_kernel(CudaAcsr<c10::complex<float>, 2> v,
+                                                  CudaAcsr<c10::complex<float>, 2> x,
+                                                  CudaAcsr<float, 2> dout,
+                                                  CudaAcsr<c10::complex<float>, 3> dv,
+                                                  CudaAcsr<c10::complex<float>, 3> dx,
+                                                  int L,
+                                                  int L_chunk_size) {
+  using cfloat_t = typename c10::complex<float>;
+  // We just want a shared array:
+  // __shared__ c10::complex<float> s_b[16];
+  // But it doesn't work for complex: https://github.com/pytorch/pytorch/issues/39270
+  // So we declare a char array and cast it.
+  // The casting is subtle: https://stackoverflow.com/questions/12692310/convert-array-to-two-dimensional-array-by-pointer
+  __shared__ char dv_smem_char[C10_WARP_SIZE * sizeof(cfloat_t)];
+  cfloat_t *dv_smem = (cfloat_t *)&dv_smem_char;
+  __shared__ char dx_smem_char[C10_WARP_SIZE * sizeof(cfloat_t)];
+  cfloat_t *dx_smem = (cfloat_t *)&dx_smem_char;
+  int batch_idx = blockIdx.x;
+  int N_idx = blockIdx.y;
+  int L_chunk_idx = blockIdx.z;
+  int tid = threadIdx.x;
+  cfloat_t x_t = x[batch_idx][N_idx];
+  cfloat_t dv_t = 0;
+  cfloat_t dx_t = 0;
+  #pragma unroll
+  for (int item = 0; item < ITEMS_PER_THREAD_SYM_BWD; ++item) {
+    int l = L_chunk_idx * L_chunk_size + item * blockDim.x + threadIdx.x;
+    float dout_t;
+    if (check_L_boundary) {
+      dout_t = l < L ? dout[batch_idx][l] : 0;
+    } else { // Not checking boundary can speed it up quite a bit.
+      dout_t = dout[batch_idx][l];
+    }
+    // Need to conjugate as we're doing complex gradient.
+    cfloat_t do_exp_x_t = dout_t * std::conj(std::exp(x_t * l));
+    dv_t += do_exp_x_t;
+    dx_t += do_exp_x_t * l;
+  }
+  dv_t = at::native::cuda_utils::BlockReduceSum<cfloat_t>(dv_t, dv_smem);
+  dx_t = at::native::cuda_utils::BlockReduceSum<cfloat_t>(dx_t, dx_smem);
+  if (tid == 0) {
+    dx[batch_idx][N_idx][L_chunk_idx] = 2 * dx_t * std::conj(v[batch_idx][N_idx]);
+    dv[batch_idx][N_idx][L_chunk_idx] = 2 * dv_t;
+  }
+}
+
+
+std::tuple<torch::Tensor, torch::Tensor>
+vand_log_mult_sym_bwd_cuda(torch::Tensor v,
+                           torch::Tensor x,
+                           torch::Tensor dout) {
+  const int batch_size = v.size(0);
+  const int N = v.size(1);
+  const int L = dout.size(1);
+  constexpr int MAX_BLOCK_SIZE = 1024;
+  constexpr int MAX_L_CHUNK_SIZE = ITEMS_PER_THREAD_SYM_BWD * MAX_BLOCK_SIZE;
+  const int n_L_chunks = div_up(L, MAX_L_CHUNK_SIZE);
+  auto dv = torch::empty({batch_size, N, n_L_chunks}, torch::dtype(v.dtype()).device(v.device()));
+  auto dx = torch::empty({batch_size, N, n_L_chunks}, torch::dtype(x.dtype()).device(x.device()));
+  auto stream = at::cuda::getCurrentCUDAStream();
+  using cfloat_t = c10::complex<float>;
+  const auto v_a = v.packed_accessor32<cfloat_t, 2, at::RestrictPtrTraits>();
+  const auto x_a = x.packed_accessor32<cfloat_t, 2, at::RestrictPtrTraits>();
+  const auto dout_a = dout.packed_accessor32<float, 2, at::RestrictPtrTraits>();
+  auto dv_a = dv.packed_accessor32<cfloat_t, 3, at::RestrictPtrTraits>();
+  auto dx_a = dx.packed_accessor32<cfloat_t, 3, at::RestrictPtrTraits>();
+  // Each block need to have a multiple of 32 threads, otherwise
+  // at::native::cuda_utils::BlockReduceSum to produce wrong result.
+  // int block_x = max(div_up(L, ITEMS_PER_THREAD_SYM_BWD), C10_WARP_SIZE);
+  const int L_chunk_size = min(L, MAX_L_CHUNK_SIZE);
+  int block_x = div_up(L_chunk_size, ITEMS_PER_THREAD_SYM_BWD * C10_WARP_SIZE) * C10_WARP_SIZE;
+  bool check_L_boundary = L != block_x * ITEMS_PER_THREAD_SYM_BWD * n_L_chunks;
+  dim3 block(block_x);
+  dim3 grid(batch_size, N, n_L_chunks);
+  check_L_boundary
+    ? vand_log_mult_sym_bwd_cuda_kernel<true>
+      <<<grid, block, 0, stream>>>(v_a, x_a, dout_a, dv_a, dx_a, L, L_chunk_size)
+    : vand_log_mult_sym_bwd_cuda_kernel<false>
+      <<<grid, block, 0, stream>>>(v_a, x_a, dout_a, dv_a, dx_a, L, L_chunk_size);
+  C10_CUDA_KERNEL_LAUNCH_CHECK();
+  return std::make_tuple(dv.sum(-1), dx.sum(-1));
+}
diff --git a/extensions/kernels/map.h b/extensions/kernels/map.h
new file mode 100644
index 0000000..f2dcdb3
--- /dev/null
+++ b/extensions/kernels/map.h
@@ -0,0 +1,72 @@
+// Downloaded from https://github.com/swansontec/map-macro
+
+/*
+ * Copyright (C) 2012 William Swanson
+ *
+ * Permission is hereby granted, free of charge, to any person
+ * obtaining a copy of this software and associated documentation
+ * files (the "Software"), to deal in the Software without
+ * restriction, including without limitation the rights to use, copy,
+ * modify, merge, publish, distribute, sublicense, and/or sell copies
+ * of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be
+ * included in all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
+ * CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Except as contained in this notice, the names of the authors or
+ * their institutions shall not be used in advertising or otherwise to
+ * promote the sale, use or other dealings in this Software without
+ * prior written authorization from the authors.
+ */
+
+#ifndef MAP_H_INCLUDED
+#define MAP_H_INCLUDED
+
+#define EVAL0(...) __VA_ARGS__
+#define EVAL1(...) EVAL0(EVAL0(EVAL0(__VA_ARGS__)))
+#define EVAL2(...) EVAL1(EVAL1(EVAL1(__VA_ARGS__)))
+#define EVAL3(...) EVAL2(EVAL2(EVAL2(__VA_ARGS__)))
+#define EVAL4(...) EVAL3(EVAL3(EVAL3(__VA_ARGS__)))
+#define EVAL(...)  EVAL4(EVAL4(EVAL4(__VA_ARGS__)))
+
+#define MAP_END(...)
+#define MAP_OUT
+#define MAP_COMMA ,
+
+#define MAP_GET_END2() 0, MAP_END
+#define MAP_GET_END1(...) MAP_GET_END2
+#define MAP_GET_END(...) MAP_GET_END1
+#define MAP_NEXT0(test, next, ...) next MAP_OUT
+#define MAP_NEXT1(test, next) MAP_NEXT0(test, next, 0)
+#define MAP_NEXT(test, next)  MAP_NEXT1(MAP_GET_END test, next)
+
+#define MAP0(f, x, peek, ...) f(x) MAP_NEXT(peek, MAP1)(f, peek, __VA_ARGS__)
+#define MAP1(f, x, peek, ...) f(x) MAP_NEXT(peek, MAP0)(f, peek, __VA_ARGS__)
+
+#define MAP_LIST_NEXT1(test, next) MAP_NEXT0(test, MAP_COMMA next, 0)
+#define MAP_LIST_NEXT(test, next)  MAP_LIST_NEXT1(MAP_GET_END test, next)
+
+#define MAP_LIST0(f, x, peek, ...) f(x) MAP_LIST_NEXT(peek, MAP_LIST1)(f, peek, __VA_ARGS__)
+#define MAP_LIST1(f, x, peek, ...) f(x) MAP_LIST_NEXT(peek, MAP_LIST0)(f, peek, __VA_ARGS__)
+
+/**
+ * Applies the function macro `f` to each of the remaining parameters.
+ */
+#define MAP(f, ...) EVAL(MAP1(f, __VA_ARGS__, ()()(), ()()(), ()()(), 0))
+
+/**
+ * Applies the function macro `f` to each of the remaining parameters and
+ * inserts commas between the results.
+ */
+#define MAP_LIST(f, ...) EVAL(MAP_LIST1(f, __VA_ARGS__, ()()(), ()()(), ()()(), 0))
+
+#endif
diff --git a/extensions/kernels/setup.py b/extensions/kernels/setup.py
new file mode 100644
index 0000000..38bc4f3
--- /dev/null
+++ b/extensions/kernels/setup.py
@@ -0,0 +1,25 @@
+from setuptools import setup
+import torch.cuda
+from torch.utils.cpp_extension import CppExtension, CUDAExtension, BuildExtension
+from torch.utils.cpp_extension import CUDA_HOME
+
+ext_modules = []
+if torch.cuda.is_available() and CUDA_HOME is not None:
+    extension = CUDAExtension(
+        'structured_kernels', [
+            'cauchy.cpp',
+            'cauchy_cuda.cu',
+        ],
+        extra_compile_args={'cxx': ['-g', '-march=native', '-funroll-loops'],
+                            # 'nvcc': ['-O2', '-lineinfo']
+                            'nvcc': ['-O2', '-lineinfo', '--use_fast_math']
+                            }
+    )
+    ext_modules.append(extension)
+
+setup(
+    name='structured_kernels',
+    version="0.1.0",
+    ext_modules=ext_modules,
+    # cmdclass={'build_ext': BuildExtension.with_options(use_ninja=False)})
+    cmdclass={'build_ext': BuildExtension})
diff --git a/extensions/kernels/test_cauchy.py b/extensions/kernels/test_cauchy.py
new file mode 100644
index 0000000..2632bec
--- /dev/null
+++ b/extensions/kernels/test_cauchy.py
@@ -0,0 +1,77 @@
+import math
+import torch
+
+import pytest
+
+from einops import rearrange
+
+from cauchy import cauchy_mult_torch, cauchy_mult_keops, cauchy_mult
+
+
+def generate_data(batch_size, N, L, symmetric=True, device='cuda'):
+    if not symmetric:
+        v = torch.randn(batch_size, N, dtype=torch.complex64, device=device, requires_grad=True)
+        w = torch.randn(batch_size, N, dtype=torch.complex64, device=device, requires_grad=True)
+        z = torch.randn(L, dtype=torch.complex64, device=device)
+    else:
+        assert N % 2 == 0
+        v_half = torch.randn(batch_size, N // 2, dtype=torch.complex64, device=device)
+        v = torch.cat([v_half, v_half.conj()], dim=-1).requires_grad_(True)
+        w_half = torch.randn(batch_size, N // 2, dtype=torch.complex64, device=device)
+        w = torch.cat([w_half, w_half.conj()], dim=-1).requires_grad_(True)
+        z = torch.exp(1j * torch.randn(L, dtype=torch.float32, device=device))
+    return v, z, w
+
+
+def grad_to_half_grad(dx):
+    dx_half, dx_half_conj = dx.chunk(2, dim=-1)
+    return dx_half + dx_half_conj.conj()
+
+
+@pytest.mark.parametrize('L', [3, 17, 489, 2**10, 1047, 2**11, 2**12, 2**13, 2**14, 2**18])
+@pytest.mark.parametrize('N', [4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048])
+def test_cauchy_mult_symmetric(N, L):
+    # rtol, atol = (1e-4, 1e-4) if N <= 64 and L <= 1024 else(1e-3, 1e-3)
+    atol = 1e-4
+    tol_factor = 2.0  # Our error shouldn't be this much higher than Keops' error
+    device = 'cuda'
+    batch_size = 4
+    torch.random.manual_seed(2357)
+    v, z, w = generate_data(batch_size, N, L, symmetric=True, device=device)
+    v_half = v[:, :N // 2].clone().detach().requires_grad_(True)
+    w_half = w[:, :N // 2].clone().detach().requires_grad_(True)
+    # out_torch = cauchy_mult_torch(v, z, w, symmetric=True)
+    out_torch = cauchy_mult_torch(v.cdouble(), z.cdouble(), w.cdouble(), symmetric=True).cfloat()
+    out_keops = cauchy_mult_keops(v, z, w)
+    out = cauchy_mult(v_half, z, w_half)
+    relerr_out_keops = (out_keops - out_torch).abs() / out_torch.abs()
+    relerr_out = (out - out_torch).abs() / out_torch.abs()
+
+    dout = torch.randn_like(out)
+    dv_torch, dw_torch = torch.autograd.grad(out_torch, (v, w), dout, retain_graph=True)
+    dv_torch, dw_torch = dv_torch[:, :N // 2], dw_torch[:, :N // 2]
+    dv_keops, dw_keops = torch.autograd.grad(out_keops, (v, w), dout, retain_graph=True)
+    dv_keops, dw_keops = grad_to_half_grad(dv_keops), grad_to_half_grad(dw_keops)
+    dv, dw = torch.autograd.grad(out, (v_half, w_half), dout, retain_graph=True)
+    relerr_dv_keops = (dv_keops - dv_torch).abs() / dv_torch.abs()
+    relerr_dv = (dv - dv_torch).abs() / dv_torch.abs()
+    relerr_dw_keops = (dw_keops - dw_torch).abs() / dw_torch.abs()
+    relerr_dw = (dw - dw_torch).abs() / dw_torch.abs()
+    print(f'Keops out relative error: max {relerr_out_keops.amax().item():.6f}, mean {relerr_out_keops.mean().item():6f}')
+    print(f'out relative error: max {relerr_out.amax().item():.6f}, mean {relerr_out.mean().item():.6f}')
+    print(f'Keops dv relative error: max {relerr_dv_keops.amax().item():.6f}, mean {relerr_dv_keops.mean().item():6f}')
+    print(f'dv relative error: max {relerr_dv.amax().item():.6f}, mean {relerr_dv.mean().item():.6f}')
+    print(f'Keops dw relative error: max {relerr_dw_keops.amax().item():.6f}, mean {relerr_dw_keops.mean().item():6f}')
+    print(f'dw relative error: max {relerr_dw.amax().item():.6f}, mean {relerr_dw.mean().item():.6f}')
+    assert (relerr_out.amax() <= relerr_out_keops.amax() * tol_factor + atol)
+    assert (relerr_out.mean() <= relerr_out_keops.mean() * tol_factor + atol)
+    # assert torch.allclose(out, out_torch, rtol=rtol, atol=atol)
+    # assert torch.allclose(out, out_keops, rtol=rtol, atol=atol)
+    assert (relerr_dv.amax() <= relerr_dv_keops.amax() * tol_factor + atol)
+    assert (relerr_dv.mean() <= relerr_dv_keops.mean() * tol_factor + atol)
+    assert (relerr_dw.amax() <= relerr_dw_keops.amax() * tol_factor + atol)
+    assert (relerr_dw.mean() <= relerr_dw_keops.mean() * tol_factor + atol)
+    # assert torch.allclose(dv, dv_torch, rtol=1e-4, atol=1e-4)
+    # assert torch.allclose(dv, dv_keops, rtol=1e-4, atol=1e-4)
+    # assert torch.allclose(dw, dw_torch, rtol=1e-4, atol=1e-4)
+    # assert torch.allclose(dw, dw_keops, rtol=1e-4, atol=1e-4)
diff --git a/extensions/kernels/test_vandermonde.py b/extensions/kernels/test_vandermonde.py
new file mode 100644
index 0000000..4ffff61
--- /dev/null
+++ b/extensions/kernels/test_vandermonde.py
@@ -0,0 +1,44 @@
+import math
+
+import torch
+import torch.nn.functional as F
+import pytest
+
+from einops import rearrange
+
+from src.ops.vandermonde import log_vandermonde, log_vandermonde_fast
+
+
+@pytest.mark.parametrize('L', [3, 17, 489, 2**10, 1047, 2**11, 2**12])
+@pytest.mark.parametrize('N', [4, 8, 16, 32, 64, 128, 256])
+# @pytest.mark.parametrize('L', [2048])
+# @pytest.mark.parametrize('N', [64])
+def test_vand_mult_symmetric(N, L):
+    assert log_vandermonde_fast is not None, 'cauchy extension is not installed'
+    rtol, atol = (1e-4, 1e-4) if N <= 64 and L <= 1024 else(1e-3, 1e-3)
+    device = 'cuda'
+    batch_size = 4
+    torch.random.manual_seed(2357)
+    v = torch.randn(batch_size, N // 2, dtype=torch.cfloat, device=device, requires_grad=True)
+    x = (0.001 * torch.rand(batch_size, N // 2, device=device)
+         + 1j * N * torch.rand(batch_size, N // 2, device=device))
+    x.requires_grad_()
+    v_keops = v.detach().clone().requires_grad_()
+    x_keops = x.detach().clone().requires_grad_()
+    out_keops = log_vandermonde(v_keops, x_keops, L)
+    out = log_vandermonde_fast(v, x, L)
+    err_out = (out - out_keops).abs()
+
+    dout = torch.randn_like(out)
+    dv_keops, dx_keops = torch.autograd.grad(out_keops, (v_keops, x_keops), dout, retain_graph=True)
+    dv, dx = torch.autograd.grad(out, (v, x), dout, retain_graph=True)
+    err_dv = (dv - dv_keops).abs()
+    err_dx = (dx - dx_keops).abs()
+
+    print(f'out error: max {err_out.amax().item():.6f}, mean {err_out.mean().item():.6f}')
+    print(f'dv error: max {err_dv.amax().item():.6f}, mean {err_dv.mean().item():.6f}')
+    print(f'dx relative error: max {err_dx.amax().item():.6f}, mean {err_dx.mean().item():.6f}')
+
+    assert torch.allclose(out, out_keops, rtol=rtol, atol=atol)
+    assert torch.allclose(dv, dv_keops, rtol=rtol, atol=atol)
+    assert torch.allclose(dx, dx_keops, rtol=rtol, atol=atol)
diff --git a/extensions/kernels/tune_cauchy.py b/extensions/kernels/tune_cauchy.py
new file mode 100644
index 0000000..2cf862b
--- /dev/null
+++ b/extensions/kernels/tune_cauchy.py
@@ -0,0 +1,63 @@
+import math
+import json
+import argparse
+import itertools
+from pathlib import Path
+
+from tuner import KernelTuner
+
+
+def forward_params_list(N):
+    blocksize_params = ('MAX_BLOCK_SIZE_VALUE', [64, 128, 256, 512, 1024])
+    thread_value_default = [2, 4, 8, 16, 32, 32, 32, 32, 32, 32]
+    thread_values_supported = [2, 4, 8, 16, 32, 64, 128]
+    log_N_half = int(math.log2(N)) - 1
+    thread_values = []
+    for val in thread_values_supported:
+        if val <= N // 2:
+            array = list(thread_value_default)
+            array[log_N_half - 1] = val
+            thread_values.append('{' + ', '.join(str(v) for v in array) + '}')
+    thread_params = ('ITEMS_PER_THREAD_SYM_FWD_VALUES', thread_values)
+    value_prod = itertools.product(thread_params[1], blocksize_params[1])
+    params_list = [{thread_params[0]: value[0], blocksize_params[0]: value[1]}
+                   for value in value_prod]
+    return params_list
+
+
+def backward_params_list(L):
+    thread_value_supported = [8, 16, 32, 64, 128]
+    thread_params = ('ITEMS_PER_THREAD_SYM_BWD_VALUE', [v for v in thread_value_supported
+                                                        if (L + v - 1) // v <= 1024])
+    params_list = [{thread_params[0]: value} for value in thread_params[1]]
+    return params_list
+
+
+parser = argparse.ArgumentParser(description='Tuning Cauchy multiply')
+parser.add_argument('--mode', default='forward', choices=['forward', 'backward'])
+parser.add_argument('-N', default=64, type=int)
+parser.add_argument('-L', default=2 ** 14, type=int)
+parser.add_argument('--filename', default='tuning_result.json')
+
+
+if __name__ == '__main__':
+    args = parser.parse_args()
+
+    extension_dir = Path(__file__).absolute().parent
+    source_files = ['cauchy_cuda.cu']
+    if args.mode == 'forward':
+        params_list = forward_params_list(args.N)
+        tuner = KernelTuner(extension_dir, source_files, params_list,
+                            benchmark_script='benchmark_cauchy_tune.py',
+                            benchmark_args=['--mode', 'forward', '-N', str(args.N), '-L', '16384'],
+                            npool=16)
+    else:
+        params_list = backward_params_list(args.L)
+        tuner = KernelTuner(extension_dir, source_files, params_list,
+                            benchmark_script='benchmark_cauchy_tune.py',
+                            benchmark_args=['--mode', 'backward', '-N', '64', '-L', str(args.L)],
+                            npool=16)
+
+    result = tuner.tune()
+    with open(args.filename, 'w') as f:
+        json.dump(result, f)
diff --git a/extensions/kernels/tune_cauchy.sh b/extensions/kernels/tune_cauchy.sh
new file mode 100755
index 0000000..564eb62
--- /dev/null
+++ b/extensions/kernels/tune_cauchy.sh
@@ -0,0 +1,15 @@
+#!/usr/bin/env bash
+python tune_cauchy.py --mode forward -N 4 --filename tuning_result_fwd_N_4.json
+python tune_cauchy.py --mode forward -N 8 --filename tuning_result_fwd_N_8.json
+python tune_cauchy.py --mode forward -N 16 --filename tuning_result_fwd_N_16.json
+python tune_cauchy.py --mode forward -N 32 --filename tuning_result_fwd_N_32.json
+python tune_cauchy.py --mode forward -N 64 --filename tuning_result_fwd_N_64.json
+python tune_cauchy.py --mode forward -N 128 --filename tuning_result_fwd_N_128.json
+python tune_cauchy.py --mode forward -N 256 --filename tuning_result_fwd_N_256.json
+python tune_cauchy.py --mode forward -N 512 --filename tuning_result_fwd_N_512.json
+
+python tune_cauchy.py --mode backward -L 1024 --filename tuning_result_bwd_L_1k.json
+python tune_cauchy.py --mode backward -L 2048 --filename tuning_result_bwd_L_2k.json
+python tune_cauchy.py --mode backward -L 4096 --filename tuning_result_bwd_L_4k.json
+python tune_cauchy.py --mode backward -L 8192 --filename tuning_result_bwd_L_8k.json
+python tune_cauchy.py --mode backward -L 16384 --filename tuning_result_bwd_L_16k.json
diff --git a/extensions/kernels/tuner.py b/extensions/kernels/tuner.py
new file mode 100644
index 0000000..ed911c5
--- /dev/null
+++ b/extensions/kernels/tuner.py
@@ -0,0 +1,182 @@
+import os
+import shutil
+import subprocess
+import sys
+# import tempfile
+# import importlib
+import random
+import string
+import json
+
+
+from functools import partial
+from multiprocessing import Pipe, Pool, Process
+from pathlib import Path
+
+from tqdm import tqdm
+
+import numpy as np
+
+
+def read_file(filename):
+    """ return the contents of the file named filename or None if file not found """
+    if os.path.isfile(filename):
+        with open(filename, 'r') as f:
+            return f.read()
+
+
+def write_file(filename, string):
+    """dump the contents of string to a file called filename"""
+    with open(filename, 'w', encoding="utf-8") as f:
+        f.write(string)
+
+
+def prepare_kernel_string(kernel_string, params):
+    for k, v in params.items():
+        kernel_string = "#define " + k + " " + str(v) + "\n" + kernel_string
+    return kernel_string
+
+
+def compile_extension(temp_dir, install=False, verbose=True):
+    # Need to copy this process's environments, otherwise it can't find the compilers
+    env = {**os.environ,
+           'TUNING_SOURCE_DIR': str(temp_dir),
+           'TUNING_EXTENSION_NAME': str(temp_dir.stem)}
+    # https://stackoverflow.com/questions/53173314/how-to-change-distutils-output-directory
+    # Need separate build directories for parallel compilation
+    output = subprocess.run(
+        # [sys.executable, "tuning_setup.py", 'build', f'--build-base={str(temp_dir)}',
+        #  f'--build-lib={str(temp_dir)}'],
+        [sys.executable, "tuning_setup.py", 'build' if not install else 'develop'],
+        cwd=temp_dir,
+        env=env,
+        capture_output=True,
+        # check=True
+    )
+    if verbose:
+        print(output)
+        print('Done compiling' if not install else 'Done installing')
+
+
+def uninstall_extensions(tuning_extension_names, verbose=True):
+    # Need to copy this process's environments, otherwise it can't find the compilers
+    env = {**os.environ}
+    output = subprocess.run(
+        [sys.executable, '-m', 'pip', 'uninstall', '-y', *tuning_extension_names],
+        env=env,
+        capture_output=True,
+        # check=True
+    )
+    if verbose:
+        print(output)
+        print('Done uninstalling')
+
+
+def benchmark_extension(benchmark_script, *benchmark_args, verbose=True):
+    # Need to copy this process's environments, otherwise it can't find the compilers
+    env = os.environ
+    # https://stackoverflow.com/questions/53173314/how-to-change-distutils-output-directory
+    # Need separate build directories for parallel compilation
+    process = subprocess.run(
+        [sys.executable, benchmark_script, *benchmark_args],
+        env=os.environ,
+        capture_output=True,
+        # check=True
+    )
+    if verbose:
+        print(process)
+        print('Done benchmarking')
+    return json.loads(process.stdout.decode(sys.stdout.encoding))
+
+
+# def benchmark(connection, temp_dir):
+#     import torch
+#     # module = importlib.import_module(tuning_extension_name)
+#     torch.ops.load_library(temp_dir / 'torch_butterfly_tuning.so')
+#     batch_size = 1024
+#     n = 32
+#     twiddle = torch.randn(1, 1, 5, n // 2, 2, 2, device='cuda')
+#     input = torch.randn(batch_size, 1, n, device=twiddle.device)
+#     output = torch.ops.torch_butterfly.butterfly_multiply_fw(twiddle, input, True)
+#     # https://medium.com/@auro_227/timing-your-pytorch-code-fragments-e1a556e81f2
+#     res = []
+#     for _ in range(32):
+#         start = torch.cuda.Event(enable_timing=True)
+#         end = torch.cuda.Event(enable_timing=True)
+#         start.record()
+#         output = torch.ops.torch_butterfly.butterfly_multiply_fw(twiddle, input, True)
+#         end.record()
+#         torch.cuda.synchronize()
+#         res.append(start.elapsed_time(end))
+#     print(output.shape)
+#     res = np.array(res)
+#     connection.send((np.mean(res), np.std(res)))
+
+
+def set_up_tuning_temp_dir(params: dict, source_files, extension_dir, verbose=True):
+    if verbose:
+        print('params: ', params)
+    # TD [2021-10-22]: tempfile.mkdtemp sometimes create dir name with '_' in it, thus messing up
+    # the extension name.
+    # temp_dir = Path(tempfile.mkdtemp(prefix="temp_", dir=Path.cwd().parent)).absolute()
+    tuning_extension_name = 'temp_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k=10))
+    temp_dir = (Path.cwd().parent / tuning_extension_name).absolute()
+    if temp_dir.exists():
+        shutil.rmtree(temp_dir)  # shutil.copytree doesn't want directory that already exists
+    shutil.copytree(extension_dir, temp_dir)
+    sources = [temp_dir / name for name in source_files]
+    for kernel_source in sources:
+        ks = read_file(kernel_source)
+        ks = prepare_kernel_string(ks, params)
+        write_file(kernel_source, ks)
+    return temp_dir
+
+
+class KernelTuner:
+
+    def __init__(self, extension_dir, source_files, params_list, benchmark_script,
+                 benchmark_args, npool=8, verbose=True):
+        self.extension_dir = extension_dir
+        self.source_files = source_files
+        self.params_list = params_list
+        self.benchmark_script = benchmark_script
+        self.benchmark_args = benchmark_args
+        self.npool = npool
+        self.verbose = verbose
+
+    def tune(self):
+        temp_dirs = [set_up_tuning_temp_dir(params, self.source_files, self.extension_dir,
+                                            verbose=self.verbose)
+                     for params in self.params_list]
+        # Compile in parallel (for speed), then install sequentially to ensure correctness
+        with Pool(self.npool) as p:
+            p.map(compile_extension, temp_dirs)
+        # with Pool(1) as p:
+        #     p.map(partial(compile_extension, install=True), [temp_dirs])
+        for temp_dir in tqdm(temp_dirs):
+            try:
+                compile_extension(temp_dir, install=True)
+            except:
+                pass
+        # # We benchmark on a separate process so that they can import the extension that just got compiled.
+        # for params, temp_dir in params_tempdir:
+        #     print('Benchmarking: ', params)
+        #     recv_conn, send_conn = Pipe(duplex=False)
+        #     benchmark_process = Process(target=benchmark_fwd, args=(send_conn, str(temp_dir.stem)))
+        #     benchmark_process.start()
+        #     result = recv_conn.recv()
+        #     benchmark_process.join()
+        #     print('result', result)
+        results = []
+        for params, temp_dir in tqdm(list(zip(self.params_list, temp_dirs))):
+            try:
+                results.append((params,
+                                benchmark_extension(self.benchmark_script,
+                                                    *['--name', temp_dir.stem] + self.benchmark_args)))
+            except:
+                pass
+        print(results)
+        uninstall_extensions([temp_dir.stem for temp_dir in temp_dirs])
+        for temp_dir in temp_dirs:
+            shutil.rmtree(temp_dir)
+        return results
diff --git a/extensions/kernels/tuning_setup.py b/extensions/kernels/tuning_setup.py
new file mode 100644
index 0000000..795dbf4
--- /dev/null
+++ b/extensions/kernels/tuning_setup.py
@@ -0,0 +1,36 @@
+import os
+from setuptools import setup
+from pathlib import Path
+
+import torch.cuda
+from torch.utils.cpp_extension import CppExtension, CUDAExtension, BuildExtension
+from torch.utils.cpp_extension import CUDA_HOME
+
+
+extensions_dir = Path(os.getenv('TUNING_SOURCE_DIR')).absolute()
+assert extensions_dir.exists()
+source_files=[
+    'cauchy.cpp',
+    'cauchy_cuda.cu',
+]
+sources = [str(extensions_dir / name) for name in source_files]
+
+extension_name = os.getenv('TUNING_EXTENSION_NAME', default='cauchy_mult_tuning')
+ext_modules = []
+if torch.cuda.is_available() and CUDA_HOME is not None:
+    extension = CUDAExtension(
+        extension_name,
+        sources,
+        include_dirs=[extensions_dir],
+        extra_compile_args={'cxx': ['-g', '-march=native', '-funroll-loops'],
+                            # 'nvcc': ['-O2', '-lineinfo']
+                            'nvcc': ['-O2', '-lineinfo', '--use_fast_math']
+                            }
+    )
+    ext_modules.append(extension)
+
+setup(
+    name=extension_name,
+    ext_modules=ext_modules,
+    # cmdclass={'build_ext': BuildExtension.with_options(use_ninja=False)})
+    cmdclass={'build_ext': BuildExtension})
diff --git a/extensions/kernels/vandermonde.py b/extensions/kernels/vandermonde.py
new file mode 100644
index 0000000..a116e3e
--- /dev/null
+++ b/extensions/kernels/vandermonde.py
@@ -0,0 +1,45 @@
+import torch
+
+from structured_kernels import vand_log_mult_sym_fwd, vand_log_mult_sym_bwd
+
+def log_vandermonde_cuda(v, z, L):
+    """ Wrap the cuda method to deal with shapes """
+    v, z = torch.broadcast_tensors(v, z)
+    shape = v.shape
+
+    v = v.contiguous()
+    z = z.contiguous()
+
+    N = v.size(-1)
+    assert z.size(-1) == N
+    y = LogVandMultiplySymmetric.apply(v.view(-1, N), z.view(-1, N), L)
+    y = y.view(*shape[:-1], L)
+    return y
+
+class LogVandMultiplySymmetric(torch.autograd.Function):
+
+    @staticmethod
+    def forward(ctx, v, x, L):
+        batch, N = v.shape
+        supported_N_values = [1 << log_n for log_n in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]
+        if not N in supported_N_values:
+            raise NotImplementedError(f'Only support N values in {supported_N_values}')
+        max_L_value = 32 * 1024 * 64 * 1024
+        if L > max_L_value:
+            raise NotImplementedError(f'Only support L values <= {max_L_value}')
+        if not v.is_cuda and x.is_cuda:
+            raise NotImplementedError(f'Only support CUDA tensors')
+        ctx.save_for_backward(v, x)
+        return vand_log_mult_sym_fwd(v, x, L)
+
+    @staticmethod
+    def backward(ctx, dout):
+        v, x = ctx.saved_tensors
+        dv, dx = vand_log_mult_sym_bwd(v, x, dout)
+        return dv, dx, None
+
+
+if vand_log_mult_sym_fwd and vand_log_mult_sym_bwd is not None:
+    log_vandermonde_fast = LogVandMultiplySymmetric.apply
+else:
+    log_vandermonde_fast = None
diff --git a/generate.py b/generate.py
new file mode 100644
index 0000000..8876eaf
--- /dev/null
+++ b/generate.py
@@ -0,0 +1,213 @@
+import math
+import os
+import time
+# import warnings
+# warnings.filterwarnings("ignore")
+
+from functools import partial
+import multiprocessing as mp
+import torchaudio
+import numpy as np
+import torch
+import torch.nn as nn
+import hydra
+from omegaconf import DictConfig, OmegaConf
+from tqdm import tqdm
+# from torch.utils.tensorboard import SummaryWriter # If tensorboard is preferred over wandb
+
+from scipy.io.wavfile import write as wavwrite
+# from scipy.io.wavfile import read as wavread
+
+from models import construct_model
+from utils import find_max_epoch, print_size, calc_diffusion_hyperparams, local_directory, smooth_ckpt
+
+def sampling(net, bg, tv, m, mc):
+    """
+    Perform the complete sampling step according to p(x_0|x_T) = \prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)
+
+    Parameters:
+    net (torch network):            the model
+    size (tuple):                   size of tensor to be generated,
+                                    usually is (number of audios to generate, channels=1, length of audio)
+    diffusion_hyperparams (dict):   dictionary of diffusion hyperparameters returned by calc_diffusion_hyperparams
+                                    note, the tensors need to be cuda tensors
+
+    Returns:
+    the generated audio(s) in torch.tensor, shape=size
+    """
+
+    with torch.no_grad():        
+            
+        x, _ = net(
+            bg,
+            tv,
+            m,
+            mc
+        )  
+        
+    return x
+
+
+@torch.no_grad()
+def generate(
+        rank,
+        diffusion_cfg,
+        model_cfg,
+        dataset_cfg,
+        ckpt_iter="max",
+        n_samples=1, # Samples per GPU
+        name=None,
+        batch_size=None,
+        ckpt_smooth=None,
+        mel_path=None, mel_name=None,
+        dataloader=None,
+    ):
+    """
+    Generate audio based on ground truth mel spectrogram
+
+    Parameters:
+    output_directory (str):         checkpoint path
+    n_samples (int):                number of samples to generate, default is 4
+    ckpt_iter (int or 'max'):       the pretrained checkpoint to be loaded;
+                                    automatically selects the maximum iteration if 'max' is selected
+    mel_path, mel_name (str):       condition on spectrogram "{mel_path}/{mel_name}.wav.pt"
+    # dataloader:                     condition on spectrograms provided by dataloader
+    """
+
+    if rank is not None:
+        print(f"rank {rank} {torch.cuda.device_count()} GPUs")
+        torch.cuda.set_device(rank % torch.cuda.device_count())
+
+    local_path, output_directory = local_directory(name, model_cfg, dataset_cfg, 'waveforms')
+
+    # map diffusion hyperparameters to gpu
+
+    # predefine model
+    net = construct_model(model_cfg).cuda()
+    print_size(net)
+    net.eval()
+
+    # load checkpoint
+    print('ckpt_iter', ckpt_iter)
+    ckpt_path = os.path.join('exp', local_path, 'checkpoint')
+    if ckpt_iter == 'max':
+        ckpt_iter = find_max_epoch(ckpt_path)
+    ckpt_iter = int(ckpt_iter)
+
+    if ckpt_smooth is None:
+        try:
+            model_path = os.path.join(ckpt_path, '{}.pkl'.format(ckpt_iter))
+            checkpoint = torch.load(model_path, map_location='cpu')
+            net.load_state_dict(checkpoint['model_state_dict'])
+            print('Successfully loaded model at iteration {}'.format(ckpt_iter))
+        except:
+            raise Exception('No valid model found')
+    else:
+        state_dict = smooth_ckpt(ckpt_path, ckpt_smooth, ckpt_iter, alpha=None)
+        net.load_state_dict(state_dict)
+
+    # Add checkpoint number to output directory
+    output_directory = os.path.join(output_directory, str(ckpt_iter))
+    if rank == 0:
+        os.makedirs(output_directory, mode=0o775, exist_ok=True)
+        print("saving to output directory", output_directory)
+
+    if batch_size is None:
+        batch_size = n_samples
+    assert n_samples % batch_size == 0
+
+    # if mel_path is not None and mel_name is not None:
+    #     # use ground truth mel spec
+    #     try:
+    #         ground_truth_mel_name = os.path.join(mel_path, '{}.wav.pt'.format(mel_name))
+    #         ground_truth_mel_spectrogram = torch.load(ground_truth_mel_name).unsqueeze(0).cuda()
+    #     except:
+    #         raise Exception('No ground truth mel spectrogram found')
+    #     audio_length = ground_truth_mel_spectrogram.shape[-1] * dataset_cfg["hop_length"]
+    
+    dsp_paths = [f"dataset/testset/subject{i+1}" for i in range(8)]
+    test_sequences = [f"subject{i+1}" for i in range(8)]
+    # inference
+    generated_audio = []
+    use_gt_mean_condition = True
+    mean_condition_folder = False
+    result_folder = "results"
+    sr = 48000
+    for i in range(8):
+        dsp_path = dsp_paths[i]
+        output = f"{result_folder}/{test_sequences[i]}.wav"
+        mono, _ = torchaudio.load(f"{dsp_path}/mono.wav")
+        binaural, _ = torchaudio.load(f"{dsp_path}/binaural.wav")
+        binaural_geowarp, _ = torchaudio.load(f"{dsp_path}/binaural_geowarp.wav")
+        # receiver is fixed at origin in this dataset, so we only need transmitter view
+        tx_view = np.loadtxt(f"{dsp_path}/tx_positions.txt").transpose()
+        tx_view = torch.from_numpy(np.repeat(tx_view.T, 400, axis=0).T)
+
+        mean_condition_dsp = binaural_geowarp.mean(0, keepdim=True)
+
+        mean_condition_gt = binaural.mean(0, keepdim=True)
+
+        if use_gt_mean_condition:
+            mean_condition = mean_condition_gt
+        elif mean_condition_folder:
+            mean_condition, _ = torchaudio.load(f"{args.mean_condition_folder}/{args.output.strip('/').split('/')[-1]}")
+        else:
+            mean_condition = mean_condition_dsp
+
+
+        all_audio = []
+        clip_len = 16000
+        for i in tqdm(range(int(math.ceil(binaural_geowarp.shape[1] / clip_len)))):
+            with torch.no_grad():
+                bg = binaural_geowarp[:, clip_len * i : clip_len * (i + 1)].unsqueeze(0).cuda().float()
+                tv = tx_view[:, clip_len * i : clip_len * (i + 1)].unsqueeze(0).cuda().float()
+                m = mono[:, clip_len * i : clip_len * (i + 1)].unsqueeze(0).cuda().float()
+                mc = mean_condition[:, clip_len * i : clip_len * (i + 1)].unsqueeze(0).cuda().float()
+                audio, _ = net(
+                    bg,
+                    tv,
+                    m,
+                    mean_condition=mc
+                )
+            
+            audio = audio.cpu() # + binaural_geowarp[:, clip_len * i : clip_len * (i + 1)]
+            all_audio.append(audio)
+        generated_audio.append(all_audio)
+        concat_audio = torch.cat(all_audio, axis=-1).squeeze(0).cpu()
+        normalized_audio = torch.clamp(concat_audio, -1.0, 1.0)
+        pcm_s16_audio = (normalized_audio * 32767).to(torch.int16)
+
+        torchaudio.save(output, pcm_s16_audio, sample_rate=sr)
+
+
+    return generated_audio
+
+
+@hydra.main(version_base=None, config_path="configs/", config_name="config")
+def main(cfg: DictConfig) -> None:
+    print(OmegaConf.to_yaml(cfg))
+    OmegaConf.set_struct(cfg, False)  # Allow writing keys
+
+    num_gpus = torch.cuda.device_count()
+    generate_fn = partial(
+        generate,
+        model_cfg=cfg.model,
+        dataset_cfg=cfg.dataset,
+        **cfg.generate,
+    )
+
+    if num_gpus <= 1:
+        generate_fn(0)
+    else:
+        mp.set_start_method("spawn")
+        processes = []
+        for i in range(num_gpus):
+            p = mp.Process(target=generate_fn, args=(i,))
+            p.start()
+            processes.append(p)
+        for p in processes:
+            p.join()
+
+
+if __name__ == "__main__":
+    main()
\ No newline at end of file
diff --git a/infarance.py b/infarance.py
new file mode 100644
index 0000000..62ef164
--- /dev/null
+++ b/infarance.py
@@ -0,0 +1,91 @@
+from models.binaurals4 import BinauralS4
+import numpy as np
+import os
+import torch
+import torchaudio
+import math
+from tqdm import tqdm
+start = 0
+end = 247000
+hop_len = 10000
+for n_iter in range(end, start - 1, -hop_len):
+    print(n_iter)
+    ckpt_path = "/home/users/grad/2022/22t0007/exp"
+    name_ckpt = "/snet_d64_n8_pool_2-2_expand2_ff2_L32000half_mean_condition_batch4_mse"
+    PATH = f"/checkpoint/{n_iter}.pkl"
+    checkpoint = torch.load(ckpt_path + name_ckpt + PATH)
+    model = BinauralS4(n_layers=8, d_model=64, pool=[2, 2], use_mean_condition=True).cuda()
+    model.load_state_dict(checkpoint['model_state_dict'])
+    params = 0
+    for p in model.parameters():
+        if p.requires_grad:
+            params += p.numel()
+        
+    print(params)  # 121898
+    # model.load_state_dict(torch.load(ckpt_path + name_ckpt + PATH))  # PATH
+
+    base_path = "/home/users/grad/2022/22t0007/dataset/testset"
+    test_sequences = [f"subject{i+1}" for i in range(8)]  + ["validation_sequence"]
+
+    result_name = [""]
+    use_gt_mean_condition = False
+    mean_condition_folder = "/home/users/grad/2022/22t0007/out_stage1"
+    use_mean_condition = True
+
+    result_folder = ckpt_path + name_ckpt + f"/waveform/{n_iter}"
+    os.makedirs(result_folder, exist_ok=True)
+    sr = 48000
+    for test_sequence in test_sequences:
+        output = f"{result_folder}/{test_sequence}.wav"
+        mono, _ = torchaudio.load(base_path + f"/{test_sequence}/mono.wav")
+        binaural, _ = torchaudio.load(base_path + f"/{test_sequence}/binaural.wav")
+        binaural_geowarp, _ = torchaudio.load(base_path + f"/{test_sequence}/binaural_geowarp.wav")
+        # receiver is fixed at origin in this dataset, so we only need transmitter view
+        tx_view = np.loadtxt(base_path + f"/{test_sequence}/tx_positions.txt").transpose()
+        tx_view = torch.from_numpy(np.repeat(tx_view.T, 400, axis=0).T)
+
+        
+        if use_mean_condition:
+            mean_condition_dsp = binaural_geowarp.mean(0, keepdim=True)
+
+            mean_condition_gt = binaural.mean(0, keepdim=True)
+
+            if use_gt_mean_condition:
+                mean_condition = mean_condition_gt
+            elif mean_condition_folder:
+                mean_condition, _ = torchaudio.load(f"{mean_condition_folder}/{test_sequence}.wav")
+            else:
+                mean_condition = mean_condition_dsp
+        else:
+            mean_condition = None
+
+        all_audio = [] 
+        clip_len = 1000000
+        start = torch.cuda.Event(enable_timing=True)
+        end = torch.cuda.Event(enable_timing=True)
+        start.record()
+        for i in range(int(math.ceil(binaural_geowarp.shape[1] / clip_len))):
+            with torch.no_grad():
+                bg = binaural_geowarp[:, clip_len * i : clip_len * (i + 1)].unsqueeze(0).cuda().float()
+                tv = tx_view[:, clip_len * i : clip_len * (i + 1)].unsqueeze(0).cuda().float()
+                m = mono[:, clip_len * i : clip_len * (i + 1)].unsqueeze(0).cuda().float()
+                mc = mean_condition[:, clip_len * i : clip_len * (i + 1)].unsqueeze(0).cuda().float() if use_mean_condition else None
+                audio, _ = model(
+                    bg,
+                    tv,
+                    m,
+                    mean_condition=mc
+                )
+        
+            audio = audio.cpu() # + binaural_geowarp[:, clip_len * i : clip_len * (i + 1)]
+            all_audio.append(audio)
+        end.record()
+        torch.cuda.synchronize()
+        elapsed_time = start.elapsed_time(end)
+        print(elapsed_time / 1000, 'sec')
+        concat_audio = torch.cat(all_audio, axis=-1).squeeze(0).cpu()
+        normalized_audio = torch.clamp(concat_audio, -1.0, 1.0)
+        pcm_s16_audio = (normalized_audio * 32767).to(torch.int16)
+        torchaudio.save(output, pcm_s16_audio, sample_rate=sr)
+
+
diff --git a/losses.py b/losses.py
new file mode 100644
index 0000000..9d2ea90
--- /dev/null
+++ b/losses.py
@@ -0,0 +1,267 @@
+import numpy as np
+import torch as th
+import torch
+import torch.nn.functional as F
+import torchaudio as ta
+
+
+class STFTLoss(torch.nn.Module):
+    def __init__(self, n_fft, hop_length, win_length, window):
+        super(STFTLoss, self).__init__()
+        self.n_fft = n_fft
+        self.hop_length = hop_length
+        self.win_length = win_length
+        self.window = window
+
+    def forward(self, generated_audio, target_audio):
+        batch_size, channels, _ = generated_audio.size()
+        loss = 0.0
+
+        for i in range(batch_size):
+            for j in range(channels):
+                G = th.stft(
+                    generated_audio[i,j],
+                    n_fft=self.n_fft,
+                    hop_length=self.hop_length,
+                    win_length=self.win_length,
+                    window=self.window,
+                    return_complex=True
+                )
+        
+                T = th.stft(
+                    target_audio[i,j],
+                    n_fft=self.n_fft,
+                    hop_length=self.hop_length,
+                    win_length=self.win_length,
+                    window=self.window,
+                    return_complex=True
+                )
+
+                loss += F.mse_loss(G.abs(), T.abs())
+        
+        return loss / (batch_size * channels)
+
+
+
+class FourierTransform:
+    def __init__(self,
+                 fft_bins=2048,
+                 win_length_ms=40,
+                 frame_rate_hz=100,
+                 causal=False,
+                 preemphasis=0.0,
+                 sample_rate=48000,
+                 normalized=False):
+        self.sample_rate = sample_rate
+        self.frame_rate_hz = frame_rate_hz
+        self.preemphasis = preemphasis
+        self.fft_bins = fft_bins
+        self.win_length = int(sample_rate * win_length_ms / 1000)
+        self.hop_length = int(sample_rate / frame_rate_hz)
+        self.causal = causal
+        self.normalized = normalized
+        if self.win_length > self.fft_bins:
+            print('FourierTransform Warning: fft_bins should be larger than win_length')
+
+    def _convert_format(self, data, expected_dims):
+        if not type(data) == th.Tensor:
+            data = th.Tensor(data)
+        if len(data.shape) < expected_dims:
+            data = data.unsqueeze(0)
+        if not len(data.shape) == expected_dims:
+            raise Exception(f"FourierTransform: data needs to be a Tensor with {expected_dims} dimensions but got shape {data.shape}")
+        return data
+
+    def _preemphasis(self, audio):
+        if self.preemphasis > 0:
+            return th.cat((audio[:, 0:1], audio[:, 1:] - self.preemphasis * audio[:, :-1]), dim=1)
+        return audio
+
+    def _revert_preemphasis(self, audio):
+        if self.preemphasis > 0:
+            for i in range(1, audio.shape[1]):
+                audio[:, i] = audio[:, i] + self.preemphasis * audio[:, i-1]
+        return audio
+
+    def _magphase(self, complex_stft):
+        mag, phase = ta.functional.magphase(complex_stft, 1.0)
+        return mag, phase
+
+    def stft(self, audio):
+        '''
+        wrapper around th.stft
+        audio: wave signal as th.Tensor
+        '''
+        hann = th.hann_window(self.win_length)
+        hann = hann.cuda() if audio.is_cuda else hann
+        spec = th.stft(audio, n_fft=self.fft_bins, hop_length=self.hop_length, win_length=self.win_length,
+                       window=hann, center=not self.causal, normalized=self.normalized, return_complex=False)
+        return spec.contiguous()
+
+    def complex_spectrogram(self, audio):
+        '''
+        audio: wave signal as th.Tensor
+        return: th.Tensor of size channels x frequencies x time_steps (channels x y_axis x x_axis)
+        '''
+        self._convert_format(audio, expected_dims=2)
+        audio = self._preemphasis(audio)
+        return self.stft(audio)
+
+    def magnitude_phase(self, audio):
+        '''
+        audio: wave signal as th.Tensor
+        return: tuple containing two th.Tensor of size channels x frequencies x time_steps for magnitude and phase spectrum
+        '''
+        stft = self.complex_spectrogram(audio)
+        return self._magphase(stft)
+
+    def mag_spectrogram(self, audio):
+        '''
+        audio: wave signal as th.Tensor
+        return: magnitude spectrum as th.Tensor of size channels x frequencies x time_steps for magnitude and phase spectrum
+        '''
+        return self.magnitude_phase(audio)[0]
+
+    def power_spectrogram(self, audio):
+        '''
+        audio: wave signal as th.Tensor
+        return: power spectrum as th.Tensor of size channels x frequencies x time_steps for magnitude and phase spectrum
+        '''
+        return th.pow(self.mag_spectrogram(audio), 2.0)
+
+    def phase_spectrogram(self, audio):
+        '''
+        audio: wave signal as th.Tensor
+        return: phase spectrum as th.Tensor of size channels x frequencies x time_steps for magnitude and phase spectrum
+        '''
+        return self.magnitude_phase(audio)[1]
+
+    def mel_spectrogram(self, audio, n_mels):
+        '''
+        audio: wave signal as th.Tensor
+        n_mels: number of bins used for mel scale warping
+        return: mel spectrogram as th.Tensor of size channels x n_mels x time_steps for magnitude and phase spectrum
+        '''
+        spec = self.power_spectrogram(audio)
+        mel_warping = ta.transforms.MelScale(n_mels, self.sample_rate)
+        return mel_warping(spec)
+
+    def complex_spec2wav(self, complex_spec, length):
+        '''
+        inverse stft
+        complex_spec: complex spectrum as th.Tensor of size channels x frequencies x time_steps x 2 (real part/imaginary part)
+        length: length of the audio to be reconstructed (in frames)
+        '''
+        complex_spec = self._convert_format(complex_spec, expected_dims=4)
+        hann = th.hann_window(self.win_length)
+        hann = hann.cuda() if complex_spec.is_cuda else hann
+        wav = ta.functional.istft(complex_spec, n_fft=self.fft_bins, hop_length=self.hop_length, win_length=self.win_length, window=hann, length=length, center=not self.causal)
+        wav = self._revert_preemphasis(wav)
+        return wav
+
+    def magphase2wav(self, mag_spec, phase_spec, length):
+        '''
+        reconstruction of wav signal from magnitude and phase spectrum
+        mag_spec: magnitude spectrum as th.Tensor of size channels x frequencies x time_steps
+        phase_spec: phase spectrum as th.Tensor of size channels x frequencies x time_steps
+        length: length of the audio to be reconstructed (in frames)
+        '''
+        mag_spec = self._convert_format(mag_spec, expected_dims=3)
+        phase_spec = self._convert_format(phase_spec, expected_dims=3)
+        complex_spec = th.stack([mag_spec * th.cos(phase_spec), mag_spec * th.sin(phase_spec)], dim=-1)
+        return self.complex_spec2wav(complex_spec, length)
+
+
+
+
+class Loss(th.nn.Module):
+    def __init__(self, mask_beginning=0):
+        '''
+        base class for losses that operate on the wave signal
+        :param mask_beginning: (int) number of samples to mask at the beginning of the signal
+        '''
+        super().__init__()
+        self.mask_beginning = mask_beginning
+
+    def forward(self, data, target):
+        '''
+        :param data: predicted wave signals in a B x channels x T tensor
+        :param target: target wave signals in a B x channels x T tensor
+        :return: a scalar loss value
+        '''
+        data = data[..., self.mask_beginning:]
+        target = target[..., self.mask_beginning:]
+        return self._loss(data, target)
+
+    def _loss(self, data, target):
+        pass
+
+
+class L2Loss(Loss):
+    def _loss(self, data, target):
+        '''
+        :param data: predicted wave signals in a B x channels x T tensor
+        :param target: target wave signals in a B x channels x T tensor
+        :return: a scalar loss value
+        '''
+        return th.mean((data - target).pow(2))
+
+
+class AmplitudeLoss(Loss):
+    def __init__(self, sample_rate, mask_beginning=0):
+        '''
+        :param sample_rate: (int) sample rate of the audio signal
+        :param mask_beginning: (int) number of samples to mask at the beginning of the signal
+        '''
+        super().__init__(mask_beginning)
+        self.fft = FourierTransform(sample_rate=sample_rate)
+
+    def _transform(self, data):
+        return self.fft.stft(data.view(-1, data.shape[-1]))
+
+    def _loss(self, data, target):
+        '''
+        :param data: predicted wave signals in a B x channels x T tensor
+        :param target: target wave signals in a B x channels x T tensor
+        :return: a scalar loss value
+        '''
+        data, target = self._transform(data), self._transform(target)
+        data = th.sum(data**2, dim=-1) ** 0.5
+        target = th.sum(target**2, dim=-1) ** 0.5
+        return th.mean(th.abs(data - target))
+
+
+class PhaseLoss(Loss):
+    def __init__(self, sample_rate, mask_beginning=0, ignore_below=0.1):
+        '''
+        :param sample_rate: (int) sample rate of the audio signal
+        :param mask_beginning: (int) number of samples to mask at the beginning of the signal
+        '''
+        super().__init__(mask_beginning)
+        self.ignore_below = ignore_below
+        self.fft = FourierTransform(sample_rate=sample_rate)
+
+    def _transform(self, data):
+        return self.fft.stft(data.reshape(-1, data.shape[-1]))
+
+    def _loss(self, data, target):
+        '''
+        :param data: predicted wave signals in a B x channels x T tensor
+        :param target: target wave signals in a B x channels x T tensor
+        :return: a scalar loss value
+        '''
+        data, target = self._transform(data).view(-1, 2), self._transform(target).view(-1, 2)
+        # ignore low energy components for numerical stability
+        target_energy = th.sum(th.abs(target), dim=-1)
+        pred_energy = th.sum(th.abs(data.detach()), dim=-1)
+        target_mask = target_energy > self.ignore_below * th.mean(target_energy)
+        pred_mask = pred_energy > self.ignore_below * th.mean(target_energy)
+        indices = th.nonzero(target_mask * pred_mask).view(-1)
+        data, target = th.index_select(data, 0, indices), th.index_select(target, 0, indices)
+        # compute actual phase loss in angular space
+        data_angles, target_angles = th.atan2(data[:, 0], data[:, 1]), th.atan2(target[:, 0], target[:, 1])
+        loss = th.abs(data_angles - target_angles)
+        # positive + negative values in left part of coordinate system cause angles > pi
+        # => 2pi -> 0, 3/4pi -> 1/2pi, ... (triangle function over [0, 2pi] with peak at pi)
+        loss = np.pi - th.abs(loss - np.pi)
+        return th.mean(loss)
\ No newline at end of file
diff --git a/models/__init__.py b/models/__init__.py
new file mode 100644
index 0000000..8cf5414
--- /dev/null
+++ b/models/__init__.py
@@ -0,0 +1,17 @@
+from .binaurals4 import BinauralS4
+
+
+def construct_model(model_cfg):
+    name = model_cfg.pop("__name__")
+    model_cls = {
+        "binaurals4": BinauralS4,
+    }[name]
+    model = model_cls(**model_cfg)
+    model_cfg["__name__"] = name # restore
+    return model
+
+def model_identifier(model_cfg):
+    model_cls = {
+        "binaurals4": BinauralS4,
+    }[model_cfg.name]
+    return model_cls.name(model_cfg)
\ No newline at end of file
diff --git a/models/__pycache__/__init__.cpython-38.pyc b/models/__pycache__/__init__.cpython-38.pyc
new file mode 100644
index 0000000..7e66d81
Binary files /dev/null and b/models/__pycache__/__init__.cpython-38.pyc differ
diff --git a/models/__pycache__/binaurals4.cpython-38.pyc b/models/__pycache__/binaurals4.cpython-38.pyc
new file mode 100644
index 0000000..0fc8c40
Binary files /dev/null and b/models/__pycache__/binaurals4.cpython-38.pyc differ
diff --git a/models/__pycache__/s4.cpython-38.pyc b/models/__pycache__/s4.cpython-38.pyc
new file mode 100644
index 0000000..5821d10
Binary files /dev/null and b/models/__pycache__/s4.cpython-38.pyc differ
diff --git a/models/__pycache__/s4v2.cpython-38.pyc b/models/__pycache__/s4v2.cpython-38.pyc
new file mode 100644
index 0000000..2c9cd4a
Binary files /dev/null and b/models/__pycache__/s4v2.cpython-38.pyc differ
diff --git a/models/binaurals4.py b/models/binaurals4.py
new file mode 100644
index 0000000..6a36d3c
--- /dev/null
+++ b/models/binaurals4.py
@@ -0,0 +1,547 @@
+from models.s4 import LinearActivation, S4Block
+# from  import LinearActivation, S4Block
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from einops import rearrange
+
+
+
+class PositionEncoder(nn.Module):
+    def __init__(self, d_input, d_model):
+        super().__init__()
+        self.linear = nn.Linear(d_input, d_model)
+
+    def forward(self, x):
+        return self.linear(x)
+
+def Conv1d(*args, **kwargs):
+  layer = nn.Conv1d(*args, **kwargs)
+  nn.init.kaiming_normal_(layer.weight)
+  return layer
+
+class DownPool(nn.Module):
+    def __init__(self, d_input, expand, pool):
+        super().__init__()
+        self.d_out = d_input * expand
+        self.pool = pool
+        self.linear = LinearActivation(
+            d_input * pool,
+            self.d_out,
+            transposed=True
+        )
+    def forward(self, x):
+        # Reshape x to make the last dimension divisible by self.pool
+        # print(x.shape)
+        x = rearrange(x, '... h (l s) -> ... (h s) l', s=self.pool)
+
+        x = self.linear(x)
+        # print(x.shape)
+        return x, None
+    def step(self, x, state, **kwargs):
+        """
+        x: (..., H)
+        """
+        if x is None: return None, state
+        state.append(x)
+        if len(state) == self.pool:
+            x = rearrange(torch.stack(state, dim=-1), '... h s -> ... (h s)')
+            x = x.unsqueeze(-1)
+            x = self.linear(x)
+            x = x.squeeze(-1)
+            return x, []
+        else:
+            return None, state
+
+    def default_state(self, *args, **kwargs):
+        return []
+
+
+class UpPool(nn.Module):
+    def __init__(self, d_input, expand, pool):
+        super().__init__()
+        self.d_output = d_input // expand
+        self.pool = pool
+
+        self.linear = LinearActivation(
+            d_input,
+            self.d_output * pool,
+            transposed=True,
+        )
+    def forward(self, x, skip=None):
+
+        x = self.linear(x)
+
+        x = F.pad(x[..., :-1], (1, 0)) # Shift to ensure causality
+
+        x = rearrange(x, '... (h s) l -> ... h (l s)', s=self.pool)
+
+        if skip is not None:
+            x = x + skip
+
+        return x, None
+    def step(self, x, state, **kwargs):
+        """
+        x: (..., H)
+        """
+        assert len(state) > 0
+        y, state = state[0], state[1:]
+        if len(state) == 0:
+            assert x is not None
+            x = x.unsqueeze(-1)
+            x = self.linear(x)
+            x = x.squeeze(-1)
+            x = rearrange(x, '... (h s) -> ... h s', s=self.pool)
+            state = list(torch.unbind(x, dim=-1))
+        else: assert x is None
+        return y, state
+
+    def default_state(self, *batch_shape, device=None):
+        state = torch.zeros(batch_shape + (self.d_output, self.pool), device=device) # (batch, h, s)
+        state = list(torch.unbind(state, dim=-1)) # List of (..., H)
+        return state
+
+class FFBlock(nn.Module):
+
+    def __init__(self, d_model, expand=2, dropout=0.0):
+        """
+        Feed-forward block
+
+        Args:
+            d_model: dimension of input
+            expand: expansion factor for incerted bottleneck
+            dropout: dropout rate
+        """
+        super().__init__()
+
+        input_linear = LinearActivation(
+            d_model,
+            d_model * expand,
+            transposed=True,
+            activation="gelu",
+            activate=True,
+        )
+        dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()
+        output_linear = LinearActivation(
+            d_model * expand,
+            d_model,
+            transposed=True,
+            activation=None,
+            activate=False,
+        )
+        self.ff = nn.Sequential(
+            input_linear,
+            dropout,
+            output_linear,
+        )
+
+    def forward(self, x):
+        return self.ff(x), None
+
+    def default_state(self, *args, **kwargs):
+        return None
+
+    def step(self, x, state, **kwargs):
+        # expects (B, D, L)
+        return self.ff(x.unsqueeze(-1)).squeeze(-1), state
+
+class Conv(nn.Module):
+    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):
+        super(Conv, self).__init__()
+        self.padding = dilation * (kernel_size - 1) // 2
+        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation, padding=self.padding)
+        self.conv = nn.utils.weight_norm(self.conv)
+        nn.init.kaiming_normal_(self.conv.weight)
+
+    def forward(self, x):
+        out = self.conv(x)
+        return out
+
+class ZeroConv1d(nn.Module):
+    def __init__(self, in_channel, out_channel):
+        super(ZeroConv1d, self).__init__()
+        self.conv = nn.Conv1d(in_channel, out_channel, kernel_size=1, padding=0)
+        self.conv.weight.data.zero_()
+        self.conv.bias.data.zero_()
+
+    def forward(self, x):
+        out = self.conv(x)
+        return out
+
+class BinauralPreNet(nn.Module):
+    def __init__(self, n_mels, binaural_type="", addmono=False, use_mean_condition=False,
+          predict_mean_condition=False):
+        super().__init__()
+        self.conv_view1 = torch.nn.Conv1d(7, 20, 3, padding=1)
+        self.conv_view2 = torch.nn.Conv1d(20, 40, 3, padding=1)
+        self.addmono = addmono
+        self.use_mean_condition = use_mean_condition
+        self.predict_mean_condition = predict_mean_condition
+        if addmono:
+            self.conv_dsp1 = torch.nn.Conv1d(3 + (0 if not use_mean_condition else 1), 20, 3, padding=1)
+        else:
+            self.conv_dsp1 = torch.nn.Conv1d(2 if not use_mean_condition else 3, 20, 3, padding=1)
+        self.conv_dsp2 = torch.nn.Conv1d(20, 40, 3, padding=1)
+        self.conv = torch.nn.Conv1d(80, n_mels, 3, padding=1)
+
+
+    def forward(self, geowarp, view, mono, mean_condition):
+        
+        if self.addmono:
+            if self.use_mean_condition:
+                geowarp = torch.cat([geowarp, mono, mean_condition], axis=1)
+            else:
+                geowarp = torch.cat([geowarp, mono ], axis=1)
+        else:
+            geowarp = torch.cat([geowarp, mono], axis=1)
+        geowarp = self.conv_dsp1(geowarp)
+        geowarp = F.leaky_relu(geowarp, 0.4)
+        geowarp = self.conv_dsp2(geowarp)
+        geowarp = F.leaky_relu(geowarp, 0.4)
+
+        view = self.conv_view1(view)
+        view = F.leaky_relu(view, 0.4)
+        view = self.conv_view2(view)
+        view = F.leaky_relu(view, 0.4)
+
+        x = self.conv(torch.cat([geowarp, view], axis=1))
+        x = F.leaky_relu(x, 0.4)
+        return x
+    def step(self, geowarp, view, mono, mean_condition, state=None):
+        if self.addmono:
+            if self.use_mean_condition:
+                geowarp = torch.cat([geowarp, mono, mean_condition], axis=1)
+            else:
+                geowarp = torch.cat([geowarp, mono], axis=1)
+        geowarp = geowarp.unsqueeze(-1)
+        geowarp = self.conv_dsp1(geowarp)
+        geowarp = F.leaky_relu(geowarp, 0.4)
+        geowarp = self.conv_dsp2(geowarp)
+        geowarp = F.leaky_relu(geowarp, 0.4)
+
+        view = self.conv_view1(view)
+        view = F.leaky_relu(view, 0.4)
+        view = self.conv_view2(view)
+        view = F.leaky_relu(view, 0.4)
+
+        x = self.conv(torch.cat([geowarp, view], axis=1))
+        x = F.leaky_relu(x, 0.4)
+        return x.squeeze(-1), None
+    def default_state(self, *args, **kwargs):
+        return None
+
+class ResidualBlock(nn.Module):
+    def __init__(self, d_model, layer, dropout=0.0):
+        """
+        Residual S4 block
+
+        Args:
+            d_model: dimension of the model
+            layer: a layer config
+            dropout: dropout rate
+        """
+        super().__init__()
+
+        self.layer = layer
+        self.norm = nn.LayerNorm(d_model)
+        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()
+        self.linear = nn.Linear(d_model, 2*d_model)
+        self.conditioner_projection = Conv1d(80, d_model, 1)
+
+    def forward(self, x, condition=None):
+        """
+        Input x is shape (B, d_input, L)
+        """
+
+        z = x
+
+        # print(z.shape) # torch.Size([4, 1024, 25])
+        z = self.norm(z.transpose(-1, -2)).transpose(-1, -2)
+        if condition is not None:
+            condition = self.conditioner_projection(condition)
+            if condition.size(2) != z.size(2):
+                condition = F.avg_pool1d(condition, condition.size(2) // z.size(2))
+            z, _ = self.layer(z)
+            z = z + condition
+        else:
+            z, _ = self.layer(z)
+        z = F.gelu(z)
+        z = self.dropout(z)
+        z = z.transpose(1, 2)
+        #print(z.shape) # torch.Size([4, 1024, 25])
+        z = self.linear(z)
+        z = z.transpose(1, 2)
+        z = F.glu(z, dim=1)
+        # Residual connection
+        x = z + x
+
+        return x, None
+
+    def step(self, x, state, condition=None, **kwargs):
+        z = self.norm(x)
+        if condition is not None:
+            condition = self.conditioner_projection(condition.unsqueeze(-1))
+            z = z + condition.squeeze(-1)
+        z, state = self.layer.step(z, state, **kwargs)
+        z = F.gelu(z)
+        z = self.dropout(z)
+        z = z.unsqueeze(-1)
+        z = self.linear(z)
+        z = F.glu(z, dim=1)
+        x = z.squeeze(-1) + x
+        return x, state
+
+    def default_state(self, *args, **kwargs):
+        return self.layer.default_state(*args, **kwargs)
+
+
+
+class BinauralS4(nn.Module):
+    def __init__(
+        self,
+        d_model=64,
+        n_layers=8,
+        pool=[4, 4],
+        expand=2,
+        ff=2,
+        in_channels=2,
+        out_channels=2,
+        bidirectional=True,
+        unet=False,
+        use_mean_condition=True,
+        dropout=0.0,
+        **s4_args,
+    ):
+
+
+        super().__init__()
+        self.d_model = H = d_model
+        self.d_output = H
+        self.unet = unet
+        self.binaural_pre_net = BinauralPreNet(n_mels=80, binaural_type="", addmono=True,
+            use_mean_condition=use_mean_condition,
+            predict_mean_condition=False)
+
+        self.init_conv = nn.Sequential(Conv1d(in_channels, H, kernel_size=1), nn.ReLU())
+        # self.embedding = CustomEmbedding(d_input=512, d_output=d_model)
+        def s4_block(dim):
+            layer = S4Block(
+                d_model=dim,
+                d_state=64,
+                bidirectional=bidirectional,
+                dropout=dropout,
+                transposed=True,
+                **s4_args,
+            )
+            return ResidualBlock(
+                d_model=dim,
+                layer=layer,
+                dropout=dropout,
+            )
+        def ff_block(dim):
+            layer = FFBlock(
+                d_model=dim,
+                expand=ff,
+                dropout=dropout,
+            )
+            return ResidualBlock(
+                d_model=dim,
+                layer=layer,
+                dropout=dropout,
+            )
+
+        # Down blocks
+        d_layers = []
+        for p in pool:
+            if True:
+                # Add blocks in the down layers
+                for _ in range(n_layers):
+                    d_layers.append(s4_block(H))
+                    if ff > 0: d_layers.append(ff_block(H))
+
+            # Add sequence downsampling and feature expanding
+            d_layers.append(DownPool(H, expand, p))
+            H *= expand
+        # print(d_layers)
+
+        # Center block
+        c_layers = []
+        for _ in range(n_layers):
+            c_layers.append(s4_block(H))
+            if ff > 0: c_layers.append(ff_block(H))
+
+        # Up blocks
+        u_layers = []
+        for p in pool[::-1]:
+            block = []
+            H //= expand
+            block.append(UpPool(H * expand, expand, p))
+
+            for _ in range(n_layers):
+                block.append(s4_block(H))
+                if ff > 0: block.append(ff_block(H))
+
+            u_layers.append(nn.ModuleList(block))
+        d_layers_for_condition = []
+        for p in pool:
+            d_layers_for_condition.append(DownPool(H, expand, p))
+            H *= expand
+
+        for p in pool[::-1]:
+            H //= expand
+
+        self.d_layers = nn.ModuleList(d_layers)
+
+        self.c_layers = nn.ModuleList(c_layers)
+        self.u_layers = nn.ModuleList(u_layers)
+
+        self.norm = nn.LayerNorm(H)
+        self.final_conv = nn.Sequential(Conv1d(H, H, kernel_size=1),
+                                        nn.ReLU(),
+                                        ZeroConv1d(H, out_channels)
+                                        )
+        assert H == d_model
+
+    def forward(self, x, view, mono, mean_condition, state=None):
+        """
+        input: (batch, length, d_input)
+        output: (batch, length, d_output)
+        """
+        # embedding = self.embedding(view, mean_condition)
+
+        # print(x[0][:][:])
+
+
+        # Down blocks
+        """
+        input_length = x.shape[0] * x.shape[1] * x.shape[2]
+        print(input_length)
+        context_length = input_length // self.d_model
+        padding_length = context_length * self.d_model - input_length
+
+        if padding_length < 0:
+            x = F.pad(x, (0, -padding_length))
+
+
+        x =  x.view(x.shape[0], self.d_model, -1)
+        """
+
+
+        condition = self.binaural_pre_net(x, view, mono, mean_condition)
+
+        x = self.init_conv(x)
+
+        outputs = []
+        for layer in self.d_layers:
+            outputs.append(x)
+            if isinstance(layer, ResidualBlock):
+                x, _ = layer(x, condition=condition)
+            else:
+                x, _ = layer(x)
+        outputs.append(x)
+        # Center block
+        for layer in self.c_layers:
+            x, _  = layer(x, condition)
+
+        x = x + outputs.pop()  # + embedding # add a skip connection to the last out put of the down block
+
+
+        # Up blocks
+        for block in self.u_layers:
+            if self.unet:
+                for layer in block:
+                    x, _ = layer(x)
+                    x = x + outputs.pop() # skip connection
+            else:
+                for layer in block:
+                    if isinstance(layer, ResidualBlock):
+                        x, _ = layer(x, condition)
+                    else :
+                        x, _ = layer(x)
+                    x = x + outputs.pop() # add a skip connection from the input of the modeling part of this up block
+
+
+
+        x = x.transpose(1, 2) # (batch, length, expand)
+        x = self.norm(x)
+        x = x.transpose(1, 2)
+
+        x = self.final_conv(x)
+
+        return x, None # required to return state
+
+    def default_state(self, *args, **kwargs):
+        layers = list(self.d_layers) + list(self.c_layers) + [layer for block in self.u_layers for layer in block]
+        return [layer.default_state(*args, **kwargs) for layer in layers]
+
+    def step(self, x, state, view, mono, mean_condition):
+        condition, state = state[0], state[1:]
+        x, next_state = self.binaural_pre_net.step(x, view, mono, mean_condition, state=condition)
+        
+        x = self.init_conv(x)
+        
+        outputs = []
+        next_state = [next_state] # First state is for BinauralPreNet
+        for i, layer in enumerate(self.d_layers):
+            outputs.append(x)
+            if isinstance(layer, ResidualBlock):
+                x, _next_state = layer.step(x, condition=condition, state=state[i], **kwargs)
+            else:
+                x, _next_state = layer.step(x, state=state[i], **kwargs)
+            next_state.append(_next_state)
+            
+        outputs.append(x)
+        
+        for i, layer in enumerate(self.c_layers):
+            x, _next_state = layer.step(x, condition, state=state[len(self.d_layers) + i], **kwargs)
+            next_state.append(_next_state)
+        
+        x = x + outputs.pop()
+
+        for i, block in enumerate(self.u_layers):
+            if self.unet:
+                for j, layer in enumerate(block):
+                    x, _next_state = layer.step(x, state=state[len(self.d_layers) + len(self.c_layers) + i*len(block) + j], **kwargs)
+                    next_state.append(_next_state)
+                    x = x + outputs.pop()
+            else:
+                for j, layer in enumerate(block):
+                    if isinstance(layer, ResidualBlock):
+                        x, _next_state = layer.step(x, condition, state=state[len(self.d_layers) + len(self.c_layers) + i*len(block) + j], **kwargs)
+                    else :
+                        x, _next_state = layer.step(x, state=state[len(self.d_layers) + len(self.c_layers) + i*len(block) + j], **kwargs)
+                    next_state.append(_next_state)
+                    if isinstance(layer, UpPool):
+                        x = x + outputs.pop()
+                        outputs.append(x)
+                x = x + outputs.pop()
+
+        x = self.norm(x)
+        x = self.final_conv(x)
+        
+        return x, next_state
+    def setup_rnn(self, mode="dense"):
+        """
+        Convert the model to a RNN for autoregressive generation.
+        Args:
+            mode: S4 recurrence mode. Using `diagonal` can speed up generation by 10-20%.
+                `linear` should be faster theoretically but is slow in practice since it
+                dispatches more operations (could benefit from fused operations).
+                Note that `diagonal` could potentially be unstable if the diagonalization is numerically unstable
+                (although we haven't encountered this case in practice), while `dense` should always be stable.
+        """
+        assert mode in ["dense", "diagonal", "linear"]
+        for module in self.modules():
+            if hasattr(module, '_setup_step'): module._setup_step(mode=mode)
+    
+    @classmethod
+    def name(cls, cfg):
+        return "{}_d{}_n{}_pool_{}-{}_expand{}_ff{}".format(
+            "unet" if cfg["unet"] else "snet",
+            cfg["d_model"],
+            cfg["n_layers"],
+            len(cfg["pool"]),
+            cfg["pool"][0],
+            cfg["expand"],
+            cfg["ff"],
+        )
\ No newline at end of file
diff --git a/models/s4.py b/models/s4.py
new file mode 100644
index 0000000..adb7485
--- /dev/null
+++ b/models/s4.py
@@ -0,0 +1,1933 @@
+"""Standalone version of Structured State Space sequence model (S4)."""
+
+from collections import defaultdict
+from typing import Optional, Mapping, Tuple, Union
+import logging
+from functools import partial
+import math
+import numpy as np
+from scipy import special as ss
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from pytorch_lightning.utilities import rank_zero_only
+from einops import rearrange, repeat
+
+# Function aliases
+contract = torch.einsum
+
+_conj = lambda x: torch.cat([x, x.conj()], dim=-1)
+_c2r = torch.view_as_real
+_r2c = torch.view_as_complex
+if tuple(map(int, torch.__version__.split('.')[:2])) >= (1, 10):
+    _resolve_conj = lambda x: x.conj().resolve_conj()
+else:
+    _resolve_conj = lambda x: x.conj()
+
+
+def get_logger(name=__name__, level=logging.INFO) -> logging.Logger:
+    """Initializes multi-GPU-friendly python logger."""
+
+    logger = logging.getLogger(name)
+    logger.setLevel(level)
+
+    # this ensures all logging levels get marked with the rank zero decorator
+    # otherwise logs would get multiplied for each GPU process in multi-GPU setup
+    for level in ("debug", "info", "warning", "error", "exception", "fatal", "critical"):
+        setattr(logger, level, rank_zero_only(getattr(logger, level)))
+
+    return logger
+log = get_logger(__name__)
+
+"""Structured matrix kernels"""
+
+# Try CUDA extension
+try:
+    from extensions.kernels.cauchy import cauchy_mult as cauchy_cuda
+    from extensions.kernels.vandermonde import log_vandermonde_cuda
+    has_cuda_extension = True
+    log.info("CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) found.")
+except:
+    log.warning(
+        "CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled."
+    )
+    has_cuda_extension = False
+
+# Try pykeops
+try:
+    import pykeops
+    from pykeops.torch import Genred
+    has_pykeops = True
+    log.info("Pykeops installation found.")
+
+    def _broadcast_dims(*tensors):
+        max_dim = max([len(tensor.shape) for tensor in tensors])
+        tensors = [tensor.view((1,)*(max_dim-len(tensor.shape))+tensor.shape) for tensor in tensors]
+        return tensors
+
+    def cauchy_keops(v, z, w):
+        expr_num = 'z * ComplexReal(v) - Real2Complex(Sum(v * w))'
+        expr_denom = 'ComplexMult(z-w, z-Conj(w))'
+
+        cauchy_mult = Genred(
+            f'ComplexDivide({expr_num}, {expr_denom})',
+            [
+                'v = Vj(2)',
+                'z = Vi(2)',
+                'w = Vj(2)',
+            ],
+            reduction_op='Sum',
+            axis=1,
+        )
+
+        v, z, w = _broadcast_dims(v, z, w)
+        v = _c2r(v)
+        z = _c2r(z)
+        w = _c2r(w)
+
+        r = 2*cauchy_mult(v, z, w, backend='GPU')
+        return _r2c(r)
+
+    def log_vandermonde_keops(v, x, L):
+        expr = 'ComplexMult(v, ComplexExp(ComplexMult(x, l)))'
+        vandermonde_mult = Genred(
+            expr,
+            [
+                'v = Vj(2)',
+                'x = Vj(2)',
+                'l = Vi(2)',
+            ],
+            reduction_op='Sum',
+            axis=1,
+        )
+
+        l = torch.arange(L).to(x)
+        v, x, l = _broadcast_dims(v, x, l)
+        v = _c2r(v)
+        x = _c2r(x)
+        l = _c2r(l)
+
+        r = vandermonde_mult(v, x, l, backend='GPU')
+        return 2*_r2c(r).real
+
+    def log_vandermonde_transpose_keops(u, v, x, L):
+        """
+        u: ... H L
+        v: ... H N
+        x: ... H N
+        Returns: ... H N
+
+        V = Vandermonde(a, L) : (H N L)
+        contract_L(V * u * v)
+        """
+        expr = 'ComplexMult(ComplexMult(v, u), ComplexExp(ComplexMult(x, l)))'
+        vandermonde_mult = Genred(
+            expr,
+            [
+                'u = Vj(2)',
+                'v = Vi(2)',
+                'x = Vi(2)',
+                'l = Vj(2)',
+            ],
+            reduction_op='Sum',
+            axis=1,
+        )
+
+        l = torch.arange(L).to(x)
+        u, v, x, l = _broadcast_dims(u, v, x, l)
+        u = _c2r(u)
+        v = _c2r(v)
+        x = _c2r(x)
+        l = _c2r(l)
+
+        r = vandermonde_mult(u, v, x, l, backend='GPU')
+        return _r2c(r)
+
+except ImportError:
+    has_pykeops = False
+    if not has_cuda_extension:
+        log.warning(
+            "Falling back on slow Cauchy and Vandermonde kernel. Install at least one of pykeops or the CUDA extension for better speed and memory efficiency."
+        )
+
+# Fallback versions
+def cauchy_naive(v, z, w):
+    """
+    v: (..., N)
+    z: (..., L)
+    w: (..., N)
+    returns: (..., L) \sum v/(z-w)
+    """
+    v = _conj(v)
+    w = _conj(w)
+    cauchy_matrix = v.unsqueeze(-1) / (z.unsqueeze(-2) - w.unsqueeze(-1)) # (... N L)
+    return torch.sum(cauchy_matrix, dim=-2)
+
+def log_vandermonde_naive(v, x, L, conj=True):
+    """
+    v: (..., N)
+    x: (..., N)
+    returns: (..., L) \sum v x^l
+    """
+    vandermonde_matrix = torch.exp(x.unsqueeze(-1) * torch.arange(L).to(x)) # (... N L)
+    vandermonde_prod = contract('... n, ... n l -> ... l', v, vandermonde_matrix) # (... L)
+    return 2*vandermonde_prod.real
+
+def log_vandermonde_transpose_naive(u, v, x, L):
+    vandermonde_matrix = torch.exp(x.unsqueeze(-1) * torch.arange(L).to(x)) # (... N L)
+    vandermonde_prod = contract('... l, ... n, ... n l -> ... n', u.to(x), v.to(x), vandermonde_matrix) # (... L)
+    return vandermonde_prod
+
+
+
+""" Simple nn.Module components """
+
+def Activation(activation=None, dim=-1):
+    if activation in [ None, 'id', 'identity', 'linear' ]:
+        return nn.Identity()
+    elif activation == 'tanh':
+        return nn.Tanh()
+    elif activation == 'relu':
+        return nn.ReLU()
+    elif activation == 'gelu':
+        return nn.GELU()
+    elif activation == 'elu':
+        return nn.ELU()
+    elif activation in ['swish', 'silu']:
+        return nn.SiLU()
+    elif activation == 'glu':
+        return nn.GLU(dim=dim)
+    elif activation == 'sigmoid':
+        return nn.Sigmoid()
+    elif activation == 'softplus':
+        return nn.Softplus()
+    else:
+        raise NotImplementedError("hidden activation '{}' is not implemented".format(activation))
+
+def LinearActivation(
+        d_input, d_output, bias=True,
+        transposed=False,
+        activation=None,
+        activate=False, # Apply activation as part of this module
+        **kwargs,
+    ):
+    """Returns a linear nn.Module with control over axes order, initialization, and activation."""
+
+    # Construct core module
+    linear_cls = partial(nn.Conv1d, kernel_size=1) if transposed else nn.Linear
+    if activation is not None and activation == 'glu': d_output *= 2
+    linear = linear_cls(d_input, d_output, bias=bias, **kwargs)
+
+    if activate and activation is not None:
+        activation = Activation(activation, dim=-2 if transposed else -1)
+        linear = nn.Sequential(linear, activation)
+    return linear
+
+class DropoutNd(nn.Module):
+    def __init__(self, p: float = 0.5, tie=True, transposed=True):
+        """
+        tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)
+        """
+        super().__init__()
+        if p < 0 or p >= 1:
+            raise ValueError("dropout probability has to be in [0, 1), " "but got {}".format(p))
+        self.p = p
+        self.tie = tie
+        self.transposed = transposed
+        self.binomial = torch.distributions.binomial.Binomial(probs=1-self.p)
+
+    def forward(self, X):
+        """X: (batch, dim, lengths...)."""
+        if self.training:
+            if not self.transposed: X = rearrange(X, 'b ... d -> b d ...')
+            mask_shape = X.shape[:2] + (1,)*(X.ndim-2) if self.tie else X.shape
+            mask = torch.rand(*mask_shape, device=X.device) < 1.-self.p
+            X = X * mask * (1.0/(1-self.p))
+            if not self.transposed: X = rearrange(X, 'b d ... -> b ... d')
+            return X
+        return X
+
+"""Misc functional utilities"""
+
+def power(L, A, v=None):
+    """Compute A^L and the scan sum_i A^i v_i.
+
+    A: (..., N, N)
+    v: (..., N, L)
+    """
+
+    I = torch.eye(A.shape[-1]).to(A) # , dtype=A.dtype, device=A.device)
+
+    powers = [A]
+    l = 1
+    while True:
+        if L % 2 == 1: I = powers[-1] @ I
+        L //= 2
+        if L == 0: break
+        l *= 2
+        if v is None:
+            powers = [powers[-1] @ powers[-1]]
+        else:
+            powers.append(powers[-1] @ powers[-1])
+
+    if v is None: return I
+
+    # Invariants:
+    # powers[-1] := A^l
+    # l := largest po2 at most L
+
+    # Note that an alternative divide and conquer to compute the reduction is possible and can be embedded into the above loop without caching intermediate powers of A
+    # We do this reverse divide-and-conquer for efficiency reasons:
+    # 1) it involves fewer padding steps for non-po2 L
+    # 2) it involves more contiguous arrays
+
+    # Take care of edge case for non-po2 arrays
+    # Note that this initial step is a no-op for the case of power of 2 (l == L)
+    k = v.size(-1) - l
+    v_ = powers.pop() @ v[..., l:]
+    v = v[..., :l]
+    v[..., :k] = v[..., :k] + v_
+
+    # Handle reduction for power of 2
+    while v.size(-1) > 1:
+        v = rearrange(v, '... (z l) -> ... z l', z=2)
+        v = v[..., 0, :] + powers.pop() @ v[..., 1, :]
+    return I, v.squeeze(-1)
+
+
+"""HiPPO utilities"""
+
+def transition(measure, N, **measure_args):
+    """A, B transition matrices for different measures.
+
+    measure: the type of measure
+      legt - Legendre (translated)
+      legs - Legendre (scaled)
+      glagt - generalized Laguerre (translated)
+      lagt, tlagt - previous versions of (tilted) Laguerre with slightly different normalization
+    """
+    # Legendre (translated)
+    if measure == 'legt':
+        Q = np.arange(N, dtype=np.float64)
+        R = (2*Q + 1) ** .5
+        j, i = np.meshgrid(Q, Q)
+        A = R[:, None] * np.where(i < j, (-1.)**(i-j), 1) * R[None, :]
+        B = R[:, None]
+        A = -A
+
+        # Halve again for timescale correctness
+        A *= 0.5
+        B *= 0.5
+    # Legendre (scaled)
+    elif measure == 'legs':
+        q = np.arange(N, dtype=np.float64)
+        col, row = np.meshgrid(q, q)
+        r = 2 * q + 1
+        M = -(np.where(row >= col, r, 0) - np.diag(q))
+        T = np.sqrt(np.diag(2 * q + 1))
+        A = T @ M @ np.linalg.inv(T)
+        B = np.diag(T)[:, None]
+        B = B.copy() # Otherwise "UserWarning: given NumPY array is not writeable..." after torch.as_tensor(B)
+    elif measure in ['fourier', 'fout']:
+        freqs = np.arange(N//2)
+        d = np.stack([np.zeros(N//2), freqs], axis=-1).reshape(-1)[1:]
+        A = np.pi*(-np.diag(d, 1) + np.diag(d, -1))
+        B = np.zeros(N)
+        B[0::2] = 2**.5
+        B[0] = 1
+
+        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case
+        A = A - B[:, None] * B[None, :]
+        B = B[:, None]
+    else:
+        raise NotImplementedError
+
+    return A, B
+
+def rank_correction(measure, N, rank=1, dtype=torch.float):
+    """Return low-rank matrix L such that A + L is normal."""
+
+    if measure == 'legs':
+        assert rank >= 1
+        P = torch.sqrt(.5+torch.arange(N, dtype=dtype)).unsqueeze(0) # (1 N)
+    elif measure == 'legt':
+        assert rank >= 2
+        P = torch.sqrt(1+2*torch.arange(N, dtype=dtype)) # (N)
+        P0 = P.clone()
+        P0[0::2] = 0.
+        P1 = P.clone()
+        P1[1::2] = 0.
+        P = torch.stack([P0, P1], dim=0) # (2 N)
+        P *= 2**(-0.5) # Halve the rank correct just like the original matrix was halved
+    elif measure in ['fourier', 'fout']:
+        P = torch.zeros(N)
+        P[0::2] = 2**.5
+        P[0] = 1
+        P = P.unsqueeze(0)
+    else: raise NotImplementedError
+
+    d = P.size(0)
+    if rank > d:
+        P = torch.cat([P, torch.zeros(rank-d, N, dtype=dtype)], dim=0) # (rank N)
+    return P
+
+def nplr(measure, N, rank=1, dtype=torch.float, diagonalize_precision=True, B_clip=2.0, **kwargs):
+    """Constructs NPLR form of HiPPO matrices.
+
+    Returns w, p, q, V, B such that
+    (w - p q^*, B) is unitarily equivalent to the original HiPPO A, B by the matrix V
+    i.e. A = V[w - p q^*]V^*, B = V B
+
+    measure: Name of HiPPO method.
+    N: Size of recurrent A matrix (also known as `d_state` elsewhere).
+    dtype: Single or double precision.
+    diagonalize_precision: Calculate diagonalization in double precision.
+    B_clip: Clip values of B, can help with stability. None for no clipping.
+    """
+
+    assert dtype == torch.float or dtype == torch.double
+    cdtype = torch.cfloat if dtype == torch.float else torch.cdouble
+
+    A, B = transition(measure, N)
+    A = torch.as_tensor(A, dtype=dtype) # (N, N)
+    B = torch.as_tensor(B, dtype=dtype)[:, 0] # (N,)
+
+    P = rank_correction(measure, N, rank=rank, dtype=dtype) # (r N)
+    AP = A + torch.sum(P.unsqueeze(-2)*P.unsqueeze(-1), dim=-3)
+
+    # We require AP to be nearly skew-symmetric
+    _A = AP + AP.transpose(-1, -2)
+    if (err := torch.sum((_A - _A[0,0]*torch.eye(N))**2) / N) > 1e-5: # if not torch.allclose(_A - _A[0,0]*torch.eye(N), torch.zeros(N, N), atol=1e-5):
+        print("WARNING: HiPPO matrix not skew symmetric", err)
+
+
+    # Take advantage of identity + skew-symmetric form to calculate real and imaginary parts separately
+    # Imaginary part can use eigh instead of eig
+    W_re = torch.mean(torch.diagonal(AP), -1, keepdim=True)
+
+    # Diagonalize in double precision
+    if diagonalize_precision: AP = AP.to(torch.double)
+    # w, V = torch.linalg.eig(AP) # (..., N) (..., N, N)
+    W_im, V = torch.linalg.eigh(AP*-1j) # (..., N) (..., N, N)
+    if diagonalize_precision: W_im, V = W_im.to(cdtype), V.to(cdtype)
+    W = W_re + 1j * W_im
+    # Check: V W V^{-1} = A
+    # print("check", V @ torch.diag_embed(W) @ V.conj().transpose(-1, -2))
+
+
+    # Only keep half of each conjugate pair
+    _, idx = torch.sort(W.imag)
+    W_sorted = W[idx]
+    V_sorted = V[:, idx]
+
+    # There is an edge case when eigenvalues can be 0, which requires some machinery to handle
+    # We use a huge hack here: Assume only one pair is 0, and that it is the first row/column of A (only happens in Fourier case)
+    V = V_sorted[:, :N//2]
+    W = W_sorted[:N//2]  # Only keep negative imaginary components
+    assert W[-2].abs() > 1e-4, "Only 1 zero eigenvalue allowed in diagonal part of A"
+    if W[-1].abs() < 1e-4:
+        V[:, -1] = 0.
+        V[0, -1] = 2**-0.5
+        V[1, -1] = 2**-0.5 * 1j
+
+    _AP = V @ torch.diag_embed(W) @ V.conj().transpose(-1, -2)
+    if ((err := torch.sum((2*_AP.real-AP)**2)/N) > 1e-5):
+        print("Warning: Diagonalization of A matrix not numerically precise - error", err)
+    # print("check", V @ torch.diag_embed(W) @ V.conj().transpose(-1, -2))
+
+    V_inv = V.conj().transpose(-1, -2)
+
+    # C = initial_C(measure, N, dtype=dtype)
+    B = contract('ij, j -> i', V_inv, B.to(V)) # V^* B
+    # C = contract('ij, j -> i', V_inv, C.to(V)) # V^* C
+    P = contract('ij, ...j -> ...i', V_inv, P.to(V)) # V^* P
+
+    if B_clip is not None:
+        B = B.real + 1j*torch.clamp(B.imag, min=-B_clip, max=B_clip)
+
+    # W represents the imaginary part of the DPLR form: A = W - PP^*
+    # Downstream classes just call this A for simplicity,
+    # which is also more consistent with the diagonal case
+    return W, P, B, V
+
+def dplr(
+    init='hippo',
+    N=64, rank=1, H=1,
+    dtype=torch.float,
+    real_random=False,
+    real_scale=1.0,
+    imag_random=False,
+    imag_scale=1.0,
+    B_random=False,
+    B_init='constant',
+    B_scale=1.0,
+    P_scale=1.0,
+    normalize=False,
+):
+    """Directly construct a DPLR matrix.
+
+    Args:
+    - init: (str) ['rand', 'lin', inv', 'real', 'hippo'] Choices for initialization of A.
+          Most of these affect the imaginary part of A, except for 'real'.
+    - real_random: (bool) Initialize A.real in -U[0, 1]. Otherwise, initialize to -1/2.
+    - real_scale: (float) Scaling factor of real part of A.
+    - imag_random: (bool) Initialize A.imag randomly.
+    - imag_scale: (bool) Scaling factor of imaginary part of A.
+    - B_init: (str) ['constant' | 'random' | 'alternating' | 'unit-cw' | 'unit-ccw' | 'hippo']
+          Choices for initialization of B.
+    - B_scale: (float) Scaling factor for B
+    - P_scale: (float) Scaling factor for P
+    - normalize: (bool) Apply an automatic normalization factor on B
+    """
+    assert dtype == torch.float or dtype == torch.double
+    dtype = torch.cfloat if dtype == torch.float else torch.cdouble
+
+    pi = torch.tensor(math.pi)
+
+    # Construct real part of diagonal A (must be non-negative)
+    if real_random:
+        real_part = torch.rand(H, N//2)
+    else:
+        real_part = .5 * torch.ones(H, N//2)
+    real_part = real_scale * real_part
+
+    # Construct imaginary part of diagonal A (must be non-negative)
+    if imag_random:
+        imag_part = N//2 * torch.rand(H, N//2)
+    else:
+        imag_part = repeat(torch.arange(N//2), 'n -> h n', h=H)
+
+    if init in ['random', 'rand']:
+        imag_part = torch.exp(torch.randn(H, N//2))
+    elif init == 'real':
+        imag_part = 0 * imag_part
+        if real_random:
+            real_part = torch.rand(H, N//2) * N//2
+        else:
+            # This is the S4D-Real method described in the S4D paper
+            # The A matrix is diag(-1, -2, ..., -N), which are the eigenvalues of the HiPPO matrix
+            real_part = 1 + repeat(torch.arange(N//2), 'n -> h n', h=H)
+    elif init in ['linear', 'lin']:
+        imag_part = pi * imag_part
+    elif init in ['inverse', 'inv']: # Based on asymptotics of the default HiPPO matrix
+        imag_part = 1/pi * N * (N/(1+2*imag_part)-1)
+    elif init in ['inverse2', 'inv2']:
+        imag_part = 1/pi * N * (N/(1+imag_part)-1)
+    elif init in ['quadratic', 'quad']:
+        imag_part = 1/pi * (1+2*imag_part)**2
+    elif init in ['legs', 'hippo']:
+        A, _, _, _ = nplr('legsd', N)
+        imag_part = -A.imag  # Positive
+    else: raise NotImplementedError
+    imag_part = imag_scale * imag_part
+
+    # Construct diagonal A
+    A = -real_part - 1j * imag_part  # Force negative real and imag
+    assert torch.all(A.real < 1e-4) and torch.all(A.imag <= 0.0)  # Allow some tolerance for numerical precision on real part
+
+    # Initialize B
+    if B_random:
+        log.warning("'B_random' is deprecated in favor of B_init='random' and will be deprecated in a future version.")
+    if init in ['legs', 'hippo']:
+        log.info(f'Initializing with S4D-LegS and ignoring argument {B_init=}')
+        # Special initialization using the HiPPO B matrix
+        # Note that theory (from S4D paper) says that B should be halved
+        # to match DPLR but we drop this 0.5 factor for simplicity
+        _, P, B, _ = nplr('legs', N, B_clip=2.0)
+        B = repeat(B, 'n -> h n', h=H).clone().contiguous()
+    elif B_init == 'constant':
+        B = torch.ones(H, N//2, dtype=dtype)
+    elif B_init == 'random':
+        B = torch.randn(H, N//2, dtype=dtype)
+    elif B_init == 'alternating':  # Seems to track 'constant' exactly for some reason
+        B = torch.ones(H, N//4, 2, dtype=dtype)
+        B[:, :, 1] *= -1
+        B = B.view(H, N//2)
+    elif B_init == 'unit-cw':
+        z = torch.tensor(torch.exp(-2j * pi / N), dtype=dtype)
+        B = z ** torch.arange(0, N // 2)
+        B = repeat(B, 'n -> h n', h=H).clone().contiguous()
+    elif B_init == 'unit-ccw':
+        z = torch.tensor(torch.exp(2j * pi / N), dtype=dtype)
+        B = z ** torch.arange(0, N // 2)
+        B = repeat(B, 'n -> h n', h=H).clone().contiguous()
+    else: raise NotImplementedError
+    B *= B_scale
+
+    # Experimental feature that appeared in earlier versions of HTTYH (not extensively tested)
+    # Seems more principled for normalization theoretically, but seemed to hurt on PathX
+    if normalize:
+        norm = -B/A # (H, N) # Result if you integrate the kernel with constant 1 function
+        zeta = 2*torch.sum(torch.abs(norm)**2, dim=-1, keepdim=True) # Variance with a random C vector
+        B = B / zeta**.5
+
+    # Initialize P
+    if B_init in ['legs', 'hippo', 'legsd']:
+        # P constructed earlier
+        P = repeat(P, 'r n -> r h n', h=H).clone().contiguous()
+    else:
+        P = torch.randn(rank, H, N//2, dtype=dtype)
+        P = P * P_scale
+
+    # Initialize V (only used in testing)
+    V = torch.eye(N, dtype=dtype)[:, :N//2]
+    V = repeat(V, 'n m -> h n m', h=H)
+
+    return A, P, B, V
+
+def ssm(init, N, R, H, **ssm_args):
+    """Dispatcher to create single SSM initialization
+
+    N: state size
+    R: rank (for DPLR parameterization)
+    H: number of independent SSM copies
+    """
+
+    if init.startswith("diag") or init.startswith("dplr"):
+        if init.startswith("diag"):
+            ssm_args["P_scale"] = 0.0
+        args = init[4:].split("-")
+        assert args[0] == ""
+        if len(args) > 1:
+            ssm_args["init"] = args[1]
+        A, P, B, V = dplr(N=N, rank=R, H=H, **ssm_args)
+    else:
+        A, P, B, V = nplr(init, N, R, **ssm_args)
+        A = repeat(A, 'n -> s n', s=H)
+        P = repeat(P, 'r n -> r s n', s=H)
+        B = repeat(B, 'n -> s n', s=H)
+        V = repeat(V, 'n m -> s n m', s=H)
+    return A, P, B, V
+
+combinations = {
+    'hippo': ['legs', 'fourier'],
+    'diag': ['diag-inv', 'diag-lin'],
+    'all': ['legs', 'fourier', 'diag-inv', 'diag-lin'],
+}
+
+def combination(inits, N, R, S, **ssm_args):
+    if isinstance(inits, str):
+        inits = combinations[inits] if inits in combinations else [inits]
+
+    assert S % len(inits) == 0, f"{S} independent trainable SSM copies must be multiple of {len(inits)} different inits"
+    A, P, B, V = zip(
+        *[ssm(init, N, R, S // len(inits), **ssm_args) for init in inits]
+    )
+    A = torch.cat(A, dim=0) # (S N)
+    P = torch.cat(P, dim=1) # (R S N)
+    B = torch.cat(B, dim=0) # (S N)
+    V = torch.cat(V, dim=0) # (S N N)
+    return A, P, B, V
+
+
+"""SSM convolution kernels"""
+
+def inv_transform(param, transform='none'):
+    """Initialize a (positive) parameter under a transform."""
+    param = torch.clamp(param, min=1e-4)
+    if transform == 'none':
+        return param
+    elif transform == 'exp':
+        return torch.log(param) # Some of the HiPPO methods have real part 0
+    elif transform == 'relu':
+        return param
+    elif transform == 'sigmoid':
+        return torch.logit(param)
+    elif transform == 'softplus':
+        return torch.log(torch.exp(param)-1)
+    else: raise NotImplementedError
+
+def param_transform(param, transform='none'):
+    """Get a (positive) parameter under a transform."""
+    if transform == 'none':
+        p = param
+    elif transform == 'exp':
+        p = torch.exp(param)
+    elif transform == 'relu':
+        # JAX version seems to NaN if you allow 0's, although this code was fine without it
+        p = F.relu(param)+1e-4
+    elif transform == 'sigmoid':
+        p = F.sigmoid(param)
+    elif transform == 'softplus':
+        p = F.softplus(param)
+    else: raise NotImplementedError
+    return p
+
+class Kernel(nn.Module):
+    """Interface for modules that produce convolution kernels.
+
+    A main distinction between these and normal Modules is that the forward pass
+    does not take inputs. It is a mapping from parameters to a tensor that can
+    be used in other modules, in particular as a convolution kernel.
+
+    Because of the unusual parameterization, these kernels may often want special
+    hyperparameter settings on their parameters. The `register` method provides
+    an easy interface for controlling this, and is intended to be used with an
+    optimizer hook that can be found in train.py or example.py.
+
+    This class also defines an interface for interacting with kernels *statefully*,
+    in particular for state space models (SSMs). This interface handles the setting
+    when a model can be converted from a "CNN" into an "RNN".
+    _setup_step()
+    step()
+    default_state()
+    forward_state()
+
+    See ConvKernel for the simplest instantiation of this interface.
+    """
+
+    def __init__(
+        self,
+        d_model: int = 0,
+        channels: int = 1,
+        l_max: Optional[int] = None,
+        lr: Union[float, Optional[Mapping]] = None,
+        wd: Union[float, Optional[Mapping]] = 0.0,
+        verbose: bool = True,
+        **kwargs,
+    ):
+        """General interface.
+
+        d_model (H): Model dimension, or number of independent convolution kernels created.
+        channels (C): Extra dimension in the returned output (see .forward()).
+            - One interpretation is that it expands the input dimension giving it C separate "heads" per feature.
+              That is convolving by this kernel maps shape (B L D) -> (B L C D)
+            - This is also used to implement a particular form of bidirectionality in an efficient way.
+            - In general for making a more powerful model, instead of increasing C
+              it is recommended to set channels=1 and adjust H to control parameters instead.
+        l_max (L): Maximum kernel length (optional). If unspecified, most Kernel instantiations
+            will return kernels of arbitrary length as passed into .forward().
+        lr: Optional dictionary specifying special hyperparameters for .register().
+            Passing in a number (e.g. 0.001) sets attributes of SSM parameters (A, B, dt).
+            A custom optimizer hook is needed to configure the optimizer to set the learning rates appropriately for these parameters.
+        wd: Same as lr, but for weight decay.
+        """
+        super().__init__()
+        assert d_model > 0
+        self.H = self.d_model = d_model
+        self.L = self.l_max = l_max
+        self.channels = channels
+        self.lr = lr
+        self.wd = wd
+        self.verbose = verbose
+
+        # Add a catch-all **kwargs to make it easier to change kernels
+        # without manually moving other options passed in the config.
+        # Good to log these just so it's explicit.
+        if self.verbose and len(kwargs) > 0:
+            log.info(f"{type(self)} extra kwargs: {kwargs}")
+
+        # Logic for registering parameters
+        # Case 1: lr: None | float
+        #   All params should have this lr (None means inherit from global lr)
+        # Case 2: lr: dict
+        #   Specified params should have that lr, all others should be None
+        if self.lr is None or isinstance(self.lr, float):
+            self.lr_dict = defaultdict(lambda: self.lr)
+        else:
+            self.lr_dict = defaultdict(lambda: None)
+            self.lr_dict.update(self.lr)
+
+        # Same logic for weight decay
+        # (but is always just set to 0.0 and hasn't been ablated)
+        if self.wd is None or isinstance(self.wd, float):
+            self.wd_dict = defaultdict(lambda: self.wd)
+        else:
+            self.wd_dict = defaultdict(lambda: None)
+            self.wd_dict.update(self.wd)
+
+    def forward(self, state=None, rate=1.0, L=None):
+        """General interface to generate a global convolution kernel.
+
+        state: Initial state for recurrent updates.
+            E.g. for SSMs, this should have shape (B, H, N) (batch, d_model, d_state).
+        rate: Relative sampling rate.
+        L: Target kernel length.
+
+        Returns:
+          - (C, H, L) (channels, d_model, l_kernel) The convolution kernel.
+          - (B, H, L) (batch, d_model, l_kernel)
+              Extra information for how the state affects the output of convolving by kernel.
+        """
+        raise NotImplementedError
+
+    def register(self, name, tensor, lr=None, wd=0.0):
+        """Register a tensor with a configurable learning rate and 0 weight decay"""
+
+        if lr == 0.0:
+            self.register_buffer(name, tensor)
+        else:
+            self.register_parameter(name, nn.Parameter(tensor))
+
+            optim = {}
+            if lr is not None: optim["lr"] = lr
+            if wd is not None: optim["weight_decay"] = wd
+            setattr(getattr(self, name), "_optim", optim)
+
+    def _setup_step(self, **kwargs):
+        """Convert a model into a recurrent mode for autoregressive inference."""
+        raise NotImplementedError
+
+    def step(self, x, state, **kwargs):
+        """Step the model for one timestep with input x and recurrent state."""
+        raise NotImplementedError
+
+    def default_state(self, *args, **kwargs):
+        """Return a default initial state."""
+        raise NotImplementedError
+
+    @torch.no_grad()
+    def forward_state(self, u, state):
+        """Forward the state through a sequence, i.e. computes the state after passing chunk through the kernel."""
+        raise NotImplementedError
+
+    @property
+    def d_state(self):
+        """Implement this for interfaces that want to interact with a stateful layer (i.e. SSMs).
+
+        Currently the only codepath that might use this is the StateDecoder, which is not used.
+        """
+        raise NotImplementedError
+
+    @property
+    def state_to_tensor(self):
+        """Same as d_state, only needed for niche codepaths involving recurrent state."""
+        raise NotImplementedError
+
+class SSMKernel(Kernel):
+    """Parent class for different SSM parameterizations.
+
+    This class is abstract and only defines some initializations and flags that are common to all SSM variants.
+    It is instantiated by subclasses SSMKernel{Dense,Real,Diag,DPLR}.
+
+    Options:
+    d_state (N): State size (dimensionality of parameters A, B, C). Generally shouldn't need to be adjusted and doens't affect speed much for most kernels (e.g. S4, S4D).
+    deterministic: Use a deterministic initialization for dt, A, B, C.
+        Useful for debugging as well as constructing a simple exponential decay kernel (e.g. used in S4ND image->video inflation).
+    
+    dt_min, dt_max: min and max values for the step size dt
+    dt_tie: Keep dt tied across the N dimensions of the state. Although this theoretically makes more sense, models such as S5 and Mega have found slightly improvements by setting it to False.
+    dt_transform: Transform function for parameterization of dt (default 'softplus', used to be 'exp')
+
+    rank: Rank of low-rank correction for DPLR mode. Needs to be increased for init "legt".
+    n_ssm: Number of independent trainable (A, B) SSMs, e.g.
+        `n_ssm=1` means all A/B parameters are tied across the H different instantiations of C.
+        `n_ssm=None` means all H SSMs are completely independent.
+        Generally, changing this option can save parameters but doesn't affect performance or speed much.
+        This parameter must divide H.
+    init: Options for initialization of (A, B). For DPLR mode, recommendations are "legs", "fout", "hippo" (combination of both). For Diag mode, recommendations are "diag-inv", "diag-lin", "diag-legs", and "diag" (combination of diag-inv and diag-lin).
+    init_args: Extra arguments passed into initialization function (see dplr.py for options).
+    """
+
+    def init_dt(self):
+        # Generate dt
+        if self.deterministic:  # Meant for debugging
+            assert self.dt_tie, "Deterministic dt initialization is tied"
+            assert self.dt_transform == 'exp', "Deterministic dt transform should be 'exp' for simplicity"
+            inv_dt = torch.exp(torch.linspace(math.log(self.dt_min), math.log(self.dt_max), self.H)).unsqueeze(-1) # (H 1)
+        else:
+            shape = (self.H, 1) if self.dt_tie else (self.H, self.N//2)
+            # Initialize log dt
+            inv_dt = torch.rand(*shape, dtype=self.dtype) * (
+                math.log(self.dt_max) - math.log(self.dt_min)
+            ) + math.log(self.dt_min)
+            if self.dt_transform != 'exp':
+                inv_dt = inv_transform(torch.exp(inv_dt), self.dt_transform)
+
+        return inv_dt
+
+    def init_ssm_real(self):
+        """Returns (dense, real) (A, B, C) parameters for init options."""
+        # Generate A, B
+        A, B = transition(self.init, self.N)
+        A = torch.as_tensor(A, dtype=self.dtype)
+        B = torch.as_tensor(B, dtype=self.dtype)[:, 0]
+        B = repeat(B, 'n -> v n', v=self.n_ssm).clone().contiguous()
+        A = repeat(A, 'n m -> v n m', v=self.n_ssm).clone().contiguous()
+
+        # Generate C
+        if self.deterministic:
+            C = torch.zeros(self.channels, self.H, self.N, dtype=self.dtype)
+            C[..., :1] = 1.0
+        else:
+            C = torch.randn(self.channels, self.H, self.N, dtype=self.dtype)
+
+        return A, B, C
+
+    def init_ssm_dplr(self):
+        """Returns DPLR (A, P, B, C) parameters for init options."""
+        A, P, B, V = combination(self.init, self.N, self.rank, self.n_ssm, **self.init_args)
+
+        # Broadcast C to have H channels
+        if self.deterministic:
+            C = torch.zeros(self.channels, self.n_ssm, self.N, dtype=self.cdtype)
+            C[:, :, :1] = 1.
+            C = contract('hmn, chn -> chm', V.conj().transpose(-1, -2), C) # V^* C
+            C = repeat(C, 'c t n -> c (v t) n', v=self.H // C.size(-2)).clone().contiguous()
+        else:
+            C = torch.randn(self.channels, self.H, self.N//2, dtype=self.cdtype)
+
+        # Broadcast other parameters to have n_ssm copies
+        assert self.n_ssm % B.size(-2) == 0 \
+                and self.n_ssm % P.size(-2) == 0 \
+                and self.n_ssm % A.size(-2) == 0
+
+        # Broadcast tensors to n_ssm copies
+        # These will be the parameters, so make sure tensors are materialized and contiguous
+        B = repeat(B, 't n -> (v t) n', v=self.n_ssm // B.size(-2)).clone().contiguous()
+        P = repeat(P, 'r t n -> r (v t) n', v=self.n_ssm // P.size(-2)).clone().contiguous()
+        A = repeat(A, 't n -> (v t) n', v=self.n_ssm // A.size(-2)).clone().contiguous()
+
+        # Because these complex parameterizations assume conjugate symmetry,
+        # halve the value of self.N for convenience
+        self.N //= 2
+
+        return A, P, B, C
+
+    def __init__(
+        self,
+        # General Kernel arguments for parent class
+        d_model: int = 0,
+        channels: int = 1,
+        l_max: Optional[int] = None,
+        lr: Union[float, Optional[Mapping]] = None,
+        wd: Union[float, Optional[Mapping]] = 0.0,
+        verbose: bool = True,
+        # SSM arguments
+        d_state: int = 64,
+        deterministic: bool = False,
+        # dt options
+        dt_min: float = 0.001,
+        dt_max: float = 0.1,
+        dt_tie: bool = True,
+        dt_transform: str = 'exp',
+        # (A, B, C) options
+        rank: int = 1,
+        n_ssm: Optional[int] = None,
+        measure: Optional[str] = None,
+        init: Optional[str] = "legs",
+        # Extra hyperparameters for initialization
+        **init_args,
+    ):
+        super().__init__(d_model=d_model, channels=channels, l_max=l_max, lr=lr, wd=wd, verbose=verbose)
+        self.N = d_state
+        self.dtype, self.cdtype = torch.float, torch.cfloat
+        self.deterministic = deterministic
+        # dt options
+        self.dt_min = dt_min
+        self.dt_max = dt_max
+        self.dt_tie = dt_tie
+        self.dt_transform = dt_transform
+        # SSM options (A, B, C)
+        self.rank = rank
+        self.n_ssm = n_ssm if n_ssm is not None else self.H
+        if measure is not None:
+            log.warning("Warning: 'measure' option changed to 'init' and will be removed in a future version.")
+            assert init is None, "'measure' and 'init' cannot both be passed into SSMKernel"
+            init, measure = measure, init
+        self.init = init
+        self.init_args = init_args
+
+    @torch.no_grad()
+    def forward_state(self, u, state):
+        """Forward the state through a sequence, i.e. computes the state after passing chunk through SSM
+
+        This is a generic version of this functionality that works for SSMs.
+        It is currently used by SSMKernelDense and SSMKernelDPLR.
+        This is a suboptimal implementation; it is recommended to use SSMKernelDiag
+        if this functionality is desired.
+
+        state: (B, H, N)
+        u: (B, H, L)
+
+        Returns: (B, H, N)
+        """
+
+        # Construct dA, dB matrices
+        dA, dB = self._setup_state() # (H N N) (H N)
+
+        conj = state.size(-1) != dA.size(-1)
+        if conj: state = _conj(state)
+
+        v = contract('h n, b h l -> b h n l', dB, u.flip(-1))
+        AL, v = power(u.size(-1), dA, v)
+        next_state = contract("h m n, b h n -> b h m", AL, state)
+        next_state = next_state + v
+
+        if conj: next_state = next_state[..., : next_state.size(-1) // 2]
+        return next_state
+
+    def _setup_state(self):
+        """Register dA and dB to module."""
+        raise NotImplementedError
+
+    @property
+    def d_state(self):
+        """d_state and state_to_tensor are used by specific decoders.
+
+        These were used in earlier versions and should not be needed in general.
+        """
+        return self.H * self.N
+
+    @property
+    def state_to_tensor(self):
+        return lambda state: rearrange('... h n -> ... (h n)', state)
+
+
+class SSMKernelDiag(SSMKernel):
+    """SSM kernel using diagonal state matrix (S4D model).
+
+    Options:
+    disc: ['zoh' | 'bilinear' | 'dss'] Discretization options.
+    dt_fast:  (experimental) Parameterize inv_dt under sinh function.
+        (Ohno et al. "Fast Saturating Gate for Learning Long Time Scales with RNNs")
+    real_transform, imag_transform: ['none' | 'exp' | 'relu' | 'sigmoid' | 'softplus']
+        Parameterize the real/imag parts of the diagonal of A under this function.
+    bandlimit: Mask high frequencies of the kernel (indices corresponding to
+        diagonal elements with large imaginary part). Introduced in S4ND paper.
+    kernel: ['cuda' | 'keops' | 'naive'] Options for Vandermonde/Cauchy kernel (in order of efficiency).
+    force_real : Force A to have 0 imaginary part, to emulate EMA.
+    """
+
+    def __init__(
+        self,
+        disc: str = 'zoh',  # Change to 'bilinear' to match S4, but should make little difference either way
+        dt_fast: bool = False,
+        real_transform: str = 'exp',
+        imag_transform: str = 'none',
+        bandlimit: Optional[float] = None,
+        kernel: str = 'cuda',
+        force_real: bool = False,
+        **kwargs,
+    ):
+        super().__init__(**kwargs)
+        self.disc = disc
+        self.dt_fast = dt_fast
+        self.real_transform = real_transform
+        self.imag_transform = imag_transform
+        self.bandlimit = bandlimit
+        self.kernel = kernel
+        self.force_real = force_real
+
+        # Initialize dt, A, B, C
+        inv_dt = self.init_dt()
+        A, P, B, C = self.init_ssm_dplr()
+        # Note that in the Diag case, P will be ignored
+        # The DPLR case subclasses this and uses P
+        self.register_params(A, B, C, inv_dt, P)
+
+    def register_params(self, A, B, C, inv_dt, P):
+        """Process the initialization into form of trainable parameters.
+
+        A: (S, N) diagonal matrix
+        B: (S, N)
+        C: (C, H, N)
+        dt: (H) timescale per feature
+
+        Dimensions:
+        N (or d_state): state size
+        H (or d_model): total SSM copies
+        S (or n_ssm): number of trainable copies of (A, B, dt); must divide H
+        C (or channels): system is 1-dim to C-dim
+
+        The forward pass of this Module returns a tensor of shape (C, H, L)
+        
+        Note: tensor shape N here denotes half the true state size, because of conjugate symmetry
+        """
+
+        assert self.kernel in ['cuda', 'keops', 'naive']
+
+        if self.dt_fast: inv_dt = torch.asinh(inv_dt)
+
+        # Rank of low-rank correction
+        assert self.H == inv_dt.size(0)
+        assert self.N == A.size(-1) == B.size(-1) == C.size(-1)
+        assert self.n_ssm == A.size(-2) == B.size(-2) # Number of independent SSMs trained
+        self.repeat = self.H // A.size(0)
+
+        # Check that diagonal part has negative real and imag part
+        # (allow some tolerance for numerical precision on real part
+        # since it may be constructed by a diagonalization)
+        assert torch.all(A.real < 1e-4) and torch.all(A.imag <= 0.0)
+
+        # Broadcast everything to correct shapes
+        C = C.expand(torch.broadcast_shapes(C.shape, (1, self.H, self.N))) # (C, H, N)  # TODO originally this was only in DPLR, check safe for Diag
+        B = B.unsqueeze(0) # (1, H, N)
+
+        assert self.channels == C.shape[0]
+        self.C = nn.Parameter(_c2r(_resolve_conj(C)))
+
+        # Register dt, B, A
+        self.register("inv_dt", inv_dt, self.lr_dict['dt'], self.wd_dict['dt'])
+        self.register("B", _c2r(B), self.lr_dict['B'], self.wd_dict['B'])
+        self.register("A_real", inv_transform(-A.real, self.real_transform), self.lr_dict['A'], self.wd_dict['A'])
+        self.register("A_imag", inv_transform(-A.imag, self.imag_transform), self.lr_dict['A'], self.wd_dict['A'])
+
+    def _get_params(self, rate=1.0):
+        """Process the internal parameters."""
+
+        # (S N) where S=n_ssm
+        A = -param_transform(self.A_real, self.real_transform) - 1j * param_transform(self.A_imag, self.imag_transform)
+        B = _r2c(self.B) # (1 S N)
+        C = _r2c(self.C) # (C H N)
+
+        if self.dt_fast: inv_dt = torch.sinh(self.inv_dt)
+        else: inv_dt = self.inv_dt
+        dt = param_transform(inv_dt, self.dt_transform) * rate # (H N)
+
+        # Force A to be real valued, so the whole kernel can be interpreted as a "multi-head EMA"
+        if self.force_real:
+            A = A.real + 0j
+
+        if self.bandlimit is not None:
+            freqs = dt / rate * A.imag.abs() / (2*math.pi) # (H N)
+            mask = torch.where(freqs < self.bandlimit * .5, 1, 0)
+            C = C * mask
+
+        # Incorporate dt into A and B
+        A = repeat(A, 't n -> (v t) n', v=self.repeat)  # (H N)
+        B = repeat(B, 'b t n -> b (v t) n', v=self.repeat)  # (1 H N)
+
+        # TODO: The downstream algorithm should only need to access dt*A
+        # However the current DPLR kernel still uses dt and A separately
+        # Once that is fixed, this should return dtA instead of dt and A
+        dtA = dt * A  # (H N)
+
+        return dt, A, B, C
+
+    def forward(self, L, state=None, rate=1.0):
+        """See Kernel.forward() for argument documentation."""
+
+        dt, A, B, C = self._get_params(rate)
+        dtA = dt * A
+
+        # Augment B with state
+        if state is not None:
+            s = state / dt
+            if self.disc == 'bilinear':
+                s = s * (1. + dtA/2)
+            elif self.disc == 'zoh':
+                s = s * dtA * dtA.exp() / (dtA.exp() - 1.)
+            B = torch.cat([s, B], dim=-3) # (1+B H N)
+
+
+        # Combine B and C
+        C = (B[:, None, :, :] * C).view(-1, self.H, self.N)
+
+        # Dispatch which Vandermonde kernel to use
+        if has_cuda_extension and C.dtype == torch.cfloat and C.device.type == 'cuda' and self.kernel == 'cuda':
+            log_vandermonde = log_vandermonde_cuda
+        elif has_pykeops and self.kernel in ['cuda', 'keops']:
+            log_vandermonde = log_vandermonde_keops
+        else:
+            log_vandermonde = log_vandermonde_naive
+
+        # Main kernel
+        if self.disc == 'zoh':
+            # Power up
+            C = C * (torch.exp(dtA)-1.) / A
+            K = log_vandermonde(C, dtA, L) # (H L)
+        elif self.disc == 'bilinear':
+            C = C * (1. - dtA/2).reciprocal() * dt # or * dtA / A
+            dA = (1. + dtA/2) / (1. - dtA/2)
+            K = log_vandermonde(C, dA.log(), L)
+        elif self.disc == 'dss':
+            # Implementation from DSS meant for case when real eigenvalues can be positive
+            P = dtA.unsqueeze(-1) * torch.arange(L, device=C.device) # [H N L]
+            A_gt_0 = A.real > 0                                      # [N]
+            if A_gt_0.any():
+                with torch.no_grad():
+                    P_max = dtA * (A_gt_0 * (L-1))                   # [H N]
+                P = P - P_max.unsqueeze(-1)                          # [H N L]
+            S = P.exp()                                              # [H N L]
+
+            dtA_neg = dtA * (1 - 2*A_gt_0)                           # [H N]
+            num = dtA_neg.exp() - 1                                  # [H N]
+            den = (dtA_neg * L).exp() - 1                            # [H N]
+
+            # Inline reciprocal function for DSS logic
+            x = den * A
+            x_conj = _resolve_conj(x)
+            r = x_conj / (x*x_conj + 1e-7)
+
+            C = C * num * r             # [C H N]
+            K = contract('chn,hnl->chl', C, S).float()
+        else: raise ValueError(f"Discretization {self.disc} not supported")
+
+        K = K.view(-1, self.channels, self.H, L) # (1+B C H L)
+
+        if state is not None:
+            K_state = K[:-1, :, :, :] # (B C H L)
+        else:
+            K_state = None
+        K = K[-1, :, :, :] # (C H L)
+
+        return K, K_state
+
+    def _setup_step(self):
+        """Set up dA, dB, dC discretized parameters for stepping."""
+
+        dt, A, B, C, = self._get_params()
+        # Incorporate dt into A
+        dtA = dt * A  # (H N)
+
+        if self.disc == 'zoh':
+            self.dA = torch.exp(dtA) # (H N)
+            self.dB = B * (torch.exp(dtA)-1.) / A # (C H N)
+        elif self.disc == 'bilinear':
+            self.dA = (1. + dtA/2) / (1. - dtA/2)
+            self.dB = B * (1. - dtA/2).reciprocal() * dt # or * dtA / A
+        self.dB = rearrange(self.dB, '1 h n -> h n')
+        self.dC = C
+
+    def default_state(self, *batch_shape):
+        C = _r2c(self.C)
+        state = torch.zeros(*batch_shape, self.H, self.N, dtype=C.dtype, device=C.device)
+        return state
+
+    def step(self, u, state):
+        next_state = contract("h n, b h n -> b h n", self.dA, state) \
+                + contract("h n, b h -> b h n", self.dB, u)
+        y = contract("c h n, b h n -> b c h", self.dC, next_state)
+        return 2*y.real, next_state
+
+    def forward_state(self, u, state):
+        """Pass the state forward through an entire sequence."""
+        self._setup_step()
+        AL = self.dA ** u.size(-1)
+        u = u.flip(-1).to(self.dA).contiguous() # (B H L)
+        # Dispatch which Vandermonde kernel to use
+        if has_pykeops and self.kernel in ['cuda', 'keops']:
+            log_vandermonde_transpose = log_vandermonde_transpose_keops
+        else:
+            log_vandermonde_transpose = log_vandermonde_transpose_naive
+        v = log_vandermonde_transpose(u, self.dB, self.dA.log(), u.size(-1))
+        next_state = AL * state + v
+        return next_state
+
+class SSMKernelDPLR(SSMKernelDiag):
+    """SSM kernel for diagonal + low rank (DPLR) state matrices, corresponding to the original S4 model."""
+
+    @torch.no_grad()
+    def _setup_C(self, L):
+        """Construct C~ from C.
+        Two modes are supported: go directly to length L if self.l_kernel is 1, or length is doubled
+        """
+
+        if self.l_kernel.item() == 0:
+            if self.verbose: log.info(f"S4: Initializing kernel to length {L}")
+            double_length = False
+        elif L > self.l_kernel.item(): # 2*int(self.l_kernel) == L:
+            if self.verbose: log.info(f"S4: Doubling length from L = {self.l_kernel.item()} to {2*self.l_kernel.item()}")
+            double_length = True
+            L = self.l_kernel.item() # Convenience for the math below
+        else: return
+
+        C = _r2c(self.C)
+        dA, _ = self._setup_state()
+        dA_L = power(L, dA)
+        # Multiply C by I - dA_L
+        C_ = _conj(C)
+        prod = contract("h m n, c h n -> c h m", dA_L.transpose(-1, -2), C_)
+        if double_length: prod = -prod # Multiply by I + dA_L instead
+        C_ = C_ - prod
+        C_ = C_[..., :self.N] # Take conjugate pairs again
+        self.C.copy_(_c2r(C_))
+
+        self.l_kernel = 2*self.l_kernel if double_length else self.l_kernel+L # Preserve type/device
+
+    def _omega(self, L, dtype, device, cache=True):
+        """Calculate (and cache) FFT nodes.
+        This also caches a version of the nodes "unprocessed" with the bilinear transform.
+        This method should be called everytime the internal length self.l_kernel changes.
+        """
+
+        # Use cached if available
+        if cache and hasattr(self, 'omega') and self.omega.size(-1) == L//2+1:
+            return self.omega, self.z
+
+        omega = torch.tensor(
+            np.exp(-2j * np.pi / (L)), dtype=dtype, device=device
+        )  # \omega_{2L}
+        omega = omega ** torch.arange(0, L // 2 + 1, device=device)
+        z = 2 * (1 - omega) / (1 + omega)
+
+        # Cache if necessary
+        if cache:
+            self.omega = omega
+            self.z = z
+        return omega, z
+
+
+    def register_params(self, A, B, C, inv_dt, P):
+        """Process the initialization into form of trainable parameters.
+        The SSM state matrix is represented by diag_embed(A) - PP^*
+        Note that the A notation here is slightly overloaded:
+        normally A refers to the full SSM state matrix (DPLR in this case)
+        but here we're using it to refer to the diagonal part of the matrix.
+        This is to make variable names compatible with the SSMKernelDiag class (DSS/S4D)
+        and is a much simpler variable name (e.g. as opposed to Lambda).
+        A: (S, N) diagonal part
+        P: (R, S, N) low-rank part
+        B: (S, N)
+        C: (C, H, N)
+        dt: (H) timescale per feature
+        Dimensions:
+        N (or d_state): state size
+        H (or d_model): total SSM copies
+        S (or n_ssm): number of trainable copies of (A, B, dt); must divide H
+        R (or rank): rank of low-rank part
+        C (or channels): system is 1-dim to C-dim
+        The forward pass of this Module returns a tensor of shape (C, H, L)
+        Note: tensor shape N here denotes half the true state size, because of conjugate symmetry
+        """
+
+        # Print out kernel lengths; it can be tricky to make sure the length logic is correct
+        if self.verbose:
+            log.info(f"Constructing S4 (H, N, L) = ({self.H}, {self.N}, {self.l_max})")
+
+        # Register the basic params for diagonal SSM (A, B, C, dt)
+        super().register_params(A, B, C, inv_dt, P)
+
+        # Check shapes
+        assert self.rank == P.shape[-3]
+        assert self.N == P.size(-1)
+        assert self.n_ssm == P.size(-2)
+
+        self.register('P', _c2r(P), self.lr_dict['A'], self.wd_dict['A'])
+
+        # Track the current kernel length this is "attuned" to
+        self.register_buffer('l_kernel', torch.tensor(0))
+
+    def _get_params(self, rate=1.0):
+        dt, A, B, C = super()._get_params(rate=rate)
+        P = _r2c(self.P)  # (R S N)
+        P = repeat(P, 'r t n -> r (v t) n', v=self.repeat)  # (R H N)
+        Q = P.conj()
+
+        return dt, A, B, C, P, Q
+
+    def forward(self, state=None, rate=1.0, L=None):
+        """See Kernel.forward() for argument documentation."""
+
+        # Initialize C~ if necessary (done in forward pass so it's on the correct device)
+        if self.l_kernel.item() == 0 and self.l_max is not None and self.l_max > 0:
+            self._setup_C(self.l_max)
+
+        # Handle sampling rate logic
+        # The idea is that this kernel's length (in continuous units) is self.l_kernel, while we are asked to provide a kernel of length L at (relative) frequency rate
+        if L is None:
+            L = round(self.l_kernel.item() / rate)
+
+        # Increase the internal length if needed
+        continuous_L = round(rate*L)
+        while continuous_L > self.l_kernel.item():
+            self._setup_C(continuous_L)
+        discrete_L = round(self.l_kernel.item()/rate)
+
+        dt, A, B, C, P, Q = self._get_params(rate)
+
+        # Get FFT nodes of right length
+        omega, z = self._omega(discrete_L, dtype=A.dtype, device=A.device, cache=(rate==1.0))
+
+        # Augment B
+        if state is not None:
+            # Have to "unbilinear" the state to put it into the same "type" as B
+            # Compute 1/dt * (I + dt/2 A) @ state
+
+            # Can do this without expanding (maybe minor speedup using conj symmetry in theory), but it's easier to read this way
+            s = _conj(state) if state.size(-1) == self.N else state # (B H N)
+            sA = (
+                s * _conj(A) # (B H N)
+                - contract('bhm, rhm, rhn -> bhn', s, _conj(Q), _conj(P))
+            )
+            s = s / dt + sA / 2
+            s = s[..., :self.N]
+
+            B = torch.cat([s, B], dim=-3)  # (B+1, H, N)
+
+        # Incorporate dt into A
+        A = A * dt  # (H N)
+
+        # Stack B and p, C and q for convenient batching
+        B = torch.cat([B, P], dim=-3) # (B+1+R, H, N)
+        C = torch.cat([C, Q], dim=-3) # (C+R, H, N)
+
+        # Incorporate B and C batch dimensions
+        v = B.unsqueeze(-3) * C.unsqueeze(-4)  # (B+1+R, C+R, H, N)
+        v = v * dt  # Incorporate dt into B
+
+        # Dispatch which Cauchy kernel to use
+        if has_cuda_extension and z.dtype == torch.cfloat and z.device.type == 'cuda' and self.kernel == 'cuda':
+            cauchy_mult = cauchy_cuda
+        elif has_pykeops and self.kernel in ['cuda', 'keops']:
+            cauchy_mult = cauchy_keops
+        else:
+            cauchy_mult = cauchy_naive
+        # Calculate resolvent at omega
+        r = cauchy_mult(v, z, A)
+
+        # Low-rank Woodbury correction
+        if self.rank == 1:
+            k_f = r[:-1, :-1, :, :] - r[:-1, -1:, :, :] * r[-1:, :-1, :, :] / (1 + r[-1:, -1:, :, :])
+        elif self.rank == 2:
+            r00 = r[: -self.rank, : -self.rank, :, :]
+            r01 = r[: -self.rank, -self.rank :, :, :]
+            r10 = r[-self.rank :, : -self.rank, :, :]
+            r11 = r[-self.rank :, -self.rank :, :, :]
+            det = (1 + r11[:1, :1, :, :]) * (1 + r11[1:, 1:, :, :]) - r11[:1, 1:, :, :] * r11[1:, :1, :, :]
+            s = (
+                r01[:, :1, :, :] * (1 + r11[1:, 1:, :, :]) * r10[:1, :, :, :]
+                + r01[:, 1:, :, :] * (1 + r11[:1, :1, :, :]) * r10[1:, :, :, :]
+                - r01[:, :1, :, :] * (r11[:1, 1:, :, :]) * r10[1:, :, :, :]
+                - r01[:, 1:, :, :] * (r11[1:, :1, :, :]) * r10[:1, :, :, :]
+            )
+            s = s / det
+            k_f = r00 - s
+        else:
+            r00 = r[:-self.rank, :-self.rank, :, :]
+            r01 = r[:-self.rank, -self.rank:, :, :]
+            r10 = r[-self.rank:, :-self.rank, :, :]
+            r11 = r[-self.rank:, -self.rank:, :, :]
+            r11 = rearrange(r11, "a b h n -> h n a b")
+            r11 = torch.linalg.inv(torch.eye(self.rank, device=r.device) + r11)
+            r11 = rearrange(r11, "h n a b -> a b h n")
+            k_f = r00 - torch.einsum("i j h n, j k h n, k l h n -> i l h n", r01, r11, r10)
+
+        # Final correction for the bilinear transform
+        k_f = k_f * 2 / (1 + omega)
+
+        # Move from frequency to coefficients
+        k = torch.fft.irfft(k_f, n=discrete_L)  # (B+1, C, H, L)
+
+        # # Truncate to target length
+        k = k[..., :L]
+
+        if state is not None:
+            k_state = k[:-1, :, :, :]  # (B, C, H, L)
+        else:
+            k_state = None
+        k_B = k[-1, :, :, :] # (C H L)
+
+        return k_B, k_state
+
+    @torch.no_grad()
+    def double_length(self):
+        self._setup_C(2*self.l_kernel)
+
+    @torch.no_grad()
+    def _check(self):
+        """Check if A, B, C parameters and vanilla SSMKernel construction can be recovered"""
+
+        # assert self.l_kernel > 0, "Set up module first"
+
+        K = self.forward(L=self.l_max)[0]
+
+        self._setup_step()
+        K_ = krylov(self.l_max, self.dA, self.dB, self.dC)
+
+        diff = K - K_
+        print("checking DPLR Kernel construction", torch.sum(diff ** 2))
+
+    @torch.no_grad()
+    def _setup_linear(self):
+        """Preprocessing that allows fast linear-time (in state dimension) stepping."""
+        dt, A, B, C, P, Q = self._get_params()
+
+        # Prepare Linear stepping
+        D = (2.0 / dt - A).reciprocal()  # (H, N)
+        R = (torch.eye(self.rank, dtype=A.dtype, device=A.device) + 2*contract('r h n, h n, s h n -> h r s', Q, D, P).real) # (H R R)
+        Q_D = rearrange(Q*D, 'r h n -> h r n')
+        try:
+            R = torch.linalg.solve(R, Q_D) # (H R N)
+        except:
+            R = torch.tensor(np.linalg.solve(R.to(Q_D).contiguous().detach().cpu(), Q_D.contiguous().detach().cpu())).to(Q_D)
+        R = rearrange(R, 'h r n -> r h n')
+
+        self.step_params = {
+            "D": D, # (H N)
+            "R": R, # (R H N)
+            "P": P, # (R H N)
+            "Q": Q, # (R H N)
+            "B": B, # (1 H N)
+            "E": 2.0 / dt + A, # (H N)
+        }
+
+    def _step_state_linear(self, u=None, state=None):
+        """
+        Version of the step function that has time O(N) instead of O(N^2) per step, which takes advantage of the DPLR form and bilinear discretization.
+        Unfortunately, as currently implemented it's about 2x slower because it calls several sequential operations.
+        Perhaps a fused CUDA kernel implementation would be much faster.
+        u: (H) Input
+        state: (H, N/2) State with conjugate pairs. Optionally, the state can have last dimension N.
+        Returns: same shape as state
+        """
+        C = _r2c(self.C) # View used for dtype/device
+
+        if u is None: # Special case used to find dA
+            u = torch.zeros(self.H, dtype=C.dtype, device=C.device)
+        if state is None: # Special case used to find dB
+            state = torch.zeros(self.H, self.N, dtype=C.dtype, device=C.device)
+
+        step_params = self.step_params.copy()
+        if state.size(-1) == self.N: # Only store half of the conjugate pairs; should be true by default
+            # There should be a slightly faster way using conjugate symmetry
+            contract_fn = lambda p, x, y: contract('r h n, r h m, ... h m -> ... h n', _conj(p), _conj(x), _conj(y))[..., :self.N] # inner outer product
+        else:
+            assert state.size(-1) == 2*self.N
+            step_params = {k: _conj(v) for k, v in step_params.items()}
+            contract_fn = lambda p, x, y: contract('r h n, r h m, ... h m -> ... h n', p, x, y) # inner outer product
+        D = step_params["D"]  # (H N)
+        E = step_params["E"]  # (H N)
+        R = step_params["R"]  # (R H N)
+        P = step_params["P"]  # (R H N)
+        Q = step_params["Q"]  # (R H N)
+        B = step_params["B"]  # (1 H N)
+
+        new_state = E * state - contract_fn(P, Q, state) # (B H N)
+        new_state = new_state + 2.0 * B * u.unsqueeze(-1)  # (B H N)
+        new_state = D * (new_state - contract_fn(P, R, new_state))
+
+        return new_state
+
+    def _setup_state(self):
+        """Construct dA and dB for discretized state equation."""
+
+        # Construct dA and dB by using the stepping
+        self._setup_linear()
+        C = _r2c(self.C) # Just returns a view that we use for finding dtype/device
+
+        state = torch.eye(2*self.N, dtype=C.dtype, device=C.device).unsqueeze(-2) # (N 1 N)
+        dA = self._step_state_linear(state=state)
+        dA = rearrange(dA, "n h m -> h m n")
+
+        u = C.new_ones(self.H)
+        dB = self._step_state_linear(u=u)
+        dB = _conj(dB)
+        dB = rearrange(dB, '1 h n -> h n') # (H N)
+        return dA, dB
+
+    def _step_state(self, u, state):
+        """Must be called after self.default_state() is used to construct an initial state!"""
+        next_state = (torch.einsum(self.state_contraction, self.dA, state)
+                     + torch.einsum(self.input_contraction, self.dB, u))
+        return next_state
+
+    def _setup_step(self, mode='dense'):
+        """Set up dA, dB, dC discretized parameters for stepping."""
+        self.dA, self.dB = self._setup_state()
+
+        # Calculate original C
+        C = _conj(_r2c(self.C)) # (H C N)
+        if self.l_kernel.item() == 0:
+            dC = C
+        else:
+            # self.C represents C_tilde
+            dA_L = power(self.l_kernel.item(), self.dA)
+            I = torch.eye(self.dA.size(-1)).to(dA_L)
+
+            dC = torch.linalg.solve(
+                I - dA_L.transpose(-1, -2),
+                C.unsqueeze(-1),
+            ).squeeze(-1)
+        self.dC = dC
+
+        # Do special preprocessing for different step modes
+
+        self._step_mode = mode
+        if mode == 'linear':
+            # Linear case: special step function for the state, we need to handle output
+            # use conjugate symmetry by default, which affects the output projection
+            self.dC = 2*self.dC[:, :, :self.N]
+        elif mode == 'diagonal':
+            # Eigendecomposition of the A matrix
+            L, V = torch.linalg.eig(self.dA)
+            V_inv = torch.linalg.inv(V)
+            # Check that the eigendedecomposition is correct
+            if self.verbose:
+                print("Diagonalization error:", torch.dist(V @ torch.diag_embed(L) @ V_inv, self.dA))
+
+            # Change the parameterization to diagonalize
+            self.dA = L
+            self.dB = contract('h n m, h m -> h n', V_inv, self.dB)
+            self.dC = contract('h n m, c h n -> c h m', V, self.dC)
+
+        elif mode == 'dense':
+            pass
+        else: raise NotImplementedError("DPLR Kernel step mode must be {'dense' | 'linear' | 'diagonal'}")
+
+    def default_state(self, *batch_shape):
+        C = _r2c(self.C)
+        N = C.size(-1)
+        H = C.size(-2)
+
+        # Cache the tensor contractions we will later do, for efficiency
+        # These are put in this function because they depend on the batch size
+        step_mode = getattr(self, "_step_mode", "dense")  # Used in default_state, which is called without _setup_step() in forward_state()
+        if step_mode != 'linear':
+            N *= 2
+
+            if step_mode == 'diagonal':
+                self.state_contraction = "h n, ... h n -> ... h n"
+            else:
+                # Dense (quadratic) case: expand all terms
+                self.state_contraction = "h m n, ... h n -> ... h m"
+
+            self.input_contraction = "h n, ... h -> ... h n"
+
+        self.output_contraction = "c h n, ... h n -> ... c h"
+
+        state = torch.zeros(*batch_shape, H, N, dtype=C.dtype, device=C.device)
+        return state
+
+    def step(self, u, state):
+        """Must have called self._setup_step() and created state with self.default_state() before calling this."""
+
+        if self._step_mode == 'linear':
+            new_state = self._step_state_linear(u, state)
+        else:
+            new_state = self._step_state(u, state)
+        y = torch.einsum(self.output_contraction, self.dC, new_state)
+        return y.real, new_state
+
+    def forward_state(self, *args, **kwargs):
+        # Dispatch directly to generic state forwarding
+        # instead of using the Diag version
+
+        # TODO design pattern is ugly. Can be fixed with an intermediate
+        # subclass above Diag/DPLR that has the shared logic (parameter construction)
+        # but not the state/step logic.
+        # Fine to keep like this for now since we want Diag to be the standard
+        # instead of having too many layers of subclassing.
+
+        return SSMKernel.forward_state(self, *args, **kwargs)
+
+kernel_registry = {
+    's4d': SSMKernelDiag,
+    'diag': SSMKernelDiag,
+    's4': SSMKernelDPLR,
+    'nplr': SSMKernelDPLR,
+    'dplr': SSMKernelDPLR,
+}
+
+class FFTConv(nn.Module):
+    """Implements an FFT Convolution around a convolution kernel.
+    d_model (H): Model dimension (in CNN terminology, this would be "channels").
+    l_max (L): The maximum kernel length. Set l_max=None to always use a global kernel.
+    channels: Can be interpreted as a number of "heads"; the SSM is a map from a 1-dim to C-dim sequence. It's not recommended to change this; instead, increase d_model for larger models.
+    bidirectional: If True, convolution kernel will be two-sided.
+    activation: Activation after the full convolution.
+    transposed, dropout, tie_dropout: More general model options, see SequenceModule.
+    mode: Which kernel algorithm to use. 'nplr' is the full S4 model; 'diag' is the simpler S4D. Other options can be found in the kernel registry.
+    kernel_args: See the class .kernel.SSMKernel for the kernel constructor which accepts kernel_args. Relevant options that are worth considering and tuning include "mode", "init", "dt_min", "dt_max", "lr"
+    """
+
+    def __init__(
+        self,
+        d_model,
+        l_max=None,
+        channels=1,
+        swap_channels=False,
+        bidirectional=False,
+        activation='gelu', # Activation after layer
+        transposed=True,
+        dropout=0.0,
+        tie_dropout=False,
+        drop_kernel=0.0,
+        mode='dplr',
+        kernel=None,
+        **kernel_args,  # Arguments passed into inner convolution kernel
+    ):
+        super().__init__()
+        self.d_model = d_model
+        self.L = self.l_max = l_max
+        self.bidirectional = bidirectional
+        self.channels = channels
+        self.transposed = transposed
+        self.swap_channels = swap_channels
+
+
+        if activation is not None and activation.startswith('glu'):
+            channels *= 2
+        self.activation = Activation(activation, dim=1 if self.transposed else -1)
+
+        self.D = nn.Parameter(torch.randn(channels, self.d_model))
+
+        if self.bidirectional:
+            channels *= 2
+
+        # Inner convolution kernel
+        if mode is not None:
+            assert kernel is None, "Pass either mode or kernel but not both"
+            # log.info(
+            #     "Argument 'mode' is deprecated and renamed to 'kernel',"
+            #     "and will be removed in a future version."
+            # )
+            kernel, mode = mode, kernel
+        kernel_cls = kernel_registry[kernel]
+        self.kernel = kernel_cls(
+            d_model=self.d_model,
+            l_max=self.l_max,
+            channels=channels,
+            **kernel_args,
+        )
+
+        dropout_fn = DropoutNd if tie_dropout else nn.Dropout
+        self.drop = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()
+        self.drop_kernel = nn.Dropout(drop_kernel) if drop_kernel > 0.0 else nn.Identity()
+
+    def forward(self, x, state=None, rate=1.0, **kwargs): # absorbs return_output and transformer src mask
+        """
+        x: (B D L) if self.transposed else (B L D)
+        """
+
+        # Always work with (B D L) dimension in this module
+        if not self.transposed: x = x.transpose(-1, -2)
+        L = x.size(-1)
+
+        # Compute SS Kernel
+        l_kernel = L if self.L is None else min(L, round(self.L / rate))
+        k, k_state =  self.kernel(L=l_kernel, rate=rate, state=state) # (C H L) (B C H L)
+
+        # Convolution
+        if self.bidirectional:
+            k0, k1 = rearrange(k, '(s c) h l -> s c h l', s=2)
+            k = F.pad(k0, (0, L)) \
+                    + F.pad(k1.flip(-1), (L, 0))
+            # The above has an off-by-one in the reverse direction
+            # This is a deliberate choice since the off-by-one should not affect any applications
+            # This can be amended which may be very slightly slower
+            # k = F.pad(k0, (0, L)) \
+            #         + F.pad(k1[..., 1:].flip(-1), (L+1, 0)) \
+            #         + F.pad(k1[..., :1], (0, l_kernel+L-1))
+
+        # Kernel dropout
+        k = self.drop_kernel(k)
+
+        k_f = torch.fft.rfft(k, n=l_kernel+L) # (C H L)
+        x_f = torch.fft.rfft(x, n=l_kernel+L) # (B H L)
+        y_f = contract('bhl,chl->bchl', x_f, k_f)
+        y = torch.fft.irfft(y_f, n=l_kernel+L)[..., :L] # (B C H L)
+
+
+        # Compute D term in state space equation - essentially a skip connection
+        y = y + contract('bhl,ch->bchl', x, self.D)
+
+        # Compute state update
+        if state is not None:
+            assert not self.bidirectional, "Bidirectional not supported with state forwarding"
+            y = y + k_state #
+            next_state = self.kernel.forward_state(x, state)
+        else:
+            next_state = None
+
+
+        # Reshape to flatten channels
+        if self.swap_channels:
+            y = rearrange(y, 'b c h l -> b (h c) l')
+        else:
+            y = rearrange(y, 'b c h l -> b (c h) l')
+
+        y = self.drop(y)  # DropoutNd better with transposed=True
+
+        if not self.transposed: y = y.transpose(-1, -2)
+        y = self.activation(y)
+
+        return y, next_state
+
+
+    def setup_step(self, **kwargs):
+        self.kernel._setup_step(**kwargs)
+
+    def step(self, x, state):
+        """ Step one time step as a recurrent model. Intended to be used during validation.
+        x: (B H)
+        state: (B H N)
+        Returns: output (B H), state (B H N)
+        """
+
+        y, next_state = self.kernel.step(x, state) # (B C H)
+        y = y + x.unsqueeze(-2) * self.D
+        y = rearrange(y, 'b c h -> b (c h)')
+        y = self.activation(y)
+        return y, next_state
+
+    def default_state(self, *batch_shape, device=None):
+        # kernel is not a SequenceModule so it doesn't need to adhere to same interface
+        # the kernel will know the device of its own parameters
+        return self.kernel.default_state(*batch_shape)
+
+    @property
+    def d_output(self):
+        return self.d_model * self.channels
+
+
+class S4Block(nn.Module):
+    """General block design wrapping an inner layer. Currently only layer=FFTConv is supported, but easy to incorporate others.
+    Arguments:
+    - bottleneck: Reduce dimension of inner layer (e.g. used in GSS).
+    - gate: Add multiplicative gating (e.g. used in GSS), which is essentially a multiplicative instead of additive residual branch.
+    - gate_act: Activation function to apply on the gate residual branch.
+    - mult_act: Activation function to apply after gate multiplication (e.g. GELU in GSS).
+    - final_act: Activation function to apply after final linear layer. 'id' for no activation, None for no linear layer at all.
+    - initializer: Initializer on final linear layer.
+    - weight_norm: Weight normalization on final linear layer.
+    - dropout: standard dropout argument. tie_dropout=True ties the dropout mask across the sequence length, emulating nn.Dropout1d
+    - transposed: Choose backbone axis ordering of (B, L, H) (if False) or (B, H, L) (if True) [B=batch size, L=sequence length, H=model dimension]
+    Other options are all experimental and should not need to be configured.
+    """
+
+    def __init__(
+        self,
+        d_model,
+        bottleneck=None,
+        gate=None,
+        gate_act=None,
+        mult_act=None,
+        final_act='glu',
+        postact=None,
+        initializer=None,
+        weight_norm=False,
+        dropout=0.0,
+        tie_dropout=False,
+        transposed=True,
+        **layer_args,  # Arguments into inner layer (e.g. FFTConv)
+    ):
+        super().__init__()
+
+        self.d_model = d_model
+        self.transposed = transposed
+
+        self.gate = gate
+        self.bottleneck = bottleneck
+
+        if bottleneck is not None:
+            self.d_model = self.d_model // bottleneck
+            self.input_linear = LinearActivation(
+                self.d_model,
+                self.d_model,
+                transposed=False,
+                activation=None,
+                activate=False,
+            )
+
+        if gate is not None:
+            self.input_gate = LinearActivation(
+                self.d_model,
+                self.d_model * gate,
+                transposed=False,
+                activation=gate_act,
+                activate=True,
+            )
+            if self.layer.d_output != self.d_model * gate:
+                self.output_gate = LinearActivation(
+                    self.d_model*self.channels,
+                    self.d_model * gate,
+                    transposed=False,
+                    activation=None,
+                    activate=False,
+                )
+
+        # Currently this module only uses FFTConv for its inner module
+        # But the options here are all agnostic to the inner block
+        # If other types of inner layers are desired, it is easy
+        # to add an option to swap a different module in
+        self.layer = FFTConv(d_model, transposed=False, dropout=dropout, tie_dropout=tie_dropout, **layer_args)
+
+        # Pointwise operations
+
+        # Activation after (optional) multiplication by gate branch
+        self.mult_activation = Activation(mult_act)
+        # dropout_fn = nn.Dropout2d if self.transposed else nn.Dropout # Broken in torch==1.11
+        dropout_fn = partial(DropoutNd, transposed=False) if tie_dropout else nn.Dropout
+        self.drop = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()
+
+        # position-wise output transform to mix features
+        if postact is not None:
+            assert final_act is None
+            log.warning("Warning: 'postact' option changed to 'final_act' and will be removed in a future version.")
+            final_act, postact = postact, final_act
+        if final_act is None:
+            self.output_linear = nn.Identity()
+        else:
+            self.output_linear = LinearActivation(
+                self.d_model*gate if gate is not None else self.layer.d_output,
+                self.d_model,
+                transposed=False,
+                activation=final_act,
+                activate=True,
+            )
+
+
+
+    def forward(self, x, lengths=None, **kwargs): # absorbs return_output and transformer src mask
+        """
+        x: (B H L) if self.transposed else (B L H)
+        state: (H N) never needed unless you know what you're doing
+        Returns: same shape as x
+        """
+        if self.transposed: x = rearrange(x, 'b d ... -> b ... d')
+        L = x.size(1)
+
+        # Mask out padding tokens
+        # TODO handle option for mask - instead of lengths, which assumes suffix padding
+        if isinstance(lengths, int):
+            if lengths != L:
+                lengths = torch.tensor(lengths, dtype=torch.long, device=x.device)
+            else:
+                lengths = None
+        if lengths is not None:
+            assert isinstance(lengths, torch.Tensor) and lengths.ndim == 1 and lengths.size(0) in [1, x.size(0)]
+            mask = torch.where(torch.arange(L, device=lengths.device)[:, None] < lengths[:, None, None], 1., 0.)
+            x = x * mask
+
+        if self.gate is not None:
+            v = self.input_gate(x)
+        if self.bottleneck is not None:
+            x = self.input_linear(x)
+
+        y, state = self.layer(x, **kwargs)
+
+
+        if self.gate is not None:
+            y = self.output_gate(y)
+            y = y * v
+        y = self.mult_activation(y)
+        y = self.drop(y)
+        y = self.output_linear(y)
+
+        if self.transposed: y = rearrange(y, 'b d ... -> b ... d')
+
+        return y, state
+
+    def setup_step(self, **kwargs):
+        self.layer.setup_step(**kwargs)
+
+    def step(self, x, state):
+        """Step one time step as a recurrent model. Intended to be used during validation.
+        x: (B H)
+        state: (B H N)
+        Returns: output (B H), state (B H N)
+        """
+
+        if self.gate is not None:
+            v = self.input_gate(x)
+        if self.bottleneck is not None:
+            x = self.input_linear(x)
+        y, next_state = self.layer.step(x, state) # (B C H)
+        if self.gate is not None:
+            y = self.output_gate(y)
+            y = y * v
+        y = self.mult_activation(y)
+        y = self.drop(y)
+        y = self.output_linear(y)
+        return y, next_state
+
+    def default_state(self, *batch_shape, device=None):
+        # kernel is not a SequenceModule so it doesn't need to adhere to same interface
+        # the kernel will know the device of its own parameters
+        return self.layer.default_state(*batch_shape)
+
+    @property
+    def d_output(self):
+        return self.d_model
diff --git a/models/s4v2.py b/models/s4v2.py
new file mode 100644
index 0000000..8dcf7ae
--- /dev/null
+++ b/models/s4v2.py
@@ -0,0 +1,1964 @@
+"""Standalone version of Structured State Space sequence model (S4)."""
+
+from collections import defaultdict
+from typing import Optional, Mapping, Tuple, Union
+import logging
+from functools import partial
+import math
+import numpy as np
+from scipy import special as ss
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from pytorch_lightning.utilities import rank_zero_only
+from einops import rearrange, repeat
+
+# Function aliases
+contract = torch.einsum
+
+_conj = lambda x: torch.cat([x, x.conj()], dim=-1)
+_c2r = torch.view_as_real
+_r2c = torch.view_as_complex
+if tuple(map(int, torch.__version__.split('.')[:2])) >= (1, 10):
+    _resolve_conj = lambda x: x.conj().resolve_conj()
+else:
+    _resolve_conj = lambda x: x.conj()
+
+
+def get_logger(name=__name__, level=logging.INFO) -> logging.Logger:
+    """Initializes multi-GPU-friendly python logger."""
+
+    logger = logging.getLogger(name)
+    logger.setLevel(level)
+
+    # this ensures all logging levels get marked with the rank zero decorator
+    # otherwise logs would get multiplied for each GPU process in multi-GPU setup
+    for level in ("debug", "info", "warning", "error", "exception", "fatal", "critical"):
+        setattr(logger, level, rank_zero_only(getattr(logger, level)))
+
+    return logger
+log = get_logger(__name__)
+
+"""Structured matrix kernels"""
+
+# Try CUDA extension
+try:
+    from extensions.kernels.cauchy import cauchy_mult as cauchy_cuda
+    from extensions.kernels.vandermonde import log_vandermonde_cuda
+    has_cuda_extension = True
+    log.info("CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) found.")
+except:
+    log.warning(
+        "CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled."
+    )
+    has_cuda_extension = False
+
+# Try pykeops
+try:
+    import pykeops
+    from pykeops.torch import Genred
+    has_pykeops = True
+    log.info("Pykeops installation found.")
+
+    def _broadcast_dims(*tensors):
+        max_dim = max([len(tensor.shape) for tensor in tensors])
+        tensors = [tensor.view((1,)*(max_dim-len(tensor.shape))+tensor.shape) for tensor in tensors]
+        return tensors
+
+    def cauchy_keops(v, z, w):
+        expr_num = 'z * ComplexReal(v) - Real2Complex(Sum(v * w))'
+        expr_denom = 'ComplexMult(z-w, z-Conj(w))'
+
+        cauchy_mult = Genred(
+            f'ComplexDivide({expr_num}, {expr_denom})',
+            [
+                'v = Vj(2)',
+                'z = Vi(2)',
+                'w = Vj(2)',
+            ],
+            reduction_op='Sum',
+            axis=1,
+        )
+
+        v, z, w = _broadcast_dims(v, z, w)
+        v = _c2r(v)
+        z = _c2r(z)
+        w = _c2r(w)
+
+        r = 2*cauchy_mult(v, z, w, backend='GPU')
+        return _r2c(r)
+
+    def log_vandermonde_keops(v, x, L):
+        expr = 'ComplexMult(v, ComplexExp(ComplexMult(x, l)))'
+        vandermonde_mult = Genred(
+            expr,
+            [
+                'v = Vj(2)',
+                'x = Vj(2)',
+                'l = Vi(2)',
+            ],
+            reduction_op='Sum',
+            axis=1,
+        )
+
+        l = torch.arange(L).to(x)
+        v, x, l = _broadcast_dims(v, x, l)
+        v = _c2r(v)
+        x = _c2r(x)
+        l = _c2r(l)
+
+        r = vandermonde_mult(v, x, l, backend='GPU')
+        return 2*_r2c(r).real
+
+    def log_vandermonde_transpose_keops(u, v, x, L):
+        """
+        u: ... H L
+        v: ... H N
+        x: ... H N
+        Returns: ... H N
+
+        V = Vandermonde(a, L) : (H N L)
+        contract_L(V * u * v)
+        """
+        expr = 'ComplexMult(ComplexMult(v, u), ComplexExp(ComplexMult(x, l)))'
+        vandermonde_mult = Genred(
+            expr,
+            [
+                'u = Vj(2)',
+                'v = Vi(2)',
+                'x = Vi(2)',
+                'l = Vj(2)',
+            ],
+            reduction_op='Sum',
+            axis=1,
+        )
+
+        l = torch.arange(L).to(x)
+        u, v, x, l = _broadcast_dims(u, v, x, l)
+        u = _c2r(u)
+        v = _c2r(v)
+        x = _c2r(x)
+        l = _c2r(l)
+
+        r = vandermonde_mult(u, v, x, l, backend='GPU')
+        return _r2c(r)
+
+except ImportError:
+    has_pykeops = False
+    if not has_cuda_extension:
+        log.warning(
+            "Falling back on slow Cauchy and Vandermonde kernel. Install at least one of pykeops or the CUDA extension for better speed and memory efficiency."
+        )
+
+# Fallback versions
+def cauchy_naive(v, z, w):
+    """
+    v: (..., N)
+    z: (..., L)
+    w: (..., N)
+    returns: (..., L) \sum v/(z-w)
+    """
+    v = _conj(v)
+    w = _conj(w)
+    cauchy_matrix = v.unsqueeze(-1) / (z.unsqueeze(-2) - w.unsqueeze(-1)) # (... N L)
+    return torch.sum(cauchy_matrix, dim=-2)
+
+def log_vandermonde_naive(v, x, L, conj=True):
+    """
+    v: (..., N)
+    x: (..., N)
+    returns: (..., L) \sum v x^l
+    """
+    vandermonde_matrix = torch.exp(x.unsqueeze(-1) * torch.arange(L).to(x)) # (... N L)
+    vandermonde_prod = contract('... n, ... n l -> ... l', v, vandermonde_matrix) # (... L)
+    return 2*vandermonde_prod.real
+
+def log_vandermonde_transpose_naive(u, v, x, L):
+    vandermonde_matrix = torch.exp(x.unsqueeze(-1) * torch.arange(L).to(x)) # (... N L)
+    vandermonde_prod = contract('... l, ... n, ... n l -> ... n', u.to(x), v.to(x), vandermonde_matrix) # (... L)
+    return vandermonde_prod
+
+
+
+""" Simple nn.Module components """
+
+def Activation(activation=None, dim=-1):
+    if activation in [ None, 'id', 'identity', 'linear' ]:
+        return nn.Identity()
+    elif activation == 'tanh':
+        return nn.Tanh()
+    elif activation == 'relu':
+        return nn.ReLU()
+    elif activation == 'gelu':
+        return nn.GELU()
+    elif activation == 'elu':
+        return nn.ELU()
+    elif activation in ['swish', 'silu']:
+        return nn.SiLU()
+    elif activation == 'glu':
+        return nn.GLU(dim=dim)
+    elif activation == 'sigmoid':
+        return nn.Sigmoid()
+    elif activation == 'softplus':
+        return nn.Softplus()
+    else:
+        raise NotImplementedError("hidden activation '{}' is not implemented".format(activation))
+
+def LinearActivation(
+        d_input, d_output, bias=True,
+        transposed=False,
+        activation=None,
+        activate=False, # Apply activation as part of this module
+        **kwargs,
+    ):
+    """Returns a linear nn.Module with control over axes order, initialization, and activation."""
+
+    # Construct core module
+    linear_cls = partial(nn.Conv1d, kernel_size=1) if transposed else nn.Linear
+    if activation is not None and activation == 'glu': d_output *= 2
+    linear = linear_cls(d_input, d_output, bias=bias, **kwargs)
+
+    if activate and activation is not None:
+        activation = Activation(activation, dim=-2 if transposed else -1)
+        linear = nn.Sequential(linear, activation)
+    return linear
+
+class DropoutNd(nn.Module):
+    def __init__(self, p: float = 0.5, tie=True, transposed=True):
+        """
+        tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)
+        """
+        super().__init__()
+        if p < 0 or p >= 1:
+            raise ValueError("dropout probability has to be in [0, 1), " "but got {}".format(p))
+        self.p = p
+        self.tie = tie
+        self.transposed = transposed
+        self.binomial = torch.distributions.binomial.Binomial(probs=1-self.p)
+
+    def forward(self, X):
+        """X: (batch, dim, lengths...)."""
+        if self.training:
+            if not self.transposed: X = rearrange(X, 'b ... d -> b d ...')
+            mask_shape = X.shape[:2] + (1,)*(X.ndim-2) if self.tie else X.shape
+            mask = torch.rand(*mask_shape, device=X.device) < 1.-self.p
+            X = X * mask * (1.0/(1-self.p))
+            if not self.transposed: X = rearrange(X, 'b d ... -> b ... d')
+            return X
+        return X
+
+"""Misc functional utilities"""
+
+def power(L, A, v=None):
+    """Compute A^L and the scan sum_i A^i v_i.
+
+    A: (..., N, N)
+    v: (..., N, L)
+    """
+
+    I = torch.eye(A.shape[-1]).to(A) # , dtype=A.dtype, device=A.device)
+
+    powers = [A]
+    l = 1
+    while True:
+        if L % 2 == 1: I = powers[-1] @ I
+        L //= 2
+        if L == 0: break
+        l *= 2
+        if v is None:
+            powers = [powers[-1] @ powers[-1]]
+        else:
+            powers.append(powers[-1] @ powers[-1])
+
+    if v is None: return I
+
+    # Invariants:
+    # powers[-1] := A^l
+    # l := largest po2 at most L
+
+    # Note that an alternative divide and conquer to compute the reduction is possible and can be embedded into the above loop without caching intermediate powers of A
+    # We do this reverse divide-and-conquer for efficiency reasons:
+    # 1) it involves fewer padding steps for non-po2 L
+    # 2) it involves more contiguous arrays
+
+    # Take care of edge case for non-po2 arrays
+    # Note that this initial step is a no-op for the case of power of 2 (l == L)
+    k = v.size(-1) - l
+    v_ = powers.pop() @ v[..., l:]
+    v = v[..., :l]
+    v[..., :k] = v[..., :k] + v_
+
+    # Handle reduction for power of 2
+    while v.size(-1) > 1:
+        v = rearrange(v, '... (z l) -> ... z l', z=2)
+        v = v[..., 0, :] + powers.pop() @ v[..., 1, :]
+    return I, v.squeeze(-1)
+
+
+"""HiPPO utilities"""
+
+def transition(measure, N, **measure_args):
+    """A, B transition matrices for different measures.
+
+    measure: the type of measure
+      legt - Legendre (translated)
+      legs - Legendre (scaled)
+      glagt - generalized Laguerre (translated)
+      lagt, tlagt - previous versions of (tilted) Laguerre with slightly different normalization
+    """
+    # Legendre (translated)
+    if measure == 'legt':
+        Q = np.arange(N, dtype=np.float64)
+        R = (2*Q + 1) ** .5
+        j, i = np.meshgrid(Q, Q)
+        A = R[:, None] * np.where(i < j, (-1.)**(i-j), 1) * R[None, :]
+        B = R[:, None]
+        A = -A
+
+        # Halve again for timescale correctness
+        A *= 0.5
+        B *= 0.5
+    # Legendre (scaled)
+    elif measure == 'legs':
+        q = np.arange(N, dtype=np.float64)
+        col, row = np.meshgrid(q, q)
+        r = 2 * q + 1
+        M = -(np.where(row >= col, r, 0) - np.diag(q))
+        T = np.sqrt(np.diag(2 * q + 1))
+        A = T @ M @ np.linalg.inv(T)
+        B = np.diag(T)[:, None]
+        B = B.copy() # Otherwise "UserWarning: given NumPY array is not writeable..." after torch.as_tensor(B)
+    elif measure in ['fourier', 'fout']:
+        freqs = np.arange(N//2)
+        d = np.stack([np.zeros(N//2), freqs], axis=-1).reshape(-1)[1:]
+        A = np.pi*(-np.diag(d, 1) + np.diag(d, -1))
+        B = np.zeros(N)
+        B[0::2] = 2**.5
+        B[0] = 1
+
+        # Subtract off rank correction - this corresponds to the other endpoint u(t-1) in this case
+        A = A - B[:, None] * B[None, :]
+        B = B[:, None]
+    else:
+        raise NotImplementedError
+
+    return A, B
+
+def rank_correction(measure, N, rank=1, dtype=torch.float):
+    """Return low-rank matrix L such that A + L is normal."""
+
+    if measure == 'legs':
+        assert rank >= 1
+        P = torch.sqrt(.5+torch.arange(N, dtype=dtype)).unsqueeze(0) # (1 N)
+    elif measure == 'legt':
+        assert rank >= 2
+        P = torch.sqrt(1+2*torch.arange(N, dtype=dtype)) # (N)
+        P0 = P.clone()
+        P0[0::2] = 0.
+        P1 = P.clone()
+        P1[1::2] = 0.
+        P = torch.stack([P0, P1], dim=0) # (2 N)
+        P *= 2**(-0.5) # Halve the rank correct just like the original matrix was halved
+    elif measure in ['fourier', 'fout']:
+        P = torch.zeros(N)
+        P[0::2] = 2**.5
+        P[0] = 1
+        P = P.unsqueeze(0)
+    else: raise NotImplementedError
+
+    d = P.size(0)
+    if rank > d:
+        P = torch.cat([P, torch.zeros(rank-d, N, dtype=dtype)], dim=0) # (rank N)
+    return P
+
+def nplr(measure, N, rank=1, dtype=torch.float, diagonalize_precision=True, B_clip=2.0):
+    """Constructs NPLR form of HiPPO matrices.
+
+    Returns w, p, q, V, B such that
+    (w - p q^*, B) is unitarily equivalent to the original HiPPO A, B by the matrix V
+    i.e. A = V[w - p q^*]V^*, B = V B
+
+    measure: Name of HiPPO method.
+    N: Size of recurrent A matrix (also known as `d_state` elsewhere).
+    dtype: Single or double precision.
+    diagonalize_precision: Calculate diagonalization in double precision.
+    B_clip: Clip values of B, can help with stability. None for no clipping.
+    """
+
+    assert dtype == torch.float or dtype == torch.double
+    cdtype = torch.cfloat if dtype == torch.float else torch.cdouble
+
+    A, B = transition(measure, N)
+    A = torch.as_tensor(A, dtype=dtype) # (N, N)
+    B = torch.as_tensor(B, dtype=dtype)[:, 0] # (N,)
+
+    P = rank_correction(measure, N, rank=rank, dtype=dtype) # (r N)
+    AP = A + torch.sum(P.unsqueeze(-2)*P.unsqueeze(-1), dim=-3)
+
+    # We require AP to be nearly skew-symmetric
+    _A = AP + AP.transpose(-1, -2)
+    if (err := torch.sum((_A - _A[0,0]*torch.eye(N))**2) / N) > 1e-5: # if not torch.allclose(_A - _A[0,0]*torch.eye(N), torch.zeros(N, N), atol=1e-5):
+        print("WARNING: HiPPO matrix not skew symmetric", err)
+
+
+    # Take advantage of identity + skew-symmetric form to calculate real and imaginary parts separately
+    # Imaginary part can use eigh instead of eig
+    W_re = torch.mean(torch.diagonal(AP), -1, keepdim=True)
+
+    # Diagonalize in double precision
+    if diagonalize_precision: AP = AP.to(torch.double)
+    # w, V = torch.linalg.eig(AP) # (..., N) (..., N, N)
+    W_im, V = torch.linalg.eigh(AP*-1j) # (..., N) (..., N, N)
+    if diagonalize_precision: W_im, V = W_im.to(cdtype), V.to(cdtype)
+    W = W_re + 1j * W_im
+    # Check: V W V^{-1} = A
+    # print("check", V @ torch.diag_embed(W) @ V.conj().transpose(-1, -2))
+
+
+    # Only keep half of each conjugate pair
+    _, idx = torch.sort(W.imag)
+    W_sorted = W[idx]
+    V_sorted = V[:, idx]
+
+    # There is an edge case when eigenvalues can be 0, which requires some machinery to handle
+    # We use a huge hack here: Assume only one pair is 0, and that it is the first row/column of A (only happens in Fourier case)
+    V = V_sorted[:, :N//2]
+    W = W_sorted[:N//2]  # Only keep negative imaginary components
+    assert W[-2].abs() > 1e-4, "Only 1 zero eigenvalue allowed in diagonal part of A"
+    if W[-1].abs() < 1e-4:
+        V[:, -1] = 0.
+        V[0, -1] = 2**-0.5
+        V[1, -1] = 2**-0.5 * 1j
+
+    _AP = V @ torch.diag_embed(W) @ V.conj().transpose(-1, -2)
+    if ((err := torch.sum((2*_AP.real-AP)**2)/N) > 1e-5):
+        print("Warning: Diagonalization of A matrix not numerically precise - error", err)
+    # print("check", V @ torch.diag_embed(W) @ V.conj().transpose(-1, -2))
+
+    V_inv = V.conj().transpose(-1, -2)
+
+    # C = initial_C(measure, N, dtype=dtype)
+    B = contract('ij, j -> i', V_inv, B.to(V)) # V^* B
+    # C = contract('ij, j -> i', V_inv, C.to(V)) # V^* C
+    P = contract('ij, ...j -> ...i', V_inv, P.to(V)) # V^* P
+
+    if B_clip is not None:
+        B = B.real + 1j*torch.clamp(B.imag, min=-B_clip, max=B_clip)
+
+    # W represents the imaginary part of the DPLR form: A = W - PP^*
+    # Downstream classes just call this A for simplicity,
+    # which is also more consistent with the diagonal case
+    return W, P, B, V
+
+def dplr(
+    init='hippo',
+    N=64, rank=1, H=1,
+    dtype=torch.float,
+    real_random=False,
+    real_scale=1.0,
+    imag_random=False,
+    imag_scale=1.0,
+    B_random=False,
+    B_init='constant',
+    B_scale=1.0,
+    P_scale=1.0,
+    normalize=False,
+):
+    """Directly construct a DPLR matrix.
+
+    Args:
+    - init: (str) ['rand', 'lin', inv', 'real', 'hippo'] Choices for initialization of A.
+          Most of these affect the imaginary part of A, except for 'real'.
+    - real_random: (bool) Initialize A.real in -U[0, 1]. Otherwise, initialize to -1/2.
+    - real_scale: (float) Scaling factor of real part of A.
+    - imag_random: (bool) Initialize A.imag randomly.
+    - imag_scale: (bool) Scaling factor of imaginary part of A.
+    - B_init: (str) ['constant' | 'random' | 'alternating' | 'unit-cw' | 'unit-ccw' | 'hippo']
+          Choices for initialization of B.
+    - B_scale: (float) Scaling factor for B
+    - P_scale: (float) Scaling factor for P
+    - normalize: (bool) Apply an automatic normalization factor on B
+    """
+    assert dtype == torch.float or dtype == torch.double
+    dtype = torch.cfloat if dtype == torch.float else torch.cdouble
+
+    pi = torch.tensor(math.pi)
+
+    # Construct real part of diagonal A (must be non-negative)
+    if real_random:
+        real_part = torch.rand(H, N//2)
+    else:
+        real_part = .5 * torch.ones(H, N//2)
+    real_part = real_scale * real_part
+
+    # Construct imaginary part of diagonal A (must be non-negative)
+    if imag_random:
+        imag_part = N//2 * torch.rand(H, N//2)
+    else:
+        imag_part = repeat(torch.arange(N//2), 'n -> h n', h=H)
+
+    if init in ['random', 'rand']:
+        imag_part = torch.exp(torch.randn(H, N//2))
+    elif init == 'real':
+        imag_part = 0 * imag_part
+        if real_random:
+            real_part = torch.rand(H, N//2) * N//2
+        else:
+            # This is the S4D-Real method described in the S4D paper
+            # The A matrix is diag(-1, -2, ..., -N), which are the eigenvalues of the HiPPO matrix
+            real_part = 1 + repeat(torch.arange(N//2), 'n -> h n', h=H)
+    elif init in ['linear', 'lin']:
+        imag_part = pi * imag_part
+    elif init in ['inverse', 'inv']: # Based on asymptotics of the default HiPPO matrix
+        imag_part = 1/pi * N * (N/(1+2*imag_part)-1)
+    elif init in ['inverse2', 'inv2']:
+        imag_part = 1/pi * N * (N/(1+imag_part)-1)
+    elif init in ['quadratic', 'quad']:
+        imag_part = 1/pi * (1+2*imag_part)**2
+    elif init in ['legs', 'hippo']:
+        A, _, _, _ = nplr('legs', N)
+        imag_part = -A.imag  # Positive
+    else: raise NotImplementedError
+    imag_part = imag_scale * imag_part
+
+    # Construct diagonal A
+    A = -real_part - 1j * imag_part  # Force negative real and imag
+    assert torch.all(A.real < 1e-4) and torch.all(A.imag <= 0.0)  # Allow some tolerance for numerical precision on real part
+
+    # Initialize B
+    if B_random:
+        log.warning("'B_random' is deprecated in favor of B_init='random' and will be deprecated in a future version.")
+    if init in ['legs', 'hippo']:
+        log.info(f'Initializing with S4D-LegS and ignoring argument {B_init=}')
+        # Special initialization using the HiPPO B matrix
+        # Note that theory (from S4D paper) says that B should be halved
+        # to match DPLR but we drop this 0.5 factor for simplicity
+        _, P, B, _ = nplr('legs', N, B_clip=2.0)
+        B = repeat(B, 'n -> h n', h=H).clone().contiguous()
+    elif B_init == 'constant':
+        B = torch.ones(H, N//2, dtype=dtype)
+    elif B_init == 'random':
+        B = torch.randn(H, N//2, dtype=dtype)
+    elif B_init == 'alternating':  # Seems to track 'constant' exactly for some reason
+        B = torch.ones(H, N//4, 2, dtype=dtype)
+        B[:, :, 1] *= -1
+        B = B.view(H, N//2)
+    elif B_init == 'unit-cw':
+        z = torch.tensor(torch.exp(-2j * pi / N), dtype=dtype)
+        B = z ** torch.arange(0, N // 2)
+        B = repeat(B, 'n -> h n', h=H).clone().contiguous()
+    elif B_init == 'unit-ccw':
+        z = torch.tensor(torch.exp(2j * pi / N), dtype=dtype)
+        B = z ** torch.arange(0, N // 2)
+        B = repeat(B, 'n -> h n', h=H).clone().contiguous()
+    else: raise NotImplementedError
+    B *= B_scale
+
+    # Experimental feature that appeared in earlier versions of HTTYH (not extensively tested)
+    # Seems more principled for normalization theoretically, but seemed to hurt on PathX
+    if normalize:
+        norm = -B/A # (H, N) # Result if you integrate the kernel with constant 1 function
+        zeta = 2*torch.sum(torch.abs(norm)**2, dim=-1, keepdim=True) # Variance with a random C vector
+        B = B / zeta**.5
+
+    # Initialize P
+    if B_init in ['legs', 'hippo']:
+        # P constructed earlier
+        P = repeat(P, 'r n -> r h n', h=H).clone().contiguous()
+    else:
+        P = torch.randn(rank, H, N//2, dtype=dtype)
+        P = P * P_scale
+
+    # Initialize V (only used in testing)
+    V = torch.eye(N, dtype=dtype)[:, :N//2]
+    V = repeat(V, 'n m -> h n m', h=H)
+
+    return A, P, B, V
+
+def ssm(init, N, R, H, **ssm_args):
+    """Dispatcher to create single SSM initialization
+
+    N: state size
+    R: rank (for DPLR parameterization)
+    H: number of independent SSM copies
+    """
+
+    if init.startswith("diag") or init.startswith("dplr"):
+        if init.startswith("diag"):
+            ssm_args["P_scale"] = 0.0
+        args = init[4:].split("-")
+        assert args[0] == ""
+        if len(args) > 1:
+            ssm_args["init"] = args[1]
+        A, P, B, V = dplr(N=N, rank=R, H=H, **ssm_args)
+    else:
+        A, P, B, V = nplr(init, N, R, **ssm_args)
+        A = repeat(A, 'n -> s n', s=H)
+        P = repeat(P, 'r n -> r s n', s=H)
+        B = repeat(B, 'n -> s n', s=H)
+        V = repeat(V, 'n m -> s n m', s=H)
+    return A, P, B, V
+
+combinations = {
+    'hippo': ['legs', 'fourier'],
+    'diag': ['diag-inv', 'diag-lin'],
+    'all': ['legs', 'fourier', 'diag-inv', 'diag-lin'],
+}
+
+def combination(inits, N, R, S, **ssm_args):
+    if isinstance(inits, str):
+        inits = combinations[inits] if inits in combinations else [inits]
+
+    assert S % len(inits) == 0, f"{S} independent trainable SSM copies must be multiple of {len(inits)} different inits"
+    A, P, B, V = zip(
+        *[ssm(init, N, R, S // len(inits), **ssm_args) for init in inits]
+    )
+    A = torch.cat(A, dim=0) # (S N)
+    P = torch.cat(P, dim=1) # (R S N)
+    B = torch.cat(B, dim=0) # (S N)
+    V = torch.cat(V, dim=0) # (S N N)
+    return A, P, B, V
+
+
+"""SSM convolution kernels"""
+
+def inv_transform(param, transform='none'):
+    """Initialize a (positive) parameter under a transform."""
+    param = torch.clamp(param, min=1e-4)
+    if transform == 'none':
+        return param
+    elif transform == 'exp':
+        return torch.log(param) # Some of the HiPPO methods have real part 0
+    elif transform == 'relu':
+        return param
+    elif transform == 'sigmoid':
+        return torch.logit(param)
+    elif transform == 'softplus':
+        return torch.log(torch.exp(param)-1)
+    else: raise NotImplementedError
+
+def param_transform(param, transform='none'):
+    """Get a (positive) parameter under a transform."""
+    if transform == 'none':
+        p = param
+    elif transform == 'exp':
+        p = torch.exp(param)
+    elif transform == 'relu':
+        # JAX version seems to NaN if you allow 0's, although this code was fine without it
+        p = F.relu(param)+1e-4
+    elif transform == 'sigmoid':
+        p = F.sigmoid(param)
+    elif transform == 'softplus':
+        p = F.softplus(param)
+    else: raise NotImplementedError
+    return p
+
+class Kernel(nn.Module):
+    """Interface for modules that produce convolution kernels.
+
+    A main distinction between these and normal Modules is that the forward pass
+    does not take inputs. It is a mapping from parameters to a tensor that can
+    be used in other modules, in particular as a convolution kernel.
+
+    Because of the unusual parameterization, these kernels may often want special
+    hyperparameter settings on their parameters. The `register` method provides
+    an easy interface for controlling this, and is intended to be used with an
+    optimizer hook that can be found in train.py or example.py.
+
+    This class also defines an interface for interacting with kernels *statefully*,
+    in particular for state space models (SSMs). This interface handles the setting
+    when a model can be converted from a "CNN" into an "RNN".
+    _setup_step()
+    step()
+    default_state()
+    forward_state()
+
+    See ConvKernel for the simplest instantiation of this interface.
+    """
+
+    def __init__(
+        self,
+        d_model: int = 0,
+        channels: int = 1,
+        l_max: Optional[int] = None,
+        lr: Union[float, Optional[Mapping]] = None,
+        wd: Union[float, Optional[Mapping]] = 0.0,
+        verbose: bool = True,
+        **kwargs,
+    ):
+        """General interface.
+
+        d_model (H): Model dimension, or number of independent convolution kernels created.
+        channels (C): Extra dimension in the returned output (see .forward()).
+            - One interpretation is that it expands the input dimension giving it C separate "heads" per feature.
+              That is convolving by this kernel maps shape (B L D) -> (B L C D)
+            - This is also used to implement a particular form of bidirectionality in an efficient way.
+            - In general for making a more powerful model, instead of increasing C
+              it is recommended to set channels=1 and adjust H to control parameters instead.
+        l_max (L): Maximum kernel length (optional). If unspecified, most Kernel instantiations
+            will return kernels of arbitrary length as passed into .forward().
+        lr: Optional dictionary specifying special hyperparameters for .register().
+            Passing in a number (e.g. 0.001) sets attributes of SSM parameters (A, B, dt).
+            A custom optimizer hook is needed to configure the optimizer to set the learning rates appropriately for these parameters.
+        wd: Same as lr, but for weight decay.
+        """
+        super().__init__()
+        assert d_model > 0
+        self.H = self.d_model = d_model
+        self.L = self.l_max = l_max
+        self.channels = channels
+        self.lr = lr
+        self.wd = wd
+        self.verbose = verbose
+
+        # Add a catch-all **kwargs to make it easier to change kernels
+        # without manually moving other options passed in the config.
+        # Good to log these just so it's explicit.
+        if self.verbose and len(kwargs) > 0:
+            log.info(f"{type(self)} extra kwargs: {kwargs}")
+
+        # Logic for registering parameters
+        # Case 1: lr: None | float
+        #   All params should have this lr (None means inherit from global lr)
+        # Case 2: lr: dict
+        #   Specified params should have that lr, all others should be None
+        if self.lr is None or isinstance(self.lr, float):
+            self.lr_dict = defaultdict(lambda: self.lr)
+        else:
+            self.lr_dict = defaultdict(lambda: None)
+            self.lr_dict.update(self.lr)
+
+        # Same logic for weight decay
+        # (but is always just set to 0.0 and hasn't been ablated)
+        if self.wd is None or isinstance(self.wd, float):
+            self.wd_dict = defaultdict(lambda: self.wd)
+        else:
+            self.wd_dict = defaultdict(lambda: None)
+            self.wd_dict.update(self.wd)
+
+    def forward(self, state=None, rate=1.0, L=None):
+        """General interface to generate a global convolution kernel.
+
+        state: Initial state for recurrent updates.
+            E.g. for SSMs, this should have shape (B, H, N) (batch, d_model, d_state).
+        rate: Relative sampling rate.
+        L: Target kernel length.
+
+        Returns:
+          - (C, H, L) (channels, d_model, l_kernel) The convolution kernel.
+          - (B, H, L) (batch, d_model, l_kernel)
+              Extra information for how the state affects the output of convolving by kernel.
+        """
+        raise NotImplementedError
+
+    def register(self, name, tensor, lr=None, wd=0.0):
+        """Register a tensor with a configurable learning rate and 0 weight decay"""
+
+        if lr == 0.0:
+            self.register_buffer(name, tensor)
+        else:
+            self.register_parameter(name, nn.Parameter(tensor))
+
+            optim = {}
+            if lr is not None: optim["lr"] = lr
+            if wd is not None: optim["weight_decay"] = wd
+            setattr(getattr(self, name), "_optim", optim)
+
+    def _setup_step(self, **kwargs):
+        """Convert a model into a recurrent mode for autoregressive inference."""
+        raise NotImplementedError
+
+    def step(self, x, state, **kwargs):
+        """Step the model for one timestep with input x and recurrent state."""
+        raise NotImplementedError
+
+    def default_state(self, *args, **kwargs):
+        """Return a default initial state."""
+        raise NotImplementedError
+
+    @torch.no_grad()
+    def forward_state(self, u, state):
+        """Forward the state through a sequence, i.e. computes the state after passing chunk through the kernel."""
+        raise NotImplementedError
+
+    @property
+    def d_state(self):
+        """Implement this for interfaces that want to interact with a stateful layer (i.e. SSMs).
+
+        Currently the only codepath that might use this is the StateDecoder, which is not used.
+        """
+        raise NotImplementedError
+
+    @property
+    def state_to_tensor(self):
+        """Same as d_state, only needed for niche codepaths involving recurrent state."""
+        raise NotImplementedError
+
+class SSMKernel(Kernel):
+    """Parent class for different SSM parameterizations.
+
+    This class is abstract and only defines some initializations and flags that are common to all SSM variants.
+    It is instantiated by subclasses SSMKernel{Dense,Real,Diag,DPLR}.
+
+    Options:
+    d_state (N): State size (dimensionality of parameters A, B, C). Generally shouldn't need to be adjusted and doens't affect speed much for most kernels (e.g. S4, S4D).
+    deterministic: Use a deterministic initialization for dt, A, B, C.
+        Useful for debugging as well as constructing a simple exponential decay kernel (e.g. used in S4ND image->video inflation).
+
+    dt_min, dt_max: min and max values for the step size dt
+    dt_tie: Keep dt tied across the N dimensions of the state. Although this theoretically makes more sense, models such as S5 and Mega have found slightly improvements by setting it to False.
+    dt_transform: Transform function for parameterization of dt (default 'softplus', used to be 'exp')
+
+    rank: Rank of low-rank correction for DPLR mode. Needs to be increased for init "legt".
+    n_ssm: Number of independent trainable (A, B) SSMs, e.g.
+        `n_ssm=1` means all A/B parameters are tied across the H different instantiations of C.
+        `n_ssm=None` means all H SSMs are completely independent.
+        Generally, changing this option can save parameters but doesn't affect performance or speed much.
+        This parameter must divide H.
+    init: Options for initialization of (A, B). For DPLR mode, recommendations are "legs", "fout", "hippo" (combination of both). For Diag mode, recommendations are "diag-inv", "diag-lin", "diag-legs", and "diag" (combination of diag-inv and diag-lin).
+    init_args: Extra arguments passed into initialization function (see dplr.py for options).
+    """
+
+    def init_dt(self):
+        # Generate dt
+        if self.deterministic:  # Meant for debugging
+            assert self.dt_tie, "Deterministic dt initialization is tied"
+            assert self.dt_transform == 'exp', "Deterministic dt transform should be 'exp' for simplicity"
+            inv_dt = torch.exp(torch.linspace(math.log(self.dt_min), math.log(self.dt_max), self.H)).unsqueeze(-1) # (H 1)
+        else:
+            shape = (self.H, 1) if self.dt_tie else (self.H, self.N//2)
+            # Initialize log dt
+            inv_dt = torch.rand(*shape, dtype=self.dtype) * (
+                math.log(self.dt_max) - math.log(self.dt_min)
+            ) + math.log(self.dt_min)
+            if self.dt_transform != 'exp':
+                inv_dt = inv_transform(torch.exp(inv_dt), self.dt_transform)
+
+        return inv_dt
+
+    def init_ssm_real(self):
+        """Returns (dense, real) (A, B, C) parameters for init options."""
+        # Generate A, B
+        A, B = transition(self.init, self.N)
+        A = torch.as_tensor(A, dtype=self.dtype)
+        B = torch.as_tensor(B, dtype=self.dtype)[:, 0]
+        B = repeat(B, 'n -> v n', v=self.n_ssm).clone().contiguous()
+        A = repeat(A, 'n m -> v n m', v=self.n_ssm).clone().contiguous()
+
+        # Generate C
+        if self.deterministic:
+            C = torch.zeros(self.channels, self.H, self.N, dtype=self.dtype)
+            C[..., :1] = 1.0
+        else:
+            C = torch.randn(self.channels, self.H, self.N, dtype=self.dtype)
+
+        return A, B, C
+
+    def init_ssm_dplr(self):
+        """Returns DPLR (A, P, B, C) parameters for init options."""
+        A, P, B, V = combination(self.init, self.N, self.rank, self.n_ssm, **self.init_args)
+
+        # Broadcast C to have H channels
+        if self.deterministic:
+            C = torch.zeros(self.channels, self.n_ssm, self.N, dtype=self.cdtype)
+            C[:, :, :1] = 1.
+            C = contract('hmn, chn -> chm', V.conj().transpose(-1, -2), C) # V^* C
+            C = repeat(C, 'c t n -> c (v t) n', v=self.H // C.size(-2)).clone().contiguous()
+        else:
+            C = torch.randn(self.channels, self.H, self.N//2, dtype=self.cdtype)
+
+        # Broadcast other parameters to have n_ssm copies
+        assert self.n_ssm % B.size(-2) == 0 \
+                and self.n_ssm % P.size(-2) == 0 \
+                and self.n_ssm % A.size(-2) == 0
+
+        # Broadcast tensors to n_ssm copies
+        # These will be the parameters, so make sure tensors are materialized and contiguous
+        B = repeat(B, 't n -> (v t) n', v=self.n_ssm // B.size(-2)).clone().contiguous()
+        P = repeat(P, 'r t n -> r (v t) n', v=self.n_ssm // P.size(-2)).clone().contiguous()
+        A = repeat(A, 't n -> (v t) n', v=self.n_ssm // A.size(-2)).clone().contiguous()
+
+        # Because these complex parameterizations assume conjugate symmetry,
+        # halve the value of self.N for convenience
+        self.N //= 2
+
+        return A, P, B, C
+
+    def __init__(
+        self,
+        # General Kernel arguments for parent class
+        d_model: int = 0,
+        channels: int = 1,
+        l_max: Optional[int] = None,
+        lr: Union[float, Optional[Mapping]] = None,
+        wd: Union[float, Optional[Mapping]] = 0.0,
+        verbose: bool = True,
+        # SSM arguments
+        d_state: int = 64,
+        deterministic: bool = False,
+        # dt options
+        dt_min: float = 0.001,
+        dt_max: float = 0.1,
+        dt_tie: bool = True,
+        dt_transform: str = 'exp',
+        # (A, B, C) options
+        rank: int = 1,
+        n_ssm: Optional[int] = None,
+        measure: Optional[str] = None,
+        init: Optional[str] = "legs",
+        # Extra hyperparameters for initialization
+        **init_args,
+    ):
+        super().__init__(d_model=d_model, channels=channels, l_max=l_max, lr=lr, wd=wd, verbose=verbose)
+        self.N = d_state
+        self.dtype, self.cdtype = torch.float, torch.cfloat
+        self.deterministic = deterministic
+        # dt options
+        self.dt_min = dt_min
+        self.dt_max = dt_max
+        self.dt_tie = dt_tie
+        self.dt_transform = dt_transform
+        # SSM options (A, B, C)
+        self.rank = rank
+        self.n_ssm = n_ssm if n_ssm is not None else self.H
+        if measure is not None:
+            log.warning("Warning: 'measure' option changed to 'init' and will be removed in a future version.")
+            assert init is None, "'measure' and 'init' cannot both be passed into SSMKernel"
+            init, measure = measure, init
+        self.init = init
+        self.init_args = init_args
+
+    @torch.no_grad()
+    def forward_state(self, u, state):
+        """Forward the state through a sequence, i.e. computes the state after passing chunk through SSM
+
+        This is a generic version of this functionality that works for SSMs.
+        It is currently used by SSMKernelDense and SSMKernelDPLR.
+        This is a suboptimal implementation; it is recommended to use SSMKernelDiag
+        if this functionality is desired.
+
+        state: (B, H, N)
+        u: (B, H, L)
+
+        Returns: (B, H, N)
+        """
+
+        # Construct dA, dB matrices
+        dA, dB = self._setup_state() # (H N N) (H N)
+
+        conj = state.size(-1) != dA.size(-1)
+        if conj: state = _conj(state)
+
+        v = contract('h n, b h l -> b h n l', dB, u.flip(-1))
+        AL, v = power(u.size(-1), dA, v)
+        next_state = contract("h m n, b h n -> b h m", AL, state)
+        next_state = next_state + v
+
+        if conj: next_state = next_state[..., : next_state.size(-1) // 2]
+        return next_state
+
+    def _setup_state(self):
+        """Register dA and dB to module."""
+        raise NotImplementedError
+
+    @property
+    def d_state(self):
+        """d_state and state_to_tensor are used by specific decoders.
+
+        These were used in earlier versions and should not be needed in general.
+        """
+        return self.H * self.N
+
+    @property
+    def state_to_tensor(self):
+        return lambda state: rearrange('... h n -> ... (h n)', state)
+
+
+class SSMKernelDiag(SSMKernel):
+    """SSM kernel using diagonal state matrix (S4D model).
+
+    Options:
+    disc: ['zoh' | 'bilinear' | 'dss'] Discretization options.
+    dt_fast:  (experimental) Parameterize inv_dt under sinh function.
+        (Ohno et al. "Fast Saturating Gate for Learning Long Time Scales with RNNs")
+    real_transform, imag_transform: ['none' | 'exp' | 'relu' | 'sigmoid' | 'softplus']
+        Parameterize the real/imag parts of the diagonal of A under this function.
+    bandlimit: Mask high frequencies of the kernel (indices corresponding to
+        diagonal elements with large imaginary part). Introduced in S4ND paper.
+    backend: ['cuda' | 'keops' | 'naive'] Options for Vandermonde/Cauchy kernel (in order of efficiency).
+    is_real : Real-valued SSM; can be interpreted as EMA.
+    """
+
+    def __init__(
+        self,
+        disc: str = 'zoh',  # Change to 'bilinear' to match S4, but should make little difference either way
+        dt_fast: bool = False,
+        real_transform: str = 'exp',
+        imag_transform: str = 'none',
+        bandlimit: Optional[float] = None,
+        backend: str = 'cuda',
+        is_real: bool = False,
+        **kwargs,
+    ):
+        # Special case: for real-valued, d_state semantics change
+        if is_real and 'd_state' in kwargs:
+            kwargs['d_state'] = kwargs['d_state'] * 2
+        super().__init__(**kwargs)
+        self.disc = disc
+        self.dt_fast = dt_fast
+        self.real_transform = real_transform
+        self.imag_transform = imag_transform
+        self.bandlimit = bandlimit
+        self.backend = backend
+        self.is_real = is_real
+
+        # Initialize dt, A, B, C
+        inv_dt = self.init_dt()
+        A, P, B, C = self.init_ssm_dplr()
+        # Note that in the Diag case, P will be ignored
+        # The DPLR case subclasses this and uses P
+        self.register_params(A, B, C, inv_dt, P)
+
+    def register_params(self, A, B, C, inv_dt, P):
+        """Process the initialization into form of trainable parameters.
+
+        A: (S, N) diagonal matrix
+        B: (S, N)
+        C: (C, H, N)
+        dt: (H) timescale per feature
+
+        Dimensions:
+        N (or d_state): state size
+        H (or d_model): total SSM copies
+        S (or n_ssm): number of trainable copies of (A, B, dt); must divide H
+        C (or channels): system is 1-dim to C-dim
+
+        The forward pass of this Module returns a tensor of shape (C, H, L)
+
+        Note: tensor shape N here denotes half the true state size, because of conjugate symmetry
+        """
+
+        assert self.backend in ['cuda', 'keops', 'naive']
+
+        if self.dt_fast: inv_dt = torch.asinh(inv_dt)
+
+        # Rank of low-rank correction
+        assert self.H == inv_dt.size(0)
+        assert self.N == A.size(-1) == B.size(-1) == C.size(-1)
+        assert self.n_ssm == A.size(-2) == B.size(-2) # Number of independent SSMs trained
+        self.repeat = self.H // A.size(0)
+
+        # Check that diagonal part has negative real and imag part
+        # (allow some tolerance for numerical precision on real part
+        # since it may be constructed by a diagonalization)
+        assert torch.all(A.real < 1e-4) and torch.all(A.imag <= 0.0)
+
+        # Broadcast everything to correct shapes
+        C = C.expand(torch.broadcast_shapes(C.shape, (1, self.H, self.N))) # (C, H, N)  # TODO originally this was only in DPLR, check safe for Diag
+        B = B.unsqueeze(0) # (1, H, N)
+        assert self.channels == C.shape[0]
+
+        # Register dt
+        self.register("inv_dt", inv_dt, self.lr_dict['dt'], self.wd_dict['dt'])
+        # Register ABC
+        if self.is_real:
+            self.register("C", C.real, self.lr_dict['C'], None)
+            self.register("B", B.real, self.lr_dict['B'], self.wd_dict['B'])
+            self.register("A_real", inv_transform(-A.real, self.real_transform), self.lr_dict['A'], self.wd_dict['A'])
+        else:
+            self.register("C", _c2r(_resolve_conj(C)), self.lr_dict['C'], None)
+            self.register("B", _c2r(B), self.lr_dict['B'], self.wd_dict['B'])
+            self.register("A_real", inv_transform(-A.real, self.real_transform), self.lr_dict['A'], self.wd_dict['A'])
+            self.register("A_imag", inv_transform(-A.imag, self.imag_transform), self.lr_dict['A'], self.wd_dict['A'])
+
+    def _get_params(self, rate=1.0):
+        """Process the internal parameters."""
+
+        # (S N) where S=n_ssm
+        if self.is_real:
+            A = -param_transform(self.A_real, self.real_transform)
+            B = self.B # (1 S N)
+            C = self.C # (C H N)
+        else:
+            A = -param_transform(self.A_real, self.real_transform) - 1j * param_transform(self.A_imag, self.imag_transform)
+            B = _r2c(self.B) # (1 S N)
+            C = _r2c(self.C) # (C H N)
+
+        if self.dt_fast: inv_dt = torch.sinh(self.inv_dt)
+        else: inv_dt = self.inv_dt
+        dt = param_transform(inv_dt, self.dt_transform) * rate # (H N)
+
+        if self.bandlimit is not None:
+            freqs = dt / rate * A.imag.abs() / (2*math.pi) # (H N)
+            mask = torch.where(freqs < self.bandlimit * .5, 1, 0)
+            C = C * mask
+
+        # Incorporate dt into A and B
+        A = repeat(A, 't n -> (v t) n', v=self.repeat)  # (H N)
+        B = repeat(B, 'b t n -> b (v t) n', v=self.repeat)  # (1 H N)
+
+        # TODO: The downstream algorithm should only need to access dt*A
+        # However the current DPLR kernel still uses dt and A separately
+        # Once that is fixed, this should return dtA instead of dt and A
+        dtA = dt * A  # (H N)
+
+        return dt, A, B, C
+
+    def forward(self, L, state=None, rate=1.0):
+        """See Kernel.forward() for argument documentation."""
+
+        dt, A, B, C = self._get_params(rate)
+        dtA = dt * A
+
+        # Augment B with state
+        if state is not None:
+            s = state / dt
+            if self.disc == 'bilinear':
+                s = s * (1. + dtA/2)
+            elif self.disc == 'zoh':
+                s = s * dtA * dtA.exp() / (dtA.exp() - 1.)
+            B = torch.cat([s, B], dim=-3) # (1+B H N)
+
+
+        # Combine B and C
+        C = (B[:, None, :, :] * C).view(-1, self.H, self.N)
+
+        # Dispatch which Vandermonde kernel to use
+        if has_cuda_extension and C.dtype == torch.cfloat and C.device.type == 'cuda' and self.backend == 'cuda':
+            log_vandermonde = log_vandermonde_cuda
+        elif has_pykeops and self.backend in ['cuda', 'keops']:
+            log_vandermonde = log_vandermonde_keops
+        else:
+            log_vandermonde = log_vandermonde_naive
+
+        # Main kernel
+        if self.disc == 'zoh':
+            # Power up
+            C = C * (torch.exp(dtA)-1.) / A
+            K = log_vandermonde(C, dtA, L) # (H L)
+        elif self.disc == 'bilinear':
+            C = C * (1. - dtA/2).reciprocal() * dt # or * dtA / A
+            dA = (1. + dtA/2) / (1. - dtA/2)
+            K = log_vandermonde(C, dA.log(), L)
+        elif self.disc == 'dss':
+            # Implementation from DSS meant for case when real eigenvalues can be positive
+            P = dtA.unsqueeze(-1) * torch.arange(L, device=C.device) # [H N L]
+            A_gt_0 = A.real > 0                                      # [N]
+            if A_gt_0.any():
+                with torch.no_grad():
+                    P_max = dtA * (A_gt_0 * (L-1))                   # [H N]
+                P = P - P_max.unsqueeze(-1)                          # [H N L]
+            S = P.exp()                                              # [H N L]
+
+            dtA_neg = dtA * (1 - 2*A_gt_0)                           # [H N]
+            num = dtA_neg.exp() - 1                                  # [H N]
+            den = (dtA_neg * L).exp() - 1                            # [H N]
+
+            # Inline reciprocal function for DSS logic
+            x = den * A
+            x_conj = _resolve_conj(x)
+            r = x_conj / (x*x_conj + 1e-7)
+
+            C = C * num * r             # [C H N]
+            K = contract('chn,hnl->chl', C, S).float()
+        else: raise ValueError(f"Discretization {self.disc} not supported")
+
+        K = K.view(-1, self.channels, self.H, L) # (1+B C H L)
+
+        if state is not None:
+            K_state = K[:-1, :, :, :] # (B C H L)
+        else:
+            K_state = None
+        K = K[-1, :, :, :] # (C H L)
+
+        return K, K_state
+
+    def _setup_step(self):
+        """Set up dA, dB, dC discretized parameters for stepping."""
+
+        dt, A, B, C, = self._get_params()
+        # Incorporate dt into A
+        dtA = dt * A  # (H N)
+
+        if self.disc == 'zoh':
+            self.dA = torch.exp(dtA) # (H N)
+            self.dB = B * (torch.exp(dtA)-1.) / A # (C H N)
+        elif self.disc == 'bilinear':
+            self.dA = (1. + dtA/2) / (1. - dtA/2)
+            self.dB = B * (1. - dtA/2).reciprocal() * dt # or * dtA / A
+        self.dB = rearrange(self.dB, '1 h n -> h n')
+        self.dC = C
+
+    def default_state(self, *batch_shape):
+        C = _r2c(self.C)
+        state = torch.zeros(*batch_shape, self.H, self.N, dtype=C.dtype, device=C.device)
+        return state
+
+    def step(self, u, state):
+        next_state = contract("h n, b h n -> b h n", self.dA, state) \
+                + contract("h n, b h -> b h n", self.dB, u)
+        y = contract("c h n, b h n -> b c h", self.dC, next_state)
+        return 2*y.real, next_state
+
+    def forward_state(self, u, state):
+        """Pass the state forward through an entire sequence."""
+        self._setup_step()
+        AL = self.dA ** u.size(-1)
+        u = u.flip(-1).to(self.dA).contiguous() # (B H L)
+        # Dispatch which Vandermonde kernel to use
+        if has_pykeops and self.backend in ['cuda', 'keops']:
+            log_vandermonde_transpose = log_vandermonde_transpose_keops
+        else:
+            log_vandermonde_transpose = log_vandermonde_transpose_naive
+        v = log_vandermonde_transpose(u, self.dB, self.dA.log(), u.size(-1))
+        next_state = AL * state + v
+        return next_state
+
+
+class SSMKernelDPLR(SSMKernelDiag):
+    """SSM kernel for diagonal + low rank (DPLR) state matrices, corresponding to the original S4 model."""
+
+    @torch.no_grad()
+    def _setup_C(self, L):
+        """Construct C~ from C.
+
+        Two modes are supported: go directly to length L if self.l_kernel is 1, or length is doubled
+        """
+
+        if self.l_kernel.item() == 0:
+            if self.verbose: log.info(f"S4: Initializing kernel to length {L}")
+            double_length = False
+        elif L > self.l_kernel.item(): # 2*int(self.l_kernel) == L:
+            if self.verbose: log.info(f"S4: Doubling length from L = {self.l_kernel.item()} to {2*self.l_kernel.item()}")
+            double_length = True
+            L = self.l_kernel.item() # Convenience for the math below
+        else: return
+
+        C = _r2c(self.C)
+        dA, _ = self._setup_state()
+        dA_L = power(L, dA)
+        # Multiply C by I - dA_L
+        C_ = _conj(C)
+        prod = contract("h m n, c h n -> c h m", dA_L.transpose(-1, -2), C_)
+        if double_length: prod = -prod # Multiply by I + dA_L instead
+        C_ = C_ - prod
+        C_ = C_[..., :self.N] # Take conjugate pairs again
+        self.C.copy_(_c2r(C_))
+
+        self.l_kernel = 2*self.l_kernel if double_length else self.l_kernel+L # Preserve type/device
+
+    def _omega(self, L, dtype, device, cache=True):
+        """Calculate (and cache) FFT nodes.
+
+        This also caches a version of the nodes "unprocessed" with the bilinear transform.
+        This method should be called everytime the internal length self.l_kernel changes.
+        """
+
+        # Use cached if available
+        if cache and hasattr(self, 'omega') and self.omega.size(-1) == L//2+1:
+            return self.omega, self.z
+
+        omega = torch.tensor(
+            np.exp(-2j * np.pi / (L)), dtype=dtype, device=device
+        )  # \omega_{2L}
+        omega = omega ** torch.arange(0, L // 2 + 1, device=device)
+        z = 2 * (1 - omega) / (1 + omega)
+
+        # Cache if necessary
+        if cache:
+            self.omega = omega
+            self.z = z
+        return omega, z
+
+
+    def register_params(self, A, B, C, inv_dt, P):
+        """Process the initialization into form of trainable parameters.
+
+        The SSM state matrix is represented by diag_embed(A) - PP^*
+        Note that the A notation here is slightly overloaded:
+        normally A refers to the full SSM state matrix (DPLR in this case)
+        but here we're using it to refer to the diagonal part of the matrix.
+        This is to make variable names compatible with the SSMKernelDiag class (DSS/S4D)
+        and is a much simpler variable name (e.g. as opposed to Lambda).
+
+        A: (S, N) diagonal part
+        P: (R, S, N) low-rank part
+        B: (S, N)
+        C: (C, H, N)
+        dt: (H) timescale per feature
+
+        Dimensions:
+        N (or d_state): state size
+        H (or d_model): total SSM copies
+        S (or n_ssm): number of trainable copies of (A, B, dt); must divide H
+        R (or rank): rank of low-rank part
+        C (or channels): system is 1-dim to C-dim
+
+        The forward pass of this Module returns a tensor of shape (C, H, L)
+
+        Note: tensor shape N here denotes half the true state size, because of conjugate symmetry
+        """
+
+        # Print out kernel lengths; it can be tricky to make sure the length logic is correct
+        if self.verbose:
+            log.info(f"Constructing S4 (H, N, L) = ({self.H}, {self.N}, {self.l_max})")
+
+        # Register the basic params for diagonal SSM (A, B, C, dt)
+        super().register_params(A, B, C, inv_dt, P)
+
+        # Check shapes
+        assert self.rank == P.shape[-3]
+        assert self.N == P.size(-1)
+        assert self.n_ssm == P.size(-2)
+
+        self.register('P', _c2r(P), self.lr_dict['A'], self.wd_dict['A'])
+
+        # Track the current kernel length this is "attuned" to
+        self.register_buffer('l_kernel', torch.tensor(0))
+
+    def _get_params(self, rate=1.0):
+        dt, A, B, C = super()._get_params(rate=rate)
+        P = _r2c(self.P)  # (R S N)
+        P = repeat(P, 'r t n -> r (v t) n', v=self.repeat)  # (R H N)
+        Q = P.conj()
+
+        return dt, A, B, C, P, Q
+
+    def forward(self, state=None, rate=1.0, L=None):
+        """See Kernel.forward() for argument documentation."""
+
+        # Initialize C~ if necessary (done in forward pass so it's on the correct device)
+        if self.l_kernel.item() == 0 and self.l_max is not None and self.l_max > 0:
+            self._setup_C(self.l_max)
+
+        # Handle sampling rate logic
+        # The idea is that this kernel's length (in continuous units) is self.l_kernel, while we are asked to provide a kernel of length L at (relative) frequency rate
+        if L is None:
+            L = round(self.l_kernel.item() / rate)
+
+        # Increase the internal length if needed
+        continuous_L = round(rate*L)
+        while continuous_L > self.l_kernel.item():
+            self._setup_C(continuous_L)
+        discrete_L = round(self.l_kernel.item()/rate)
+
+        dt, A, B, C, P, Q = self._get_params(rate)
+
+        # Get FFT nodes of right length
+        omega, z = self._omega(discrete_L, dtype=A.dtype, device=A.device, cache=(rate==1.0))
+
+        # Augment B
+        if state is not None:
+            # Have to "unbilinear" the state to put it into the same "type" as B
+            # Compute 1/dt * (I + dt/2 A) @ state
+
+            # Can do this without expanding (maybe minor speedup using conj symmetry in theory), but it's easier to read this way
+            s = _conj(state) if state.size(-1) == self.N else state # (B H N)
+            sA = (
+                s * _conj(A) # (B H N)
+                - contract('bhm, rhm, rhn -> bhn', s, _conj(Q), _conj(P))
+            )
+            s = s / dt + sA / 2
+            s = s[..., :self.N]
+
+            B = torch.cat([s, B], dim=-3)  # (B+1, H, N)
+
+        # Incorporate dt into A
+        A = A * dt  # (H N)
+
+        # Stack B and p, C and q for convenient batching
+        B = torch.cat([B, P], dim=-3) # (B+1+R, H, N)
+        C = torch.cat([C, Q], dim=-3) # (C+R, H, N)
+
+        # Incorporate B and C batch dimensions
+        v = B.unsqueeze(-3) * C.unsqueeze(-4)  # (B+1+R, C+R, H, N)
+        v = v * dt  # Incorporate dt into B
+
+        # Dispatch which Cauchy kernel to use
+        if has_cuda_extension and z.dtype == torch.cfloat and z.device.type == 'cuda' and self.backend == 'cuda':
+            cauchy_mult = cauchy_cuda
+        elif has_pykeops and self.backend in ['cuda', 'keops']:
+            cauchy_mult = cauchy_keops
+        else:
+            cauchy_mult = cauchy_naive
+        # Calculate resolvent at omega
+        r = cauchy_mult(v, z, A)
+
+        # Low-rank Woodbury correction
+        if self.rank == 1:
+            k_f = r[:-1, :-1, :, :] - r[:-1, -1:, :, :] * r[-1:, :-1, :, :] / (1 + r[-1:, -1:, :, :])
+        elif self.rank == 2:
+            r00 = r[: -self.rank, : -self.rank, :, :]
+            r01 = r[: -self.rank, -self.rank :, :, :]
+            r10 = r[-self.rank :, : -self.rank, :, :]
+            r11 = r[-self.rank :, -self.rank :, :, :]
+            det = (1 + r11[:1, :1, :, :]) * (1 + r11[1:, 1:, :, :]) - r11[:1, 1:, :, :] * r11[1:, :1, :, :]
+            s = (
+                r01[:, :1, :, :] * (1 + r11[1:, 1:, :, :]) * r10[:1, :, :, :]
+                + r01[:, 1:, :, :] * (1 + r11[:1, :1, :, :]) * r10[1:, :, :, :]
+                - r01[:, :1, :, :] * (r11[:1, 1:, :, :]) * r10[1:, :, :, :]
+                - r01[:, 1:, :, :] * (r11[1:, :1, :, :]) * r10[:1, :, :, :]
+            )
+            s = s / det
+            k_f = r00 - s
+        else:
+            r00 = r[:-self.rank, :-self.rank, :, :]
+            r01 = r[:-self.rank, -self.rank:, :, :]
+            r10 = r[-self.rank:, :-self.rank, :, :]
+            r11 = r[-self.rank:, -self.rank:, :, :]
+            r11 = rearrange(r11, "a b h n -> h n a b")
+            r11 = torch.linalg.inv(torch.eye(self.rank, device=r.device) + r11)
+            r11 = rearrange(r11, "h n a b -> a b h n")
+            k_f = r00 - torch.einsum("i j h n, j k h n, k l h n -> i l h n", r01, r11, r10)
+
+        # Final correction for the bilinear transform
+        k_f = k_f * 2 / (1 + omega)
+
+        # Move from frequency to coefficients
+        k = torch.fft.irfft(k_f, n=discrete_L)  # (B+1, C, H, L)
+
+        # # Truncate to target length
+        k = k[..., :L]
+
+        if state is not None:
+            k_state = k[:-1, :, :, :]  # (B, C, H, L)
+        else:
+            k_state = None
+        k_B = k[-1, :, :, :] # (C H L)
+
+        return k_B, k_state
+
+    @torch.no_grad()
+    def double_length(self):
+        self._setup_C(2*self.l_kernel)
+
+    @torch.no_grad()
+    def _check(self):
+        """Check if A, B, C parameters and vanilla SSMKernel construction can be recovered"""
+
+        # assert self.l_kernel > 0, "Set up module first"
+
+        K = self.forward(L=self.l_max)[0]
+
+        self._setup_step()
+        K_ = krylov(self.l_max, self.dA, self.dB, self.dC)
+
+        diff = K - K_
+        print("checking DPLR Kernel construction", torch.sum(diff ** 2))
+
+    @torch.no_grad()
+    def _setup_linear(self):
+        """Preprocessing that allows fast linear-time (in state dimension) stepping."""
+        dt, A, B, C, P, Q = self._get_params()
+
+        # Prepare Linear stepping
+        D = (2.0 / dt - A).reciprocal()  # (H, N)
+        R = (torch.eye(self.rank, dtype=A.dtype, device=A.device) + 2*contract('r h n, h n, s h n -> h r s', Q, D, P).real) # (H R R)
+        Q_D = rearrange(Q*D, 'r h n -> h r n')
+        try:
+            R = torch.linalg.solve(R, Q_D) # (H R N)
+        except:
+            R = torch.tensor(np.linalg.solve(R.to(Q_D).contiguous().detach().cpu(), Q_D.contiguous().detach().cpu())).to(Q_D)
+        R = rearrange(R, 'h r n -> r h n')
+
+        self.step_params = {
+            "D": D, # (H N)
+            "R": R, # (R H N)
+            "P": P, # (R H N)
+            "Q": Q, # (R H N)
+            "B": B, # (1 H N)
+            "E": 2.0 / dt + A, # (H N)
+        }
+
+    def _step_state_linear(self, u=None, state=None):
+        """
+        Version of the step function that has time O(N) instead of O(N^2) per step, which takes advantage of the DPLR form and bilinear discretization.
+
+        Unfortunately, as currently implemented it's about 2x slower because it calls several sequential operations.
+        Perhaps a fused CUDA kernel implementation would be much faster.
+
+        u: (H) Input
+        state: (H, N/2) State with conjugate pairs. Optionally, the state can have last dimension N.
+
+        Returns: same shape as state
+        """
+        C = _r2c(self.C) # View used for dtype/device
+
+        if u is None: # Special case used to find dA
+            u = torch.zeros(self.H, dtype=C.dtype, device=C.device)
+        if state is None: # Special case used to find dB
+            state = torch.zeros(self.H, self.N, dtype=C.dtype, device=C.device)
+
+        step_params = self.step_params.copy()
+        if state.size(-1) == self.N: # Only store half of the conjugate pairs; should be true by default
+            # There should be a slightly faster way using conjugate symmetry
+            contract_fn = lambda p, x, y: contract('r h n, r h m, ... h m -> ... h n', _conj(p), _conj(x), _conj(y))[..., :self.N] # inner outer product
+        else:
+            assert state.size(-1) == 2*self.N
+            step_params = {k: _conj(v) for k, v in step_params.items()}
+            contract_fn = lambda p, x, y: contract('r h n, r h m, ... h m -> ... h n', p, x, y) # inner outer product
+        D = step_params["D"]  # (H N)
+        E = step_params["E"]  # (H N)
+        R = step_params["R"]  # (R H N)
+        P = step_params["P"]  # (R H N)
+        Q = step_params["Q"]  # (R H N)
+        B = step_params["B"]  # (1 H N)
+
+        new_state = E * state - contract_fn(P, Q, state) # (B H N)
+        new_state = new_state + 2.0 * B * u.unsqueeze(-1)  # (B H N)
+        new_state = D * (new_state - contract_fn(P, R, new_state))
+
+        return new_state
+
+    def _setup_state(self):
+        """Construct dA and dB for discretized state equation."""
+
+        # Construct dA and dB by using the stepping
+        self._setup_linear()
+        C = _r2c(self.C) # Just returns a view that we use for finding dtype/device
+
+        state = torch.eye(2*self.N, dtype=C.dtype, device=C.device).unsqueeze(-2) # (N 1 N)
+        dA = self._step_state_linear(state=state)
+        dA = rearrange(dA, "n h m -> h m n")
+
+        u = C.new_ones(self.H)
+        dB = self._step_state_linear(u=u)
+        dB = _conj(dB)
+        dB = rearrange(dB, '1 h n -> h n') # (H N)
+        return dA, dB
+
+    def _step_state(self, u, state):
+        """Must be called after self.default_state() is used to construct an initial state!"""
+        next_state = (torch.einsum(self.state_contraction, self.dA, state)
+                     + torch.einsum(self.input_contraction, self.dB, u))
+        return next_state
+
+    def _setup_step(self, mode='dense'):
+        """Set up dA, dB, dC discretized parameters for stepping."""
+        self.dA, self.dB = self._setup_state()
+
+        # Calculate original C
+        C = _conj(_r2c(self.C)) # (H C N)
+        if self.l_kernel.item() == 0:
+            dC = C
+        else:
+            # self.C represents C_tilde
+            dA_L = power(self.l_kernel.item(), self.dA)
+            I = torch.eye(self.dA.size(-1)).to(dA_L)
+
+            dC = torch.linalg.solve(
+                I - dA_L.transpose(-1, -2),
+                C.unsqueeze(-1),
+            ).squeeze(-1)
+        self.dC = dC
+
+        # Do special preprocessing for different step modes
+
+        self._step_mode = mode
+        if mode == 'linear':
+            # Linear case: special step function for the state, we need to handle output
+            # use conjugate symmetry by default, which affects the output projection
+            self.dC = 2*self.dC[:, :, :self.N]
+        elif mode == 'diagonal':
+            # Eigendecomposition of the A matrix
+            L, V = torch.linalg.eig(self.dA)
+            V_inv = torch.linalg.inv(V)
+            # Check that the eigendedecomposition is correct
+            if self.verbose:
+                print("Diagonalization error:", torch.dist(V @ torch.diag_embed(L) @ V_inv, self.dA))
+
+            # Change the parameterization to diagonalize
+            self.dA = L
+            self.dB = contract('h n m, h m -> h n', V_inv, self.dB)
+            self.dC = contract('h n m, c h n -> c h m', V, self.dC)
+
+        elif mode == 'dense':
+            pass
+        else: raise NotImplementedError("DPLR Kernel step mode must be {'dense' | 'linear' | 'diagonal'}")
+
+    def default_state(self, *batch_shape):
+        C = _r2c(self.C)
+        N = C.size(-1)
+        H = C.size(-2)
+
+        # Cache the tensor contractions we will later do, for efficiency
+        # These are put in this function because they depend on the batch size
+        step_mode = getattr(self, "_step_mode", "dense")  # Used in default_state, which is called without _setup_step() in forward_state()
+        if step_mode != 'linear':
+            N *= 2
+
+            if step_mode == 'diagonal':
+                self.state_contraction = "h n, ... h n -> ... h n"
+            else:
+                # Dense (quadratic) case: expand all terms
+                self.state_contraction = "h m n, ... h n -> ... h m"
+
+            self.input_contraction = "h n, ... h -> ... h n"
+
+        self.output_contraction = "c h n, ... h n -> ... c h"
+
+        state = torch.zeros(*batch_shape, H, N, dtype=C.dtype, device=C.device)
+        return state
+
+    def step(self, u, state):
+        """Must have called self._setup_step() and created state with self.default_state() before calling this."""
+
+        if self._step_mode == 'linear':
+            new_state = self._step_state_linear(u, state)
+        else:
+            new_state = self._step_state(u, state)
+        y = torch.einsum(self.output_contraction, self.dC, new_state)
+        return y.real, new_state
+
+    def forward_state(self, *args, **kwargs):
+        # Dispatch directly to generic state forwarding
+        # instead of using the Diag version
+
+        # TODO design pattern is ugly. Can be fixed with an intermediate
+        # subclass above Diag/DPLR that has the shared logic (parameter construction)
+        # but not the state/step logic.
+        # Fine to keep like this for now since we want Diag to be the standard
+        # instead of having too many layers of subclassing.
+
+        return SSMKernel.forward_state(self, *args, **kwargs)
+
+kernel_registry = {
+    's4d': SSMKernelDiag,
+    'diag': SSMKernelDiag,
+    's4': SSMKernelDPLR,
+    'nplr': SSMKernelDPLR,
+    'dplr': SSMKernelDPLR,
+}
+
+class FFTConv(nn.Module):
+    """Implements an FFT Convolution around a convolution kernel.
+
+    d_model (H): Model dimension (in CNN terminology, this would be "channels").
+    l_max (L): The maximum kernel length. Set l_max=None to always use a global kernel.
+    channels: Can be interpreted as a number of "heads"; the SSM is a map from a 1-dim to C-dim sequence. It's not recommended to change this; instead, increase d_model for larger models.
+    bidirectional: If True, convolution kernel will be two-sided.
+    activation: Activation after the full convolution.
+    transposed, dropout, tie_dropout: More general model options, see SequenceModule.
+    mode: Which kernel algorithm to use. 'nplr' is the full S4 model; 'diag' is the simpler S4D. Other options can be found in the kernel registry.
+
+    kernel_args: See the class .kernel.SSMKernel for the kernel constructor which accepts kernel_args. Relevant options that are worth considering and tuning include "mode", "init", "dt_min", "dt_max", "lr"
+    """
+
+    def __init__(
+        self,
+        d_model,
+        l_max=None,
+        channels=1,
+        swap_channels=False,
+        bidirectional=False,
+        activation='gelu', # Activation after layer
+        transposed=True,
+        dropout=0.0,
+        tie_dropout=False,
+        drop_kernel=0.0,
+        mode='dplr',
+        kernel=None,
+        **kernel_args,  # Arguments passed into inner convolution kernel
+    ):
+        super().__init__()
+        self.d_model = d_model
+        self.L = self.l_max = l_max
+        self.bidirectional = bidirectional
+        self.channels = channels
+        self.transposed = transposed
+        self.swap_channels = swap_channels
+
+
+        if activation is not None and activation.startswith('glu'):
+            channels *= 2
+        self.activation = Activation(activation, dim=1 if self.transposed else -1)
+
+        self.D = nn.Parameter(torch.randn(channels, self.d_model))
+
+        if self.bidirectional:
+            channels *= 2
+
+        # Inner convolution kernel
+        if mode is not None:
+            assert kernel is None, "Pass either mode or kernel but not both"
+            # log.info(
+            #     "Argument 'mode' is deprecated and renamed to 'kernel',"
+            #     "and will be removed in a future version."
+            # )
+            kernel, mode = mode, kernel
+        kernel_cls = kernel_registry[kernel]
+        self.kernel = kernel_cls(
+            d_model=self.d_model,
+            l_max=self.l_max,
+            channels=channels,
+            **kernel_args,
+        )
+
+        dropout_fn = DropoutNd if tie_dropout else nn.Dropout
+        self.drop = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()
+        self.drop_kernel = nn.Dropout(drop_kernel) if drop_kernel > 0.0 else nn.Identity()
+
+    def forward(self, x, state=None, rate=1.0, **kwargs): # absorbs return_output and transformer src mask
+        """
+        x: (B D L) if self.transposed else (B L D)
+        """
+
+        # Always work with (B D L) dimension in this module
+        if not self.transposed: x = x.transpose(-1, -2)
+        L = x.size(-1)
+
+        # Compute SS Kernel
+        l_kernel = L if self.L is None else min(L, round(self.L / rate))
+        k, k_state =  self.kernel(L=l_kernel, rate=rate, state=state) # (C H L) (B C H L)
+
+        # Convolution
+        if self.bidirectional:
+            k0, k1 = rearrange(k, '(s c) h l -> s c h l', s=2)
+            k = F.pad(k0, (0, L)) \
+                    + F.pad(k1.flip(-1), (L, 0))
+            # The above has an off-by-one in the reverse direction
+            # This is a deliberate choice since the off-by-one should not affect any applications
+            # This can be amended which may be very slightly slower
+            # k = F.pad(k0, (0, L)) \
+            #         + F.pad(k1[..., 1:].flip(-1), (L+1, 0)) \
+            #         + F.pad(k1[..., :1], (0, l_kernel+L-1))
+
+        # Kernel dropout
+        k = self.drop_kernel(k)
+
+        # In principle, we could pad to l_kernel+L-1 instead of l_kernel+L, but we choose the latter for
+        # equational simplicity. Additionally, we have not experimented to compare the efficiency of the two.
+        k_f = torch.fft.rfft(k, n=l_kernel+L) # (C H L)
+        x_f = torch.fft.rfft(x, n=l_kernel+L) # (B H L)
+        y_f = contract('bhl,chl->bchl', x_f, k_f)
+        y = torch.fft.irfft(y_f, n=l_kernel+L)[..., :L] # (B C H L)
+
+
+        # Compute D term in state space equation - essentially a skip connection
+        y = y + contract('bhl,ch->bchl', x, self.D)
+
+        # Compute state update
+        if state is not None:
+            assert not self.bidirectional, "Bidirectional not supported with state forwarding"
+            y = y + k_state #
+            next_state = self.kernel.forward_state(x, state)
+        else:
+            next_state = None
+
+
+        # Reshape to flatten channels
+        if self.swap_channels:
+            y = rearrange(y, 'b c h l -> b (h c) l')
+        else:
+            y = rearrange(y, 'b c h l -> b (c h) l')
+
+        y = self.drop(y)  # DropoutNd better with transposed=True
+
+        if not self.transposed: y = y.transpose(-1, -2)
+        y = self.activation(y)
+
+        return y, next_state
+
+
+    def setup_step(self, **kwargs):
+        self.kernel._setup_step(**kwargs)
+
+    def step(self, x, state):
+        """ Step one time step as a recurrent model. Intended to be used during validation.
+
+        x: (B H)
+        state: (B H N)
+        Returns: output (B H), state (B H N)
+        """
+
+        y, next_state = self.kernel.step(x, state) # (B C H)
+        y = y + x.unsqueeze(-2) * self.D
+        y = rearrange(y, 'b c h -> b (c h)')
+        y = self.activation(y)
+        return y, next_state
+
+    def default_state(self, *batch_shape, device=None):
+        # kernel is not a SequenceModule so it doesn't need to adhere to same interface
+        # the kernel will know the device of its own parameters
+        return self.kernel.default_state(*batch_shape)
+
+    @property
+    def d_output(self):
+        return self.d_model * self.channels
+
+
+class S4Block(nn.Module):
+    """General block design wrapping an inner layer. Currently only layer=FFTConv is supported, but easy to incorporate others.
+
+    Arguments:
+    - bottleneck: Reduce dimension of inner layer (e.g. used in GSS).
+    - gate: Add multiplicative gating (e.g. used in GSS), which is essentially a multiplicative instead of additive residual branch.
+    - gate_act: Activation function to apply on the gate residual branch.
+    - mult_act: Activation function to apply after gate multiplication (e.g. GELU in GSS).
+    - final_act: Activation function to apply after final linear layer. 'id' for no activation, None for no linear layer at all.
+
+    - initializer: Initializer on final linear layer.
+    - weight_norm: Weight normalization on final linear layer.
+    - dropout: standard dropout argument. tie_dropout=True ties the dropout mask across the sequence length, emulating nn.Dropout1d
+
+    - transposed: Choose backbone axis ordering of (B, L, H) (if False) or (B, H, L) (if True) [B=batch size, L=sequence length, H=model dimension]
+
+    Other options are all experimental and should not need to be configured.
+    """
+
+    def __init__(
+        self,
+        d_model,
+        bottleneck=None,
+        gate=None,
+        gate_act=None,
+        mult_act=None,
+        final_act='glu',
+        postact=None,
+        initializer=None,
+        weight_norm=False,
+        dropout=0.0,
+        tie_dropout=False,
+        transposed=True,
+        **layer_args,  # Arguments into inner layer (e.g. FFTConv)
+    ):
+        super().__init__()
+
+        self.d_model = d_model
+        self.transposed = transposed
+
+        self.gate = gate
+        self.bottleneck = bottleneck
+
+        if bottleneck is not None:
+            self.d_model = self.d_model // bottleneck
+            self.input_linear = LinearActivation(
+                self.d_model,
+                self.d_model,
+                transposed=False,
+                activation=None,
+                activate=False,
+            )
+
+        if gate is not None:
+            self.input_gate = LinearActivation(
+                self.d_model,
+                self.d_model * gate,
+                transposed=False,
+                activation=gate_act,
+                activate=True,
+            )
+            if self.layer.d_output != self.d_model * gate:
+                self.output_gate = LinearActivation(
+                    self.d_model*self.channels,
+                    self.d_model * gate,
+                    transposed=False,
+                    activation=None,
+                    activate=False,
+                )
+
+        # Currently this module only uses FFTConv for its inner module
+        # But the options here are all agnostic to the inner block
+        # If other types of inner layers are desired, it is easy
+        # to add an option to swap a different module in
+        self.layer = FFTConv(d_model, transposed=False, dropout=dropout, tie_dropout=tie_dropout, **layer_args)
+
+        # Pointwise operations
+
+        # Activation after (optional) multiplication by gate branch
+        self.mult_activation = Activation(mult_act)
+        # dropout_fn = nn.Dropout2d if self.transposed else nn.Dropout # Broken in torch==1.11
+        dropout_fn = partial(DropoutNd, transposed=False) if tie_dropout else nn.Dropout
+        self.drop = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()
+
+        # position-wise output transform to mix features
+        if postact is not None:
+            assert final_act is None
+            log.warning("Warning: 'postact' option changed to 'final_act' and will be removed in a future version.")
+            final_act, postact = postact, final_act
+        if final_act is None:
+            self.output_linear = nn.Identity()
+        else:
+            self.output_linear = LinearActivation(
+                self.d_model*gate if gate is not None else self.layer.d_output,
+                self.d_model,
+                transposed=False,
+                activation=final_act,
+                activate=True,
+            )
+
+
+
+    def forward(self, x, lengths=None, **kwargs): # absorbs return_output and transformer src mask
+        """
+        x: (B H L) if self.transposed else (B L H)
+        state: (H N) never needed unless you know what you're doing
+
+        Returns: same shape as x
+        """
+        if self.transposed: x = rearrange(x, 'b d ... -> b ... d')
+        L = x.size(1)
+
+        # Mask out padding tokens
+        # TODO handle option for mask - instead of lengths, which assumes suffix padding
+        if isinstance(lengths, int):
+            if lengths != L:
+                lengths = torch.tensor(lengths, dtype=torch.long, device=x.device)
+            else:
+                lengths = None
+        if lengths is not None:
+            assert isinstance(lengths, torch.Tensor) and lengths.ndim == 1 and lengths.size(0) in [1, x.size(0)]
+            mask = torch.where(torch.arange(L, device=lengths.device)[:, None] < lengths[:, None, None], 1., 0.)
+            x = x * mask
+
+        if self.gate is not None:
+            v = self.input_gate(x)
+        if self.bottleneck is not None:
+            x = self.input_linear(x)
+
+        y, state = self.layer(x, **kwargs)
+
+
+        if self.gate is not None:
+            y = self.output_gate(y)
+            y = y * v
+        y = self.mult_activation(y)
+        y = self.drop(y)
+        y = self.output_linear(y)
+
+        if self.transposed: y = rearrange(y, 'b d ... -> b ... d')
+
+        return y, state
+
+    def setup_step(self, **kwargs):
+        self.layer.setup_step(**kwargs)
+
+    def step(self, x, state):
+        """Step one time step as a recurrent model. Intended to be used during validation.
+
+        x: (B H)
+        state: (B H N)
+        Returns: output (B H), state (B H N)
+        """
+
+        if self.gate is not None:
+            v = self.input_gate(x)
+        if self.bottleneck is not None:
+            x = self.input_linear(x)
+        y, next_state = self.layer.step(x, state) # (B C H)
+        if self.gate is not None:
+            y = self.output_gate(y)
+            y = y * v
+        y = self.mult_activation(y)
+        y = self.drop(y)
+        y = self.output_linear(y)
+        return y, next_state
+
+    def default_state(self, *batch_shape, device=None):
+        # kernel is not a SequenceModule so it doesn't need to adhere to same interface
+        # the kernel will know the device of its own parameters
+        return self.layer.default_state(*batch_shape)
+
+    @property
+    def d_output(self):
+        return self.d_model
\ No newline at end of file
diff --git a/print_models.py b/print_models.py
new file mode 100644
index 0000000..5e1028a
--- /dev/null
+++ b/print_models.py
@@ -0,0 +1,195 @@
+from models.binaurals4 import BinauralS4
+from torchinfo import summary
+import torch
+import torchaudio
+import random
+import numpy as np
+from torch.utils.data.distributed import DistributedSampler
+import os
+from tqdm import tqdm
+
+class BinauralConditionalDataset(torch.utils.data.Dataset):
+  def __init__(self, paths, binaural_type="", predict_mean_condition=False, use_mean_cond=False):
+    super().__init__()
+    self.mono, self.binaural, self.binaural_geowarp, self.view = [], [], [], []
+    self.binaural_type = binaural_type
+    self.predict_mean_condition = predict_mean_condition
+    self.use_mean_cond = use_mean_cond
+    if self.use_mean_cond:
+       self.mean_cond = []
+    
+    for subject_id in range(8):
+      mono, _ = torchaudio.load(f"{paths}/subject{subject_id + 1}/mono.wav")
+      binaural, _ = torchaudio.load(f"{paths}/subject{subject_id + 1}/binaural.wav")
+      binaural_geowarp, _ = torchaudio.load(f"{paths}/subject{subject_id + 1}/binaural_geowarp.wav")
+      # receiver is fixed at origin in this dataset, so we only need transmitter view
+      tx_view = np.loadtxt(f"{paths}/subject{subject_id + 1}/tx_positions.txt").transpose()
+      self.mono.append(mono)
+      self.binaural.append(binaural)
+      self.binaural_geowarp.append(binaural_geowarp)
+      self.view.append(tx_view.astype(np.float32))
+      if self.use_mean_cond:
+        print("loading meancond...")
+        mean_cond, _ = torchaudio.load(f"{paths}/subject{subject_id + 1}/subject{subject_id + 1}.wav")
+        self.mean_cond.append(mean_cond)
+
+    # ensure that chunk_size is a multiple of 400 to match audio (48kHz) and receiver/transmitter positions (120Hz)
+    self.chunk_size = 2000 * 48
+    if self.chunk_size % 400 > 0:
+      self.chunk_size = self.chunk_size + 400 - self.chunk_size % 400
+    # compute chunks
+    self.chunks = []
+    for subject_id in range(8):
+      last_chunk_start_frame = self.mono[subject_id].shape[-1] - self.chunk_size + 1
+      hop_length = int((1 - 0.5) * self.chunk_size)
+      for offset in range(0, last_chunk_start_frame, hop_length):
+        self.chunks.append({'subject': subject_id, 'offset': offset})
+
+  def __len__(self):
+    return len(self.chunks)
+
+  def __getitem__(self, idx):
+    subject = self.chunks[idx]['subject']
+    offset = self.chunks[idx]['offset']
+    mono = self.mono[subject][:, offset:offset+self.chunk_size]
+    view = self.view[subject][:, offset//400:(offset+self.chunk_size)//400]
+
+    binaural = self.binaural[subject][0:2, offset:offset+self.chunk_size]
+    binaural_geowarp = self.binaural_geowarp[subject][0:2, offset:offset+self.chunk_size]
+    if self.use_mean_cond:
+      mean_condition = self.mean_cond[subject][:, offset:offset+self.chunk_size]
+    else:
+      mean_condition = self.binaural[subject][0:2, offset:offset+self.chunk_size].mean(0, keepdim=True)
+
+    return {
+        'mono': mono,
+        'binaural': binaural,
+        'binaural_geowarp': binaural_geowarp,
+        'view': view,
+        'mean_condition': mean_condition,
+    }
+
+
+
+
+class Collator:
+  def __init__(self, params):
+    self.params = params
+
+  def collate_binaural(self, minibatch):
+
+    clip_length = self.params.clip_length
+    for record in minibatch:
+
+      start_view = random.randint(0, record['mono'].shape[1] // 400 - clip_length // 400)
+      start = start_view * 400
+      end_view = start_view + clip_length // 400
+      end = end_view * 400
+      record['mono'] = record['mono'][:, start:end]
+      record['mean_condition'] = record['mean_condition'][:, start:end]
+      record['binaural'] = record['binaural'][:, start:end]
+      record['binaural_geowarp'] = record['binaural_geowarp'][:, start:end]
+      record['view'] = record['view'][:, start_view:end_view].T
+      record['view'] = np.repeat(record['view'], 400, axis=0).T
+
+    mono = np.stack([record['mono'] for record in minibatch if 'mono' in record])
+    mean_condition = np.stack([record['mean_condition'] for record in minibatch if 'mean_condition' in record])
+    binaural = np.stack([record['binaural'] for record in minibatch if 'binaural' in record])
+    binaural_geowarp = np.stack([record['binaural_geowarp'] for record in minibatch if 'binaural_geowarp' in record])
+    view = np.stack([record['view'] for record in minibatch if 'view' in record])
+
+    assert binaural_geowarp.shape[0] == view.shape[0]
+
+    return {
+        'mono': torch.from_numpy(mono),
+        'mean_condition': torch.from_numpy(mean_condition),
+        'audio': torch.from_numpy(binaural),
+        'binaural_geowarp': torch.from_numpy(binaural_geowarp),
+        'view': torch.from_numpy(view),
+    }
+
+class AttrDict(dict):
+  def __init__(self, *args, **kwargs):
+      super(AttrDict, self).__init__(*args, **kwargs)
+      self.__dict__ = self
+
+  def override(self, attrs):
+    if isinstance(attrs, dict):
+      self.__dict__.update(**attrs)
+    elif isinstance(attrs, (list, tuple, set)):
+      for attr in attrs:
+        self.override(attr)
+    elif attrs is not None:
+      raise NotImplementedError
+    return self
+
+def from_path(data_dirs, params, binaural_type="", is_distributed=False, use_mean_cond=False):
+  if binaural_type:
+    dataset = BinauralConditionalDataset(data_dirs[0], binaural_type,
+      predict_mean_condition=getattr(params, "predict_mean_condition", False), use_mean_cond=use_mean_cond)
+  else:
+    raise ValueError("Unsupported binaural_type")
+  return torch.utils.data.DataLoader(
+      dataset,
+      batch_size=params.batch_size,
+      collate_fn=Collator(params).collate_binaural,
+      shuffle=not is_distributed,
+      num_workers=os.cpu_count(),
+      sampler=DistributedSampler(dataset) if is_distributed else None,
+      pin_memory=True,
+      drop_last=True)
+
+
+def main():
+    params = AttrDict(
+    # Training params
+    batch_size=4,
+    learning_rate=2e-4,
+    max_grad_norm=None,
+    use_mono=False,
+    clip_length= 32000,
+    lambda_phase=0.0,
+
+    # Data params
+    sample_rate=48000,
+    n_mels=80,
+    n_fft=1024,
+    hop_samples=256,
+    crop_mel_frames=62,  # Probably an error in paper.
+
+    # Model params
+    residual_layers=30,
+    residual_channels=64,
+    dilation_cycle_length=10,
+    unconditional = False,
+    noise_schedule=np.linspace(1e-4, 0.05, 50).tolist(),
+    inference_noise_schedule=[0.0001, 0.001, 0.01, 0.05, 0.2, 0.5],
+
+    # unconditional sample len
+    audio_len = 48000*5, # unconditional_synthesis_samples
+    )
+
+    num_gpus = 1
+    trainloader = from_path(data_dirs=["dataset/trainset"], params=params, binaural_type=True, is_distributed=True if num_gpus > 1 else False, use_mean_cond=True)
+    model_binaurals4 = BinauralS4(n_layers=16, d_model=64, use_mean_condition=True).cuda()
+    bathsize = 4
+    L = 32000
+    n_iter = 100
+    for data in tqdm(trainloader, desc=f'Epoch {n_iter // len(trainloader)}'):
+        if True:
+            geowarp, mean_condition, mono, view, target = data["binaural_geowarp"], data["mean_condition"], data["mono"], data["view"], data["audio"]                
+            print(geowarp.shape)
+            print(mean_condition.shape)
+            print(mono.shape)
+            print(view.shape)
+            geowarp = (4, 2, 32000)
+            mean_condition = (4, 1, 32000)
+            mono = (4, 1, 32000)
+            view = (4, 7, 32000)
+
+
+            summary(model_binaurals4, [geowarp, view, mono, mean_condition])
+        break
+
+if __name__ == "__main__":
+  main()
\ No newline at end of file
diff --git a/train.py b/train.py
new file mode 100644
index 0000000..8a0ccd0
--- /dev/null
+++ b/train.py
@@ -0,0 +1,523 @@
+import os
+import time
+import random
+# import warnings
+# warnings.filterwarnings("ignore")
+from functools import partial
+import multiprocessing as mp
+#from BinauralS4.losses import STFTLoss
+#from BinauralS4.dataset.dataset import AttrDict, from_path
+#from dataset import AttrDict, from_path
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import numpy as np
+import os
+import random
+import torch
+import torchaudio
+
+from glob import glob
+from torch.utils.data.distributed import DistributedSampler
+import torch.nn.functional as F
+seed = 123
+random.seed(seed)
+torch.manual_seed(seed)
+class STFTLoss(torch.nn.Module):
+    def __init__(self, n_fft, hop_length, win_length, window):
+        super(STFTLoss, self).__init__()
+        self.n_fft = n_fft
+        self.hop_length = hop_length
+        self.win_length = win_length
+        self.window = window
+
+    def forward(self, generated_audio, target_audio):
+        batch_size, channels, _ = generated_audio.size()
+        loss = 0.0
+
+        for i in range(batch_size):
+            for j in range(channels):
+                G = torch.stft(
+                    generated_audio[i,j],
+                    n_fft=self.n_fft,
+                    hop_length=self.hop_length,
+                    win_length=self.win_length,
+                    window=self.window,
+                    return_complex=True
+                )
+
+                T = torch.stft(
+                    target_audio[i,j],
+                    n_fft=self.n_fft,
+                    hop_length=self.hop_length,
+                    win_length=self.win_length,
+                    window=self.window,
+                    return_complex=True
+                )
+
+                loss += F.mse_loss(G.abs(), T.abs())
+
+        return loss / (batch_size * channels)
+
+class BinauralConditionalDataset(torch.utils.data.Dataset):
+  def __init__(self, paths, binaural_type="", predict_mean_condition=False, use_mean_cond=False):
+    super().__init__()
+    self.mono, self.binaural, self.binaural_geowarp, self.view = [], [], [], []
+    self.binaural_type = binaural_type
+    self.predict_mean_condition = predict_mean_condition
+    self.use_mean_cond = use_mean_cond
+    if self.use_mean_cond:
+       self.mean_cond = []
+    
+    for subject_id in range(8):
+      mono, _ = torchaudio.load(f"{paths}/subject{subject_id + 1}/mono.wav")
+      binaural, _ = torchaudio.load(f"{paths}/subject{subject_id + 1}/binaural.wav")
+      binaural_geowarp, _ = torchaudio.load(f"{paths}/subject{subject_id + 1}/binaural_geowarp.wav")
+      # receiver is fixed at origin in this dataset, so we only need transmitter view
+      tx_view = np.loadtxt(f"{paths}/subject{subject_id + 1}/tx_positions.txt").transpose()
+      self.mono.append(mono)
+      self.binaural.append(binaural)
+      self.binaural_geowarp.append(binaural_geowarp)
+      self.view.append(tx_view.astype(np.float32))
+      if self.use_mean_cond:
+        print("loading meancond...")
+        mean_cond, _ = torchaudio.load(f"{paths}/subject{subject_id + 1}/subject{subject_id + 1}.wav")
+        self.mean_cond.append(mean_cond)
+
+    # ensure that chunk_size is a multiple of 400 to match audio (48kHz) and receiver/transmitter positions (120Hz)
+    self.chunk_size = 2000 * 48
+    if self.chunk_size % 400 > 0:
+      self.chunk_size = self.chunk_size + 400 - self.chunk_size % 400
+    # compute chunks
+    self.chunks = []
+    for subject_id in range(8):
+      last_chunk_start_frame = self.mono[subject_id].shape[-1] - self.chunk_size + 1
+      hop_length = int((1 - 0.5) * self.chunk_size)
+      for offset in range(0, last_chunk_start_frame, hop_length):
+        self.chunks.append({'subject': subject_id, 'offset': offset})
+
+  def __len__(self):
+    return len(self.chunks)
+
+  def __getitem__(self, idx):
+    subject = self.chunks[idx]['subject']
+    offset = self.chunks[idx]['offset']
+    mono = self.mono[subject][:, offset:offset+self.chunk_size]
+    view = self.view[subject][:, offset//400:(offset+self.chunk_size)//400]
+
+    binaural = self.binaural[subject][0:2, offset:offset+self.chunk_size]
+    binaural_geowarp = self.binaural_geowarp[subject][0:2, offset:offset+self.chunk_size]
+    if self.use_mean_cond:
+      mean_condition = self.mean_cond[subject][:, offset:offset+self.chunk_size]
+    else:
+      mean_condition = self.binaural[subject][0:2, offset:offset+self.chunk_size].mean(0, keepdim=True)
+
+    return {
+        'mono': mono,
+        'binaural': binaural,
+        'binaural_geowarp': binaural_geowarp,
+        'view': view,
+        'mean_condition': mean_condition,
+    }
+
+
+
+
+class Collator:
+  def __init__(self, params):
+    self.params = params
+
+  def collate_binaural(self, minibatch):
+
+    clip_length = self.params.clip_length
+    for record in minibatch:
+
+      start_view = random.randint(0, record['mono'].shape[1] // 400 - clip_length // 400)
+      start = start_view * 400
+      end_view = start_view + clip_length // 400
+      end = end_view * 400
+      record['mono'] = record['mono'][:, start:end]
+      record['mean_condition'] = record['mean_condition'][:, start:end]
+      record['binaural'] = record['binaural'][:, start:end]
+      record['binaural_geowarp'] = record['binaural_geowarp'][:, start:end]
+      record['view'] = record['view'][:, start_view:end_view].T
+      record['view'] = np.repeat(record['view'], 400, axis=0).T
+
+    mono = np.stack([record['mono'] for record in minibatch if 'mono' in record])
+    mean_condition = np.stack([record['mean_condition'] for record in minibatch if 'mean_condition' in record])
+    binaural = np.stack([record['binaural'] for record in minibatch if 'binaural' in record])
+    binaural_geowarp = np.stack([record['binaural_geowarp'] for record in minibatch if 'binaural_geowarp' in record])
+    view = np.stack([record['view'] for record in minibatch if 'view' in record])
+
+    assert binaural_geowarp.shape[0] == view.shape[0]
+
+    return {
+        'mono': torch.from_numpy(mono),
+        'mean_condition': torch.from_numpy(mean_condition),
+        'audio': torch.from_numpy(binaural),
+        'binaural_geowarp': torch.from_numpy(binaural_geowarp),
+        'view': torch.from_numpy(view),
+    }
+
+class AttrDict(dict):
+  def __init__(self, *args, **kwargs):
+      super(AttrDict, self).__init__(*args, **kwargs)
+      self.__dict__ = self
+
+  def override(self, attrs):
+    if isinstance(attrs, dict):
+      self.__dict__.update(**attrs)
+    elif isinstance(attrs, (list, tuple, set)):
+      for attr in attrs:
+        self.override(attr)
+    elif attrs is not None:
+      raise NotImplementedError
+    return self
+
+def from_path(data_dirs, params, binaural_type="", is_distributed=False, use_mean_cond=False):
+  if binaural_type:
+    dataset = BinauralConditionalDataset(data_dirs[0], binaural_type,
+      predict_mean_condition=getattr(params, "predict_mean_condition", False), use_mean_cond=use_mean_cond)
+  else:
+    raise ValueError("Unsupported binaural_type")
+  dataset_size = len(dataset)
+  half_size = dataset_size // 2
+  train_dataset, _ = torch.utils.data.random_split(dataset, [half_size, dataset_size - half_size])
+
+  return torch.utils.data.DataLoader(
+      train_dataset,
+      batch_size=params.batch_size,
+      collate_fn=Collator(params).collate_binaural,
+      shuffle=not is_distributed,
+      num_workers=os.cpu_count(),
+      sampler=DistributedSampler(train_dataset) if is_distributed else None,
+      pin_memory=True,
+      drop_last=True)
+
+
+
+params = AttrDict(
+    # Training params
+    batch_size=4,
+    learning_rate=2e-4,
+    max_grad_norm=None,
+    use_mono=False,
+    clip_length= 16000,
+    lambda_phase=0.0,
+
+    # Data params
+    sample_rate=48000,
+    n_mels=80,
+    n_fft=1024,
+    hop_samples=256,
+    crop_mel_frames=62,  # Probably an error in paper.
+
+    # Model params
+    residual_layers=30,
+    residual_channels=64,
+    dilation_cycle_length=10,
+    unconditional = False,
+    noise_schedule=np.linspace(1e-4, 0.05, 50).tolist(),
+    inference_noise_schedule=[0.0001, 0.001, 0.01, 0.05, 0.2, 0.5],
+
+    # unconditional sample len
+    audio_len = 48000*5, # unconditional_synthesis_samples
+)
+
+
+
+
+
+import numpy as np
+import torch
+import torch.nn as nn
+# from torch.utils.tensorboard import SummaryWriter
+import hydra
+import wandb
+from omegaconf import DictConfig, OmegaConf
+from tqdm import tqdm
+
+# from dataset_sc import load_Speech_commands
+# from dataset_ljspeech import load_LJSpeech
+from utils import find_max_epoch, print_size, calc_diffusion_hyperparams, local_directory
+
+from distributed_util import init_distributed, apply_gradient_allreduce, reduce_tensor
+# from generate import generate
+
+from models import construct_model
+
+def distributed_train(rank, num_gpus, group_name, cfg):
+    # Initialize logger
+    if rank == 0 and cfg.wandb is not None:
+        wandb_cfg = cfg.pop("wandb")
+        wandb.init(
+            **wandb_cfg, config=OmegaConf.to_container(cfg, resolve=True)
+        )
+
+    # Distributed running initialization
+    dist_cfg = cfg.pop("distributed")
+    if num_gpus > 1:
+        init_distributed(rank, num_gpus, group_name, **dist_cfg)
+
+    train(
+        rank=rank, num_gpus=num_gpus,
+        model_cfg=cfg.model,
+        dataset_cfg=cfg.dataset,
+        generate_cfg=cfg.generate,
+        **cfg.train,
+    )
+
+def train(
+    rank, num_gpus,
+    model_cfg, dataset_cfg, generate_cfg, # dist_cfg, wandb_cfg, # train_cfg,
+    ckpt_iter, n_iters, iters_per_ckpt, iters_per_logging,
+    learning_rate, batch_size_per_gpu, clip_length, loss_function,
+    # n_samples,
+    name=None,
+    
+    # mel_path=None,
+):
+    """
+    Parameters:
+    ckpt_iter (int or 'max'):       the pretrained checkpoint to be loaded;
+                                    automitically selects the maximum iteration if 'max' is selected
+    n_iters (int):                  number of iterations to train, default is 1M
+    iters_per_ckpt (int):           number of iterations to save checkpoint,
+                                    default is 10k, for models with residual_channel=64 this number can be larger
+    iters_per_logging (int):        number of iterations to save training log and compute validation loss, default is 100
+    learning_rate (float):          learning rate
+    batch_size_per_gpu (int):       batchsize per gpu, default is 2 so total batchsize is 16 with 8 gpus
+    n_samples (int):                audio samples to generate and log per checkpoint
+    name (str):                     prefix in front of experiment name
+    mel_path (str):                 for vocoding, path to mel spectrograms (TODO generate these on the fly)
+    """
+
+    local_path, checkpoint_directory = local_directory(name, model_cfg, dataset_cfg, 'checkpoint')
+    params = AttrDict(
+    # Training params
+    batch_size=batch_size_per_gpu,
+    learning_rate=learning_rate,
+    max_grad_norm=None,
+    use_mono=False,
+    clip_length= clip_length,
+    lambda_phase=0.0,
+
+    # Data params
+    sample_rate=48000,
+    n_mels=80,
+    n_fft=1024,
+    hop_samples=256,
+    crop_mel_frames=62,  # Probably an error in paper.
+
+    # Model params
+    residual_layers=30,
+    residual_channels=64,
+    dilation_cycle_length=10,
+    unconditional = False,
+    noise_schedule=np.linspace(1e-4, 0.05, 50).tolist(),
+    inference_noise_schedule=[0.0001, 0.001, 0.01, 0.05, 0.2, 0.5],
+
+    # unconditional sample len
+    audio_len = 48000*5, # unconditional_synthesis_samples
+    )
+
+
+    # load training data
+    trainloader = from_path(data_dirs=["dataset/trainset"], params=params, binaural_type=True, is_distributed=True if num_gpus > 1 else False, use_mean_cond=model_cfg["use_mean_condition"])
+    # testloader = from_path(data_dirs=["dataset/testset"], params=params, is_distributed=True if num_gpus > 1 else False)
+    print('Data loaded')
+
+    # predefine model
+    net = construct_model(model_cfg).cuda()
+    print_size(net, verbose=False)
+
+    # apply gradient all reduce
+    if num_gpus > 1:
+        net = apply_gradient_allreduce(net)
+
+    # define optimizer
+    if loss_function == "mse":
+        criterion =  torch.nn.MSELoss() #stft_loss # torch.nn.MSELoss()
+    elif loss_function == "stft":
+        n_fft = 512
+        hop_length = 256
+        win_length = n_fft
+        window = torch.hann_window(win_length).cuda()
+        stft_loss = STFTLoss(n_fft, hop_length, win_length, window)
+        criterion = stft_loss
+    else:
+       criterion =  torch.nn.MSELoss() #stft_loss # torch.nn.MSELoss()
+    
+
+
+    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)
+    scaler = torch.cuda.amp.GradScaler()
+
+    if ckpt_iter == 'max':
+       ckpt_iter = find_max_epoch(checkpoint_directory)
+    if ckpt_iter >= 0:
+        try:
+          # load checkpoint file
+            model_path = os.path.join(checkpoint_directory, '{}.pkl'.format(ckpt_iter))
+            checkpoint = torch.load(model_path, map_location='cpu')
+
+            # fead model dict and optimizer state
+            net.load_state_dict(checkpoint['model_state_dict'])
+            if 'optimizer_state_dict' in checkpoint:
+                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
+                optimizer.param_group[0]['lr'] = learning_rate
+
+            print('Successfully loaded model at iteration {}'.format(ckpt_iter))
+    
+        except:
+            print(f"Model checkpoint found at iteration {ckpt_iter}, but was not successfully loaded - training from scratch.")
+            ckpt_iter = -1
+       
+    else:
+        print('No valid checkpoint model found - training from scratch.')
+        ckpt_iter = -1
+
+    ckpt_iter = -1
+    # training
+    n_iter = ckpt_iter + 1
+    while n_iter < n_iters + 1:
+        epoch_loss = 0.
+        for data in tqdm(trainloader, desc=f'Epoch {n_iter // len(trainloader)}'):
+            if model_cfg["use_mean_condition"]:
+                geowarp, mean_condition, mono, view, target = data["binaural_geowarp"], data["mean_condition"], data["mono"], data["view"], data["audio"]
+                geowarp = geowarp.cuda()
+                mean_condition = mean_condition.cuda()
+                mono = mono.cuda()
+                view = view.cuda()
+                target = target.cuda()
+            else:
+                geowarp, mono, view, target = data["binaural_geowarp"], data["mono"], data["view"], data["audio"]
+                geowarp = geowarp.cuda()
+                mean_condition = None
+                mono = mono.cuda()
+                view = view.cuda()
+                target = target.cuda()
+
+            # back-propagation
+            optimizer.zero_grad()
+            with torch.cuda.amp.autocast():
+                loss = training_loss(net, criterion, geowarp,  mean_condition, mono, view, target)
+            if num_gpus > 1:
+                reduced_loss = reduce_tensor(loss.data, num_gpus).item()
+            else:
+                reduced_loss = loss.item()
+            scaler.scale(loss).backward()
+            scaler.step(optimizer)
+            # loss.backward()
+            # optimizer.step()
+            scaler.update()
+
+            epoch_loss += reduced_loss
+
+            # output to log
+            if n_iter % iters_per_logging == 0 and rank == 0:
+                # save training loss to tensorboard
+                # print("iteration: {} \treduced loss: {} \tloss: {}".format(n_iter, reduced_loss, loss.item()))
+                # tb.add_scalar("Log-Train-Loss", torch.log(loss).item(), n_iter)
+                # tb.add_scalar("Log-Train-Reduced-Loss", np.log(reduced_loss), n_iter)
+                wandb.log({
+                    'train/loss': reduced_loss,
+                    'train/log_loss': np.log(reduced_loss),
+                }, step=n_iter)
+
+            # save checkpoint
+            if n_iter % iters_per_ckpt == 0 and rank == 0:
+                checkpoint_name = '{}.pkl'.format(n_iter)
+                torch.save({'model_state_dict': net.state_dict(),
+                            'optimizer_state_dict': optimizer.state_dict()},
+                           os.path.join(checkpoint_directory, checkpoint_name))
+                print('model at iteration %s is saved' % n_iter)
+
+                # Generate samples
+                # if model_cfg["unconditional"]:
+                #     mel_path = None
+                #     mel_name = None
+                # else:
+                #     assert mel_path is not None
+                #     mel_name=generate_cfg.mel_name # "LJ001-0001"
+                # if not model_cfg["unconditional"]: assert generate_cfg.mel_name is not None
+                # generate_cfg["ckpt_iter"] = n_iter
+                
+                # samples = generate(
+                #     rank, # n_iter,
+                #     model_cfg, dataset_cfg,
+                #     name=name,
+                #     **generate_cfg,
+                #     # n_samples, n_iter, name,
+                #     # mel_path=mel_path,
+                #     # mel_name=mel_name,
+                # )
+                # samples = [wandb.Audio(sample.squeeze().cpu(), sample_rate=dataset_cfg['sampling_rate']) for sample in samples]
+                # wandb.log(
+                #     {'inference/audio': samples},
+                #     step=n_iter,
+                #     # commit=False,
+                # )
+
+            n_iter += 1
+        if rank == 0:
+            epoch_loss /= len(trainloader)
+            wandb.log({'train/loss_epoch': epoch_loss, 'train/log_loss_epoch': np.log(epoch_loss)}, step=n_iter)
+
+    # Close logger
+    if rank == 0:
+        # tb.close()
+        wandb.finish()
+
+def training_loss(net, loss_fn, audio,  mean_condition, mono, view, target):
+    """
+    Compute the training loss of epsilon and epsilon_theta
+
+    Parameters:
+    net (torch network):            the wavenet model
+    loss_fn (torch loss function):  the loss function, default is nn.MSELoss()
+    X (torch.tensor):               training data, shape=(batchsize, 1, length of audio)
+    diffusion_hyperparams (dict):   dictionary of diffusion hyperparameters returned by calc_diffusion_hyperparams
+                                    note, the tensors need to be cuda tensors
+
+    Returns:
+    training loss
+    """
+
+
+    # audio = X
+    
+    
+    output, _ = net(audio, view, mono, mean_condition)  # predict \epsilon according to \epsilon_\theta
+    return loss_fn(output, target)
+
+
+
+@hydra.main(version_base=None, config_path="configs/", config_name="config")
+def main(cfg: DictConfig) -> None:
+    print(OmegaConf.to_yaml(cfg))
+    OmegaConf.set_struct(cfg, False)  # Allow writing keys
+
+    os.makedirs("exp/", mode=0o775, exist_ok=True)
+
+    num_gpus = torch.cuda.device_count()
+    train_fn = partial(
+        distributed_train,
+        num_gpus=num_gpus,
+        group_name=time.strftime("%Y%m%d-%H%M%S"),
+        cfg=cfg,
+    )
+
+    if num_gpus <= 1:
+        train_fn(0)
+    else:
+        mp.set_start_method("spawn")
+        processes = []
+        for i in range(num_gpus):
+            p = mp.Process(target=train_fn, args=(i,))
+            p.start()
+            processes.append(p)
+        for p in processes:
+            p.join()
+
+if __name__ == "__main__":
+    main()
\ No newline at end of file
diff --git a/utils.py b/utils.py
new file mode 100644
index 0000000..b8c5baa
--- /dev/null
+++ b/utils.py
@@ -0,0 +1,176 @@
+import os
+import numpy as np
+import torch
+
+from models import model_identifier
+
+def flatten(v):
+    """
+    Flatten a list of lists/tuples
+    """
+
+    return [x for y in v for x in y]
+
+
+def rescale(x):
+    """
+    Rescale a tensor to 0-1
+    """
+
+    return (x - x.min()) / (x.max() - x.min())
+
+
+def find_max_epoch(path):
+    """
+    Find maximum epoch/iteration in path, formatted ${n_iter}.pkl
+    E.g. 100000.pkl
+
+    Parameters:
+    path (str): checkpoint path
+
+    Returns:
+    maximum iteration, -1 if there is no (valid) checkpoint
+    """
+
+    files = os.listdir(path)
+    epoch = -1
+    for f in files:
+        if len(f) <= 4:
+            continue
+        if f[-4:]  == '.pkl':
+            try:
+                epoch = max(epoch, int(f[:-4]))
+            except:
+                continue
+    return epoch
+
+def smooth_ckpt(path, min_ckpt, max_ckpt, alpha=None):
+    print(f"finding checkpoints in ({min_ckpt}, {max_ckpt}] in {path}")
+    files = os.listdir(path)
+    ckpts = []
+    for f in files:
+        if len(f) <= 4:
+            continue
+        if f[-4:]  == '.pkl':
+            print(f)
+            try:
+                it = int(f[:-4])
+                if min_ckpt < it and it <= max_ckpt:
+                    ckpts.append(it)
+            except:
+                continue
+    ckpts = sorted(ckpts)
+    print("found ckpts", ckpts)
+    state_dict = None
+    for n, it in enumerate(ckpts):
+        model_path = os.path.join(path, '{}.pkl'.format(it))
+        try:
+            checkpoint = torch.load(model_path, map_location='cpu')
+            # net.load_state_dict(checkpoint['model_state_dict'])
+            state_dict = smooth_dict(state_dict, checkpoint['model_state_dict'], n, alpha=alpha)
+            print('Successfully loaded model at iteration {}'.format(it))
+        except:
+            raise Exception(f'No valid model found at iteration {it}, path {model_path}')
+    return state_dict
+
+
+def print_size(net, verbose=False):
+    """
+    Print the number of parameters of a network
+    """
+
+    if net is not None and isinstance(net, torch.nn.Module):
+        # module_parameters = filter(lambda p: p.requires_grad, net.parameters())
+        module_parameters = list(filter(lambda p: p[1].requires_grad, net.named_parameters()))
+
+        if verbose:
+            for n, p in module_parameters:
+                print(n, p.numel())
+
+        params = sum([np.prod(p.size()) for n, p in module_parameters])
+        print("{} Parameters: {:.6f}M".format(
+            net.__class__.__name__, params / 1e6), flush=True)
+
+
+
+def local_directory(name, model_cfg, dataset_cfg, output_directory):
+    # tensorboard_directory = train_cfg['tensorboard_directory']
+    # ckpt_path = output_directory # train_cfg['output_directory']
+
+    # generate experiment (local) path
+    model_name = model_identifier(model_cfg)
+    if not model_cfg["use_mean_condition"]:
+        data_name = ""
+    else:
+        data_name = f"_L{dataset_cfg['clip_length']}"
+    local_path = model_name + data_name + "half" + f"_{'mean_condition' if model_cfg['use_mean_condition'] else 'un_cond'}_batch{dataset_cfg['batch_size']}_{model_cfg['loss']}"
+
+    if not (name is None or name == ""):
+        local_path = name + "_" + local_path
+
+    # Get shared output_directory ready
+    output_directory = os.path.join('exp', local_path, output_directory)
+    os.makedirs(output_directory, mode=0o775, exist_ok=True)
+    print("output directory", output_directory, flush=True)
+    return local_path, output_directory
+
+
+# Utilities for diffusion models
+
+def calc_diffusion_hyperparams(T, beta_0, beta_T, beta=None, fast=False):
+    """
+    Compute diffusion process hyperparameters
+
+    Parameters:
+    T (int):                    number of diffusion steps
+    beta_0 and beta_T (float):  beta schedule start/end value,
+                                where any beta_t in the middle is linearly interpolated
+
+    Returns:
+    a dictionary of diffusion hyperparameters including:
+        T (int), Beta/Alpha/Alpha_bar/Sigma (torch.tensor on cpu, shape=(T, ))
+        These cpu tensors are changed to cuda tensors on each individual gpu
+    """
+
+    if fast and beta is not None:
+        Beta = torch.tensor(beta)
+        T = len(beta)
+    else:
+        Beta = torch.linspace(beta_0, beta_T, T)
+    Alpha = 1 - Beta
+    Alpha_bar = Alpha + 0
+    Beta_tilde = Beta + 0
+    for t in range(1, T):
+        Alpha_bar[t] *= Alpha_bar[t-1]  # \bar{\alpha}_t = \prod_{s=1}^t \alpha_s
+        Beta_tilde[t] *= (1-Alpha_bar[t-1]) / (1-Alpha_bar[t])  # \tilde{\beta}_t = \beta_t * (1-\bar{\alpha}_{t-1}) / (1-\bar{\alpha}_t)
+    Sigma = torch.sqrt(Beta_tilde)  # \sigma_t^2  = \tilde{\beta}_t
+
+    _dh = {}
+    _dh["T"], _dh["Beta"], _dh["Alpha"], _dh["Alpha_bar"], _dh["Sigma"] = T, Beta.cuda(), Alpha.cuda(), Alpha_bar.cuda(), Sigma
+    return _dh
+
+
+""" Experimental feature for checkpoint smoothing. Didn't seem to help in brief tests """
+def smooth_dict(d, d0, n=None, alpha=None):
+    """ Smooth with arithmetic average (if n not None) or geometric average (if alpha not None) """
+    assert int(n is None) + int(alpha is None) == 1
+    if d is None:
+        assert n is None or n == 0 # must be first iteration
+        return d0
+
+    if n is not None:
+        avg_fn = lambda x, y: (x * n + y) / (n+1)
+    else:
+        avg_fn = lambda x, y: alpha * x + (1. - alpha) * y
+    return _bin_op_dict(d, d0, avg_fn)
+
+def _bin_op_dict(d0, d1, op):
+    """ Apply binary operator recursively to two dictionaries with matching keys """
+    if isinstance(d0, dict) and isinstance(d1, dict):
+        assert d0.keys() == d1.keys(), "Dictionaries must hvae matching keys"
+        return {
+            k: _bin_op_dict(d0[k], d1[k], op) for k in d0.keys()
+        }
+    elif not isinstance(d0, dict) and not isinstance(d1, dict):
+        return op(d0, d1)
+    else: raise Exception("Dictionaries must match keys")
diff --git a/validate.py b/validate.py
new file mode 100644
index 0000000..ace003f
--- /dev/null
+++ b/validate.py
@@ -0,0 +1,158 @@
+# Copyright (c) Microsoft Corporation.
+# Licensed under the MIT License.
+import os
+import argparse
+import numpy as np
+import torch 
+import torchaudio as ta
+import wandb
+#from src.binauralgrad.losses import L2Loss, AmplitudeLoss, PhaseLoss
+import auraloss
+import speechmetrics
+from omegaconf import DictConfig, OmegaConf
+import hydra
+import sys
+from multiprocessing import Pool
+
+from losses import L2Loss, PhaseLoss, AmplitudeLoss
+
+
+
+window_length = 5 # seconds
+scoresmetrics = speechmetrics.load(['pesq', 'stoi'], window_length)
+mrstft_function = auraloss.freq.MultiResolutionSTFTLoss()
+
+
+def compute_metrics(binauralized, reference, path_pt, path_gt, use_pesq=False):
+    '''
+    compute l2 error, amplitude error, and angular phase error for the given binaural and reference singal
+    :param binauralized: 2 x T tensor containing predicted binaural signal
+    :param reference: 2 x T tensor containing reference binaural signal
+    :return: errors as a scalar value for each metric and the number of samples in the sequence
+    '''
+    binauralized, reference = binauralized.unsqueeze(0), reference.unsqueeze(0)
+
+    # compute error metrics
+    l2_error = L2Loss()(binauralized, reference)
+    amplitude_error = AmplitudeLoss(sample_rate=48000)(binauralized, reference)
+    phase_error = PhaseLoss(sample_rate=48000, ignore_below=0.2)(binauralized, reference)
+    mrstft_error = mrstft_function(binauralized, reference)
+    if use_pesq:
+
+        scores = scoresmetrics(path_pt, path_gt)
+        pesq = scores['pesq'] if len(scores['pesq']) == 1 else scores['pesq'][0]
+
+        return{
+            "l2": l2_error,
+            "amplitude": amplitude_error,
+            "phase": phase_error,
+            "mrstft": mrstft_error,
+            "pesq_score": pesq,
+            "samples": binauralized.shape[-1]
+        }
+    
+    else:
+        return{
+            "l2": l2_error,
+            "amplitude": amplitude_error,
+            "phase": phase_error,
+            "mrstft": mrstft_error,
+            "samples": binauralized.shape[-1]
+        }
+    
+def process_test_sequence(args):
+    ckpt, test_sequence, result_folder, ref_folder, use_pesq, reference = args
+    print(f"Cal {test_sequence}...")
+    generated_path = f"{result_folder}/waveform/{ckpt}"
+    # binauralize and save output
+    binaural, _ = ta.load(f"{generated_path}/{test_sequence}.wav")
+    # compute error metrics
+    error = compute_metrics(binaural, reference, f"{generated_path}/{test_sequence}.wav", f"{ref_folder}/{test_sequence}/binaural.wav", use_pesq=use_pesq)
+    return error
+
+def validate(cfg):
+    if cfg.wandb is not None:
+        wandb_cfg = cfg.pop("wandb")
+        wandb.init(
+            **wandb_cfg, config=OmegaConf.to_container(cfg, resolve=True)
+        )
+    validate_cfg = cfg.pop("validate")
+    iters_per_logging = validate_cfg["iters_per_ckpt"]
+    max_ckpt = validate_cfg["max_ckpt"]
+    min_ckpt = validate_cfg["min_ckpt"]
+    data_path = validate_cfg["data_path"]
+    result_folder = validate_cfg["exp_path"]
+    use_pesq = validate_cfg["use_pesq"]
+    test_sequences = [f"subject{i+1}" for i in range(8)] + ["validation_sequence"]
+    ref_folder = data_path + "/testset"
+    # Preload all reference data
+    reference_data = {}
+    for test_sequence in test_sequences:
+        reference, sr = ta.load(f"{ref_folder}/{test_sequence}/binaural.wav")
+        reference_data[test_sequence] = reference
+
+
+    for ckpt in range(min_ckpt, max_ckpt + 1, iters_per_logging):
+
+        # binauralized and evaluate test sequence for the eight subjects and the validation sequence
+        with Pool() as pool:
+            args = [(ckpt, test_sequence, result_folder, ref_folder, use_pesq, reference_data[test_sequence]) for test_sequence in test_sequences]
+            errors = pool.map(process_test_sequence, args)
+        # errors = []
+        # for test_sequence in test_sequences:
+        #     print(f"Cal {test_sequence}...")
+        #     generated_path = f"{result_folder}/waveform/{ckpt}"
+        #     # binauralize and save output
+        #     binaural, _ = ta.load(f"{generated_path}/{test_sequence}.wav")
+        #     # compute error metrics
+        #     reference, sr = reference_data[test_sequence]
+        #     errors.append(compute_metrics(binaural, reference, f"{generated_path}/{test_sequence}.wav", f"{ref_folder}/{test_sequence}/binaural.wav", use_pesq=use_pesq))
+        #     print(errors[-1])
+        print("ckpt", ckpt)
+
+        # accumulate errors
+        sequence_weights = np.array([err["samples"] for err in errors])
+        sequence_weights = sequence_weights / np.sum(sequence_weights)
+        l2_error = sum([err["l2"] * sequence_weights[i] for i, err in enumerate(errors)])
+        amplitude_error = sum([err["amplitude"] * sequence_weights[i] for i, err in enumerate(errors)])
+        phase_error = sum([err["phase"] * sequence_weights[i] for i, err in enumerate(errors)])
+        mrstft_error = sum([err["mrstft"] * sequence_weights[i] for i, err in enumerate(errors)])
+        if use_pesq:
+            pesq = sum([err["pesq_score"] * sequence_weights[i] for i, err in enumerate(errors)])
+
+            #print accumulated errors on testset
+            print(f"l2 (x10^3):     {l2_error * 1000:.3f}")
+            print(f"amplitude:      {amplitude_error:.3f}")
+            print(f"phase:          {phase_error:.3f}")
+            print(f"mrstft:          {mrstft_error:.3f}")
+            print(f"pesq:           {pesq:.3f}")
+            wandb.log({
+            "l2 (x10^3)":     l2_error,
+            "amplitude":      amplitude_error,
+            "phase":          phase_error,
+            "mrstft":          mrstft_error,
+            "pesq":           pesq,
+            }, step=ckpt)
+        else:
+            print(f"l2 (x10^3):     {l2_error * 1000:.3f}")
+            print(f"amplitude:      {amplitude_error:.3f}")
+            print(f"phase:          {phase_error:.3f}")
+            print(f"mrstft:          {mrstft_error:.3f}")
+            wandb.log({
+            "l2 (x10^3)":     l2_error,
+            "amplitude":      amplitude_error,
+            "phase":          phase_error,
+            "mrstft":          mrstft_error,
+            }, step=ckpt)
+            
+    wandb.finish()
+
+@hydra.main(version_base=None, config_path="configs/", config_name="config")
+def main(cfg: DictConfig) -> None:
+    print(OmegaConf.to_yaml(cfg))
+    OmegaConf.set_struct(cfg, False)
+    validate(cfg)
+
+
+if __name__ == "__main__":
+    main()
\ No newline at end of file
